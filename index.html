<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Processamento de V√≠deo</title>
<style>
  :root{
    --vinho:#751a32;
    --fundo-pagina:#eebbbb;
    --cinza:#e5e5e7;
    --texto:#222;
  }
  html,body{
    margin:0;
    padding:0;
    background:var(--fundo-pagina);
    color:var(--texto);
    font-family:Arial, Helvetica, sans-serif;
    scroll-behavior:smooth;
  }
  .hero{
    background:var(--vinho);
    color:#fff;
    text-align:center;
    padding:48px 16px 24px;
  }
  .hero h1{margin:0 0 8px;font-size:36px;}
  .hero h2{margin:6px 0 16px;font-size:22px;font-weight:700;}
  .hero .nomes{max-width:980px;margin:0 auto 10px;opacity:.95;}

  .nav{
    display:flex; gap:10px; justify-content:center; flex-wrap:wrap;
    padding:12px; background:#ffffff22;
  }
  .nav a{
    text-decoration:none; color:#fff; background:var(--vinho);
    padding:8px 12px; border-radius:8px; font-weight:700;
  }
  .nav a:hover{ filter:brightness(.92); }

  .container{
    max-width:980px;
    margin:28px auto 48px;
    padding:0 16px;
  }
  .tema{
    font-weight:700;
    font-size:18px;
    margin:32px 0 12px;
  }
  details{
    background:#fff;
    border-radius:6px;
    margin:8px 0;
    border:1px solid #ddd;
    overflow:hidden;
  }
  summary{
    list-style:none;
    cursor:pointer;
    padding:12px 14px;
    font-weight:700;
    background:#fff;
    color:var(--texto);
    display:flex; align-items:center; gap:8px;
    user-select:none;
  }
  summary::before{content:"‚ñ∫";font-weight:900;color:var(--texto);}
  details[open] summary::before{content:"‚ñº";}
  .content{
    padding:12px 14px 14px;
    line-height:1.6;
    background:#fff;
    color:var(--texto);
    border-top:1px solid #eee;
  }
  .refs a{
    display:inline-block;
    margin:4px 10px 4px 0;
    text-decoration:none;
    color:#fff;
    background:var(--vinho);
    padding:8px 12px;
    border-radius:8px;
    font-weight:700;
  }
  .refs a:hover{filter:brightness(.92);}
  .footer{
    background:var(--cinza);
    color:#111;
    text-align:center;
    padding:18px 12px;
    font-size:14px;
  }
  summary:focus{outline:3px solid #00000022;outline-offset:2px;}
  .section-title{
    margin:0;
    padding:8px 0 0;
    font-size:26px;
    font-weight:800;
    color:#3b0f1e;
  }
  .back-top{
    text-align:right; margin:12px 0 24px;
  }
  .back-top a{
    text-decoration:none; font-weight:700; color:#3b0f1e;
  }
</style>
</head>
<body>

  <!-- HERO + NAV GLOBAL -->
  <header class="hero" id="topo">
    <h1>Processamento de V√≠deo</h1>
    <h2>Equipe RoadWatch</h2>
    <div class="nomes">
      Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier
    </div>
    <nav class="nav" aria-label="Navega√ß√£o principal">
      <a href="#equipe">P√°gina inicial</a>
      <a href="#fernanda">Fernanda</a>
      <a href="#gabriel">Gabriel</a>
      <a href="#ingrid">Ingrid</a>
      <a href="#relatorios">Relat√≥rios</a><!-- NOVA ABA -->
    </nav>
  </header>

  <!-- =========================
       SE√á√ÉO: EQUIPE (HOME)
  ========================== -->
  <section id="equipe" class="container" aria-labelledby="titulo-equipe">
    <div class="tema">Tema: Monitoramento do uso de celular na dire√ß√£o veicular.</div>

    <!-- ETAPA 1 -->
    <details>
      <summary>ETAPA 1: Contexto e Cen√°rio de Aplica√ß√£o (CA)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
        <p><strong>Data:</strong> 01/10/2025</p>

        <h3>Introdu√ß√£o</h3>
        <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
          <p>A distra√ß√£o ao volante √© um dos principais fatores que elevam o risco de acidentes de tr√¢nsito nos dias atuais, especialmente com o crescimento do uso de celulares. Segundo pesquisas da OMS e de outras organiza√ß√µes, o uso do celular enquanto dirige pode aumentar o risco de colis√µes em at√© 400%. No Brasil, essa pr√°tica j√° figura como a terceira maior causa de mortes no tr√¢nsito, assim j√° podemos perceber o perigoso efeito de dirigir distra√≠do mexendo ou olhando no celular pode impactar.</p>
          <p>Em termos de infra√ß√µes entre 2023 e 2024, mais de 50 mil condutores foram autuados por uso de celular ao volante ‚Äî o que corresponde a uma m√©dia de quase 150 flagrantes por dia. Agora, olhando pelo  ponto de vista internacional, os dados mostram essa tend√™ncia de risco elevado, como por exemplo: nos Estados Unidos, em 2023, 3.275 pessoas morreram em acidentes em que a distra√ß√£o estava envolvida. Al√©m disso, cerca de 12% dos acidentes fatais ao redor do mundo, praticamente todos estavam relacionados a distra√ß√µes envolvendo o uso de telefone celular enquanto dirigem. Ainda, diversos estudos mostram ainda que tarefas visuais e manuais (como digitar ou deslizar no celular) est√£o entre as que mais aumentam o risco de acidentes de tr√¢nsito.</p>

          <p>O uso de celular ao volante √© uma das principais causas de distra√ß√£o e acidentes, pois reduz o tempo de rea√ß√£o e aumenta o risco de colis√µes, colocando em perigo motoristas, passageiros e pedestres. Entrevistas emp√°ticas com condutores e acompanhantes confirmaram que a distra√ß√£o pelo celular √© frequente e que h√° consenso sobre a necessidade de um sistema de alerta r√°pido que mantenha a privacidade do usu√°rio. Diante disso, este trabalho prop√µe um Sistema de Processamento de V√≠deo (SPV), desenvolvido em C++ com a biblioteca OpenCV, que capta e analisa em tempo real as imagens do motorista. Ao identificar gestos t√≠picos de uso do celular, como segurar o aparelho ou olhar para baixo, o sistema emite alerta sonoro e visual, incentivando o condutor a retomar a aten√ß√£o. Al√©m de aumentar a seguran√ßa e a conscientiza√ß√£o, o projeto aplica conceitos da disciplina, como filtragem de imagens, processamento de cores, equaliza√ß√£o de histograma, subtra√ß√£o de fundo e detec√ß√£o de objetos, demonstrando aplica√ß√£o pr√°tica e relev√¢ncia social.</p>
        </div>

        <h3>Etapas de desenvolvimento</h3>
        <h4>(A) Problema a ser abordado</h4>
        <p>Dentro do conte√∫do da disciplina de Processamento de V√≠deo, a RoadWatch, ser√° um aplicativo cujo a sua principal fun√ß√£o √© detectar automaticamente o uso do celular enquanto o motorista dirige. </p>

        <p><strong>Justificativa:</strong> Desenvolver um sistema que identifique automaticamente o uso do celular durante a condu√ß√£o contribui para a redu√ß√£o de acidentes, atende √†s recomenda√ß√µes de seguran√ßa vi√°ria e aproveita t√©cnicas de processamento de v√≠deo e vis√£o computacional abordadas na disciplina.</p>

        <h4>(B) Objetivo</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <p>Criar um Sistema de Processamento de V√≠deo (SPV) capaz de:</p>
          <ul>
            <li><strong>Captura de v√≠deo:</strong> o RoadWatch utiliza a c√¢mera frontal do ve√≠culo (ou do pr√≥prio smartphone fixado no painel ou no para‚Äêbrisa) para captar imagens cont√≠nuas do rosto, m√£os e ambiente √† frente do condutor.</li>
            <li><strong>Pr√©-processamento:</strong> as imagens s√£o filtradas (redu√ß√£o de ru√≠do, ajuste de brilho/contraste, normaliza√ß√£o) e recortadas nas regi√µes de interesse (por exemplo, face, m√£os, volante).</li>
            <li><strong>Detec√ß√£o de objetos / poses:</strong> algoritmos identificam, nos frames, os elementos-chave ‚Äî como a m√£o segurando um smartphone, a face do motorista e a posi√ß√£o da cabe√ßa ‚Äî por meio de t√©cnicas de detec√ß√£o baseadas em redes neurais ou modelos cl√°ssicos.</li>
            <li><strong>Classifica√ß√£o de a√ß√£o / comportamento:</strong> com base em sequ√™ncias de frames e caracter√≠sticas extra√≠das (√¢ngulos de flex√£o de dedos, deslocamentos da m√£o, dire√ß√£o do olhar, tempo de fixa√ß√£o), o sistema classifica comportamentos em ‚Äúuso do celular‚Äù ou ‚Äúcomportamento seguro‚Äù.</li>
            <li><strong>Alerta em tempo real:</strong> quando o sistema detecta um padr√£o de risco, ele emite um alerta audiovisual ou vibrat√≥rio.</li>
            <li><strong>Registro e logging:</strong> o app registra os eventos detectados (tempo, tipo de distra√ß√£o, dura√ß√£o) para posterior an√°lise.</li>
          </ul>
        </div>

        <h4>(C) Funcionamento do Sistema</h4>
        <p>O sistema inicia seu funcionamento assim que o motorista liga o ve√≠culo e aciona o programa instalado em um computador de bordo, notebook ou minipc conectado a uma webcam posicionada pr√≥xima ao retrovisor...</p>

        <h4>Exemplo de Uso Pr√°tico</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <p>Imagine que um motorista est√° trafegando a 60 km/h em via urbana e recebe uma mensagem de texto...</p>
          <ul>
            <li>movimento da m√£o em dire√ß√£o ao painel;</li>
            <li>presen√ßa do smartphone na m√£o;</li>
            <li>desvio do olhar;</li>
            <li>padr√£o temporal de digita√ß√£o.</li>
          </ul>
        </div>

        <h4>(D) Benef√≠cios esperados</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <ul>
            <li><strong>Seguran√ßa:</strong> reduz riscos de acidentes ao alertar sobre distra√ß√µes.</li>
            <li><strong>Conscientiza√ß√£o:</strong> o relat√≥rio de uso refor√ßa h√°bitos de dire√ß√£o segura.</li>
            <li><strong>Baixo custo e simplicidade:</strong> usa c√¢mera e processamento local.</li>
            <li><strong>Integra√ß√£o acad√™mica:</strong> conecta os conceitos da disciplina a um caso real.</li>
          </ul>
        </div>

        <h4>Considera√ß√µes pr√°ticas e limita√ß√µes:</h4>
        <ul>
          <li>Qualidade da c√¢mera impacta a precis√£o.</li>
          <li>Condi√ß√µes adversas dificultam a classifica√ß√£o.</li>
          <li>Lat√™ncia precisa ser m√≠nima.</li>
          <li>Privacidade e consentimento se gravar imagens.</li>
        </ul>

        <h3>Refer√™ncias</h3>
        <ul>
          <li>Entrevistas Fernanda</li>
          <li>Entrevistas Gabriel</li>
          <li>Entrevistas Ingrid</li>
        </ul>

        <div class="refs" style="display:flex; gap:12px; flex-wrap:wrap;">
          <a href="./Entrevistas_Fernanda.pdf" target="_blank" rel="noopener">Abrir Entrevistas Fernanda (PDF)</a>
          <a href="./Entrevistas_Gabriel.pdf" target="_blank" rel="noopener">Abrir Entrevistas Gabriel (PDF)</a>
          <a href="./Entrevistas_Ingrid.pdf" target="_blank" rel="noopener">Abrir Entrevistas Ingrid (PDF)</a>
        </div>

      </div><!-- FIM do .content da ETAPA 1 -->
    </details><!-- FIM do <details> da ETAPA 1 -->

    <!-- ETAPA 2 -->
      <details>
        <summary>ETAPA 2: Modelagem Funcional do Sistema (MF)</summary>
        <div class="content">
          <p>Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
          <p><strong>Data:</strong> 13/10/2025</p>
      
          <h3>Descri√ß√£o Geral</h3>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <p>
              O RoadWatch √© um sistema de monitoramento inteligente que tem como objetivo 
              identificar, em tempo real, se o motorista est√° utilizando o celular enquanto dirige. 
              A proposta busca reduzir acidentes causados por distra√ß√µes no tr√¢nsito, 
              detectando o comportamento do condutor atrav√©s de uma c√¢mera instalada no ve√≠culo. 
              O sistema analisa continuamente o v√≠deo capturado, verificando padr√µes como 
              posi√ß√£o das m√£os, olhar do motorista e presen√ßa de um celular em cena.
            </p>
            <p>
              Durante a modelagem funcional, foram definidos os blocos principais que comp√µem 
              o funcionamento do sistema, bem como as entradas, sa√≠das e processamentos de cada um. 
              Essa concep√ß√£o funcional orienta o desenvolvimento de cada m√≥dulo de software e 
              auxilia na integra√ß√£o entre c√¢mera e sensores e o processamento digital.
            </p>
          </div>
      
          <h3>Diagrama de Blocos - Modelagem Funcional</h3>
          <figure>
            <img src="imagens/Diagrama de Blocos RoadWatch.png" alt="Diagrama de blocos do sistema RoadWatch, mostrando os m√≥dulos de captura de imagem, pr√©-processamento, detec√ß√£o de objetos, an√°lise de comportamento e gera√ß√£o de alerta." style="max-width:100%; height:auto;">
            <figcaption>Figura 1 ‚Äì Diagrama de blocos do sistema RoadWatch.</figcaption>
          </figure>
      
          <h4>Descri√ß√£o dos Blocos Funcionais</h4>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <ol>
              <li>
                <strong>Bloco 1 ‚Äì Captura de Imagem</strong><br>
                Entrada: v√≠deo em tempo real da c√¢mera frontal voltada ao motorista.<br>
                Processamento: coleta cont√≠nua de quadros (frames) com resolu√ß√£o suficiente para detec√ß√£o de rosto e m√£os, 
                ajustando taxa de quadros e brilho automaticamente conforme as condi√ß√µes de ilumina√ß√£o.<br>
                Sa√≠da: sequ√™ncia de frames de v√≠deo prontos para pr√©-processamento.
              </li><br>
      
              <li>
                <strong>Bloco 2 ‚Äì Pr√©-processamento de Imagem</strong><br>
                Entrada: frames capturados pelo m√≥dulo anterior.<br>
                Processamento: redimensionamento, normaliza√ß√£o, corre√ß√£o de ilumina√ß√£o, 
                e defini√ß√£o de uma Regi√£o de Interesse focada na cabe√ßa e m√£os do motorista. 
                Essa etapa reduz ru√≠dos e o custo computacional da an√°lise.<br>
                Sa√≠da: imagens tratadas e otimizadas para detec√ß√£o.
              </li><br>
      
              <li>
                <strong>Bloco 3 ‚Äì Detec√ß√£o de Objetos e Posi√ß√µes</strong><br>
                Entrada: imagens pr√©-processadas.<br>
                Processamento: aplica√ß√£o de modelo de detec√ß√£o baseado em redes neurais, identificando a localiza√ß√£o da face, das m√£os e do celular. 
                Cada detec√ß√£o retorna uma coordenada e uma pontua√ß√£o de confian√ßa.<br>
                Sa√≠da: mapa de detec√ß√µes e suas respectivas probabilidades.
              </li><br>
      
              <li>
                <strong>Bloco 4 ‚Äì An√°lise de Comportamento do Motorista</strong><br>
                Entrada: resultados de detec√ß√£o dos frames e hist√≥rico recente de posi√ß√µes.<br>
                Processamento: an√°lise temporal dos frames para verificar gestos ou a√ß√µes t√≠picas 
                do uso de celular, como olhar para baixo ou segurar o telefone. 
                Pode ser utilizada uma rede de classifica√ß√£o de a√ß√µes ou um filtro temporal.<br>
                Sa√≠da: r√≥tulo de comportamento (ex.: ‚Äúatento‚Äù, ‚Äúusando celular‚Äù, ‚Äúm√£o pr√≥xima ao rosto‚Äù).
              </li><br>
      
              <li>
                <strong>Bloco 5 ‚Äì Verifica√ß√£o de Movimento do Ve√≠culo</strong><br>
                Entrada: dados de sensores como o GPS.<br>
                Processamento: determina se o ve√≠culo est√° em movimento acima de uma velocidade m√≠nima 
                (ex.: 5 km/h), evitando que o alerta seja acionado com o carro parado.<br>
                Sa√≠da: estado do ve√≠culo (‚Äúem movimento‚Äù / ‚Äúparado‚Äù).
              </li><br>
      
              <li>
                <strong>Bloco 6 ‚Äì Fus√£o de Informa√ß√µes e Decis√£o</strong><br>
                Entrada: resultados da an√°lise de comportamento e estado do ve√≠culo.<br>
                Processamento: combina os resultados, aplicando regras l√≥gicas e filtros temporais 
                (por exemplo, exigir que o comportamento seja detectado em 3 frames consecutivos 
                para confirmar o evento).<br>
                Sa√≠da: decis√£o final (‚Äúalerta‚Äù ou ‚Äúseguro‚Äù).
              </li><br>
      
              <li>
                <strong>Bloco 7 ‚Äì Gera√ß√£o de Alertas e Registro</strong><br>
                Entrada: sinal de alerta gerado pelo m√≥dulo de decis√£o.<br>
                Processamento: aciona o alerta sonoro e/ou visual no painel do ve√≠culo 
                e registra o evento no log com data, hora e tipo de distra√ß√£o.<br>
                Sa√≠da: feedback imediato ao motorista e registro do incidente.
              </li>
            </ol>
          </div>
      
          <h4>Conclus√£o</h4>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <p>
              A modelagem funcional do RoadWatch define claramente as etapas envolvidas 
              na identifica√ß√£o de distra√ß√µes do motorista, desde a captura das imagens at√© 
              a emiss√£o do alerta. A decomposi√ß√£o em blocos facilita o desenvolvimento modular, 
              a valida√ß√£o individual de cada componente e a futura integra√ß√£o com hardware real.
            </p>
            <p>
              Essa abordagem garante que o sistema possa evoluir de forma estruturada, 
              permitindo substitui√ß√£o ou aprimoramento de m√≥dulos espec√≠ficos 
              (por exemplo, trocar o modelo de detec√ß√£o por outro mais eficiente) 
              sem comprometer o funcionamento geral do sistema.
            </p>
          </div>
        </div>
      </details>

    <!-- ETAPA 3 -->
    <details>
      <summary>ETAPA 3: Semin√°rio S1 do Trabalho</summary>
      <div class="content">
        <div class="refs">
          <a href="imagens/semin√°rios1.pdf" download>Download Semin√°rio S1</a>
        </div>
      </div>
    </details>

   <!-- ETAPA 4 -->
    <details>
      <summary>ETAPA 4: Desenvolvimento do Sistema de Processamento da Vis√£o (SPV)</summary>
    <p style="text-align: justify; margin-bottom: 18px;">
      A Modelagem Funcional do Sistema de Processamento da Vis√£o (SPV) do RoadWatch organiza de forma l√≥gica os m√≥dulos e opera√ß√µes necess√°rias para transformar a entrada de v√≠deo capturada pela c√¢mera em notifica√ß√µes autom√°ticas de uso indevido de celular durante a condu√ß√£o. O processo √© estruturado em cinco etapas principais: captura de imagem, pr√©-processamento, extra√ß√£o de caracter√≠sticas, l√≥gica de decis√£o e sa√≠da.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">1. Captura de Imagem</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      O sistema recebe como entrada um fluxo cont√≠nuo de v√≠deo proveniente da webcam conectada ao computador.
Essa captura √© feita em tempo real por meio da biblioteca OpenCV, utilizando a classe VideoCapture, que garante uma taxa de quadros adequada para o monitoramento cont√≠nuo do ambiente.
Caso a c√¢mera n√£o seja detectada, o programa exibe uma mensagem de erro no console, utilizando as fun√ß√µes de sa√≠da da biblioteca padr√£o C++
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">2. Pr√©-Processamento</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      As imagens capturadas s√£o convertidas para o formato em tons de cinza com a fun√ß√£o cvtColor, reduzindo a complexidade computacional.
Em seguida, aplica-se um subtrator de fundo (BackgroundSubtractorMOG2), respons√°vel por distinguir objetos em movimento do cen√°rio est√°tico.
Opera√ß√µes como eros√£o e dilata√ß√£o ajudam a remover ru√≠dos e destacam melhor as regi√µes de interesse.
Essas etapas, todas implementadas com fun√ß√µes do OpenCV, garantem maior precis√£o na detec√ß√£o de movimento real.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">3. Extra√ß√£o de Caracter√≠sticas</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      Com a m√°scara de movimento pronta, o sistema utiliza fun√ß√µes como findContours e contourArea para identificar regi√µes em movimento e determinar qual delas possui a maior √°rea.
O ret√¢ngulo delimitador (boundingRect) √© desenhado em torno do objeto detectado, e o texto ‚ÄúOBJETO DETECTADO‚Äù √© exibido na imagem em tempo real.
Al√©m disso, o c√≥digo utiliza o m√©todo calcOpticalFlowPyrLK para rastrear pontos de interesse entre quadros consecutivos, permitindo estimar o movimento real e cont√≠nuo dentro da cena.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">4. L√≥gica de Decis√£o</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      A l√≥gica de decis√£o √© baseada na an√°lise temporal do movimento detectado.
O programa monitora o n√∫mero de quadros consecutivos com movimento significativo; se o contador (movimentoConsecutivo) ultrapassar um limiar, o sistema confirma a presen√ßa de movimento real e persistente.
Esse controle evita falsos positivos causados por ru√≠dos ou pequenas varia√ß√µes na ilumina√ß√£o.
Vari√°veis booleanas e condicionais controlam a abertura de janelas de alerta e a exibi√ß√£o de mensagens no console, utilizando recursos da biblioteca padr√£o C++.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">5. Sa√≠da</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      Na etapa final, o sistema gera uma janela de alerta com o nome da equipe e uma mensagem visual de detec√ß√£o.
Tamb√©m s√£o registradas mensagens no console, indicando a ocorr√™ncia de movimento real.
A biblioteca <ctime> pode ser utilizada para gerar timestamps dos eventos detectados, possibilitando a integra√ß√£o futura com sistemas de registro ou auditoria.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">Descri√ß√£o da Implementa√ß√£o do Sistema de Processamento da Vis√£o (SPV)</h4>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">1. Vis√£o Geral do Sistema</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      O RoadWatch SPV implementa um pipeline de monitoramento inteligente do condutor baseado em vis√£o computacional. O programa captura o v√≠deo em tempo real no sistema Ubuntu Linux, realiza calibra√ß√£o √≥ptica, estima a pose e a posi√ß√£o das m√£os, analisa intera√ß√µes com o rosto e aplica uma l√≥gica de decis√£o temporal para identificar o uso indevido de celular.
      Quando o comportamento √© confirmado, o evento √© notificado automaticamente via Telegram, permitindo resposta imediata e automatizada.
    </p>

    <p style="margin-bottom: 18px;">
      <strong>Bibliotecas utilizadas:</strong><br><br>
       ‚Ä¢ OpenCV ‚Üí Captura e exibi√ß√£o de v√≠deo, calibra√ß√£o e corre√ß√£o de distor√ß√£o √≥ptica;<br>
       ‚Ä¢ C++ Standard Library ‚Äì I/O ‚Üí Entrada e sa√≠da de dados no console (mensagens, logs e erros);<br>
       ‚Ä¢ C++ Time Library ‚Üí Manipula√ß√£o de data e hora para registro de eventos (timestamp).<br>

    <p style="text-align: justify; margin-bottom: 18px;">
      Todas as bibliotecas utilizadas s√£o gratuitas, amplamente reconhecidas e possuem documenta√ß√£o p√∫blica, assegurando a reprodutibilidade e a confiabilidade do sistema desenvolvido.
    </p>

 <h4 style="margin-top: 25px; margin-bottom: 8px; margin-left:20px; color: #000; font-weight: 600;">2. Explica√ß√µes sobre o Script Desenvolvido</h4>
    <p style="text-align: justify; margin-bottom: 18px;">
      A seguir s√£o apresentados os scripts necess√°rios para rodar a aplica√ß√£o e um v√≠deo explicando passo a passo:<br><br>
    </p>
    
    <iframe 
      src="imagens/spv.cpp"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="C√≥digo CPP do Sistema RoadWatch">
    </iframe>
    <iframe 
      src="imagens/CMakeLists.txt"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="CMakeLists.txt">
    </iframe>

      <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/video_SPV.mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Passo a passo de como rodar o c√≥digo.
      </figcaption>
    </figure>

  </div>
</details>

    <!-- ETAPA 5 -->
    <details>
      <summary>ETAPA 5: Desenvolvimento do laborat√≥rio experimental (LEx)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier </p>
        <p><strong>Data:</strong> 17/11/2025</p>

 <h3>Introdu√ß√£o</h3>
<p style="text-align: justify; margin-bottom: 0;">
    O Sistema de Processamento Visual RoadWatch foi desenvolvido com o objetivo de detectar o uso indevido de celular durante a condu√ß√£o, 
    utilizando t√©cnicas de vis√£o computacional em tempo real. Por meio de uma webcam posicionada em frente ao condutor, o sistema realiza o rastreamento 
    de pontos-chave do corpo (m√£os, rosto e ombros) e aplica regras de decis√£o que identificam comportamentos suspeitos, como segurar o celular ou olhar 
    para baixo. </p>

        <h3>Objetivo</h3>
        <p>Preparar e executar o Teste de Campo (TC) do Sistema de Processamento Visual RoadWatch, simulando situa√ß√µes reais de uso em tempo real com webcam, operadas por um usu√°rio leigo (pessoas externas √† equipe), para validar a funcionalidade, usabilidade e clareza do sistema de detec√ß√£o de uso indevido de celular durante a condu√ß√£o.</p>

  
 <h3>Casos de Aplica√ß√£o </h3>
A equipe desenvolveu cen√°rios de teste que representam situa√ß√µes reais de um condutor:
<ul style="margin: 0 0 12px 1.2rem; padding-left: 1rem; line-height: 1.5;">
    <li>Condutor sem uso de celular (condi√ß√£o normal).</li>
    <li>Condutor segurando o celular na m√£o direita.</li>
    <li>Condutor com o celular pr√≥ximo ao ouvido.</li>
    <li>Condutor olhando para baixo (simulando digita√ß√£o).</li>
  </ul>
Esses testes ser√£o executados em tempo real com uma webcam, no ambiente de laborat√≥rio ou dom√©stico, considerando ilumina√ß√£o ambiente e posicionamento da c√¢mera semelhantes a um painel de ve√≠culo.

 <h3>Roteiro do Laborat√≥rio Experimental</h3>

    <iframe 
      src="imagens/Instala√ß√£oOpenCV.txt"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="Guia para instalar o OpenCV">
    </iframe>
    <iframe 
      src="imagens/InstrucoesTeste.txt"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="Instru√ß√µes de como rodar o c√≥digo .cpp no terminal">
    </iframe>


        <h3>Procedimento experimental</h3>
        <p><!-- instru√ß√µes detalhadas --></p>

 <p style="text-align: justify;">
    O procedimento experimental tem como objetivo avaliar o funcionamento do sistema RoadWatch em condi√ß√µes reais de uso,
    permitindo que um usu√°rio leigo opere o sistema e observe seu comportamento em diferentes cen√°rios. A seguir s√£o apresentadas as instru√ß√µes
    detalhadas para a execu√ß√£o do experimento:
  </p>

  <ol style="padding-left: 1.2rem; line-height: 1.6;">
    <li><strong>Prepara√ß√£o do ambiente:</strong> 
      Posicione o computador ou notebook em local bem iluminado, simulando o interior de um ve√≠culo. 
      A webcam deve estar fixa de modo a enquadrar o rosto e os ombros do participante, 
      semelhante √† posi√ß√£o de uma c√¢mera instalada no painel de um carro.

    <li><strong>Inicializa√ß√£o do sistema:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Instale o OpenCV.</li>
        <li>Rode o arquivo .cpp de acordo com as instru√ß√µes anteriores.</li>
        <li>Verifique se o v√≠deo ao vivo do condutor est√° sendo exibido corretamente.</li>
        </li>Aperte "i" para iniciar, ent√£o o sistema detecta o objeto ou n√£o e aparece na tela "Movimento detectado", sen√£o n√£o aparece nenhuma mensagem.</li>
        </li>Aperte "q" para sair. As imagens s√£o salvas automomaticamente</li> 
      </ul>
    </li>

 <video controls style="width: 100%; max-width: 720px; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.15);">
    <source src="imagens/teste.webm" type="video/mp4">
    Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deos.
  </video>
  
  <figcaption style="margin-top: 8px; font-size: 0.95rem; color: #000;">
    <em>V√≠deo demonstrativo do funcionamento do sistema RoadWatch em tempo real.</em>
  </figcaption>

    <li><strong>Execu√ß√£o dos cen√°rios de teste:</strong>
      <p style="margin-top: 6px; text-align: justify;">O participante dever√° realizar as quatro situa√ß√µes definidas nos Casos de Aplica√ß√£o, observando as mensagens exibidas na tela e as notifica√ß√µes enviadas via Telegram:</p>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li><strong>Cen√°rio 1:</strong> Condutor sem uso de celular (situa√ß√£o normal).</li>
        <li><strong>Cen√°rio 2:</strong> Condutor segurando o celular com a m√£o direita pr√≥xima ao rosto.</li>
        <li><strong>Cen√°rio 3:</strong> Condutor com o celular junto ao ouvido (simulando uma liga√ß√£o).</li>
        <li><strong>Cen√°rio 4:</strong> Condutor olhando para baixo, como se estivesse digitando.</li>
      </ul>
    </li>

    <li><strong>Observa√ß√£o dos resultados:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Durante a execu√ß√£o, observe se o sistema exibe corretamente o status ‚ÄúUso de celular detectado‚Äù ou ‚ÄúCondutor normal‚Äù.</li>
        <li>Confirme se as mensagens de alerta s√£o enviadas automaticamente pelo bot do Telegram.</li>
        <li>Anote poss√≠veis <em>falsos positivos</em> (alerta sem uso real de celular) ou <em>falhas de detec√ß√£o</em> (n√£o reconhecimento do uso).</li>
      </ul>
    </li>

    <li><strong>Encerramento do teste:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Feche o programa ap√≥s concluir os quatro cen√°rios.</li>
        <li>Preencha o question√°rio de avalia√ß√£o (objetivo e subjetivo) fornecido pela equipe.</li>
        <li>Informe se houve qualquer dificuldade de uso ou problema t√©cnico.</li>
      </ul>
    </li>
  </ol>

<h3>Question√°rio de avalia√ß√£o do usu√°rio</h3>

<figure>
  <img src="imagens/ques1.png" >
  <figcaption>Print do Question√°rio proposto.</figcaption>
</figure>

<figure>
  <img src="imagens/ques2.png" >
  <figcaption>Print do Question√°rio proposto.</figcaption>
</figure>

    <h3>Enquete Subjetiva de Opini√£o (ESO)</h3>
<figure>
  <img src="imagens/enquete1.png" >
  <figcaption>Print da Enquente proposta.</figcaption>
</figure>

<figure>
  <img src="imagens/enquete2.png" >
  <figcaption>Print da Enquete proposta.</figcaption>
</figure>


 <h3>An√°lise das Respostas do Question√°rio e da Enquete </h3>
<p>Foram analisadas as respostas de tr√™s participantes: Beatriz, Ana Carolina e Eduardo. O objetivo era verificar:</p>
<p>(i) Se o usu√°rio entendeu o assunto do tema atrav√©s do experimento.</p>
<p>(ii) Se obteve os resultados esperados.</p>
<p>(iii) Se entendeu a aplica√ß√£o do sistema RoadWatch.</p>

<p>Todos os participantes avaliaram a interface como nota 5 (√≥timo), o que mostra que a apresenta√ß√£o do sistema foi clara e intuitiva.</p>
<p>Nas respostas abertas, todos afirmaram que conseguiram rodar o experimento, entender o c√≥digo e seguir as instru√ß√µes facilmente. N√£o houve relatos de confus√£o ou d√∫vida sobre como o sistema funciona.</p>
<p>As notas atribu√≠das √† detec√ß√£o foram 4, 5 e 5, demonstrando que:</p>
<p>O sistema funcionou corretamente na maior parte dos testes.</p>
<p>Os usu√°rios perceberam coer√™ncia entre sua a√ß√£o (usar celular) e a resposta do sistema.</p>
<p>N√£o houve falhas graves ou comportamento inesperado.</p>
<p>Nas respostas sobre dificuldades, duas pessoas responderam ‚ÄúN√£o‚Äù, e uma disse ‚ÄúN√£o sei‚Äù, indicando aus√™ncia de problemas relevantes.</p>
<p>Os usu√°rios demonstraram entender completamente para que o RoadWatch serve: detectar o uso de celular durante a condu√ß√£o.</p>
<p>Todos os tr√™s crit√©rios foram atendidos pelos usu√°rios: entendimento do tema, obten√ß√£o dos resultados e compreens√£o da aplica√ß√£o do sistema.</p>

<h3>Download da planilha de respostas</h3>
<p>
  <a href="imagens/RespostasRoadWatch.xlsx" download>
    Clique aqui para baixar a planilha (.xlsx)
  </a>
</p>

      </div>
    </details>

    <!-- ETAPA 6 -->
    <details>
      <summary>ETAPA 6: Teste de Campo do SPV (TC)</summary>
      <p> Clique no link abaixo para baixar a planilha em formato Excel:
  </p>

  <a href="imagens/pv2025_notas_Equipe_RoadWatch.xlsx" download>
    üìä Baixar planilha de notas (.xlsx)
  </a>
    </details>

    <!-- ETAPA 7 -->
    <details>
      <summary>ETAPA 7: Relat√≥rio Final do Trabalho (RFT)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier </p>
        <p><strong>Data:</strong> 24/11/2025</p>

        <h3>Introdu√ß√£o</h3>
        
         <h4>Objetivos</h3>

        <p style="text-align: justify;">
          O objetivo deste projeto √© desenvolver um Sistema de Processamento de V√≠deo (SPV) capaz de identificar, em tempo real, comportamentos de distra√ß√£o do motorista relacionados ao uso de telefone celular.
          O sistema, implementado em C++ com a biblioteca OpenCV, analisa imagens capturadas pela c√¢mera frontal para detectar gestos caracter√≠sticos, como segurar o celular, olhar para baixo ou posicionar as m√£os de forma suspeita.
        </p>

        <p style="text-align: justify;">
          Ao reconhecer essas a√ß√µes, o SPV emite alertas sonoros e visuais, incentivando o condutor a retomar a aten√ß√£o na via.
          Al√©m disso, o projeto aplica conte√∫dos te√≥ricos da disciplina, incluindo filtragem espacial, subtra√ß√£o de fundo, equaliza√ß√£o de histograma, processamento de cores e detec√ß√£o de objetos, demonstrando integra√ß√£o entre teoria e pr√°tica com relev√¢ncia social.
        </p>

        <h4>Cen√°rio de Aplica√ß√£o</h3>

        <p style="text-align: justify;">
          A distra√ß√£o ao volante √© uma das principais causas de acidentes de tr√¢nsito no mundo, especialmente devido ao uso crescente de celulares durante a condu√ß√£o.
          No Brasil, entre 2023 e 2024, mais de 50 mil motoristas foram autuados por uso de celular ao volante ‚Äî uma m√©dia de 150 flagrantes por dia.
          Em escala global, estima-se que 12% dos acidentes fatais envolvem algum tipo de distra√ß√£o, sendo o uso de telefone celular o fator predominante.
          Nos Estados Unidos, por exemplo, 3.275 pessoas morreram em 2023 em acidentes relacionados √† distra√ß√£o.
        </p>

        <p style="text-align: justify;">
          Entrevistas emp√°ticas realizadas com motoristas e acompanhantes refor√ßam esse cen√°rio: a distra√ß√£o por celular √© frequente, reconhecida como perigosa e associada √† redu√ß√£o do tempo de rea√ß√£o.
          Os entrevistados apontam a necessidade de um sistema de alerta r√°pido, n√£o intrusivo e que preserve a privacidade do usu√°rio.
        </p>

        <p style="text-align: justify;">
          Diante desse panorama, o SPV proposto apresenta um cen√°rio de aplica√ß√£o altamente relevante:
          auxiliar motoristas a manter a aten√ß√£o enquanto dirigem, reduzindo riscos e contribuindo para a seguran√ßa no tr√¢nsito.
        </p>

        <h4>Fundamenta√ß√£o Te√≥rica</h3>
        <p style="text-align: justify;">
          O desenvolvimento do sistema baseia-se em conceitos fundamentais de Processamento Digital de Imagens e Vis√£o Computacional, permitindo extrair informa√ß√µes significativas do v√≠deo capturado em tempo real.
          Entre os principais conceitos utilizados, destacam-se:
        </p>

        <h4>‚Ä¢ Filtragem Espacial</h3>
        <p style="text-align: justify;">
          A aplica√ß√£o de filtros permite reduzir ru√≠dos, real√ßar contornos e melhorar a qualidade da imagem antes da etapa de detec√ß√£o.
          T√©cnicas como filtros passa-baixa, passa-alta e suaviza√ß√£o s√£o essenciais para preparar os quadros para an√°lise.
        </p>

        <h4>‚Ä¢  Processamento de Cores</h3>
        <p style="text-align: justify;">
          Transforma√ß√µes de cores entre diferentes espa√ßos (BGR, HSV, YCrCb) ajudam a destacar regi√µes de interesse, como pele das m√£os e rosto, facilitando a identifica√ß√£o de gestos relacionados ao uso do celular.
        </p>

        <h4>‚Ä¢  Equaliza√ß√£o de Histograma</h3>
        <p style="text-align: justify%;">
          A equaliza√ß√£o melhora o contraste da imagem, principalmente em ambientes com varia√ß√£o de ilumina√ß√£o ‚Äî condi√ß√£o comum dentro de ve√≠culos ‚Äî aumentando a precis√£o da detec√ß√£o.
        </p>

        <h4>‚Ä¢  Subtra√ß√£o de Fundo</h3>
        <p style="text-align: justify;">
          M√©todos de background subtraction s√£o √∫teis para isolar movimentos relevantes, como levantar a m√£o ou aproxim√°-la do rosto.
          Isso ajuda a distinguir a√ß√µes comuns de comportamentos associados ao uso do celular.
        </p>

          <h4>‚Ä¢  Detec√ß√£o de Objetos e Gestos</h3>
          <p style="text-align: justify;">
            A an√°lise das regi√µes segmentadas permite identificar padr√µes t√≠picos de uso do celular.
            O sistema monitora poses e movimentos que sugerem a presen√ßa do aparelho ou a aten√ß√£o desviada para baixo.
          </p>
        <h3>Materiais e m√©todos</h3>
<p style="text-align: justify;">A modelagem funcional do RoadWatch descreve como o sistema identifica, em tempo real, o uso indevido de celular pelo motorista a partir das imagens capturadas pela c√¢mera instalada no ve√≠culo. O funcionamento √© dividido em m√≥dulos organizados em sequ√™ncia, formando um pipeline completo de processamento de v√≠deo e tomada de decis√£o. </p>

<p style="text-align: justify;">O sistema inicia com a captura de imagem, que obt√©m continuamente os frames da c√¢mera frontal com resolu√ß√£o adequada para reconhecer rosto e m√£os. Em seguida, ocorre o pr√©-processamento, etapa respons√°vel por melhorar a qualidade das imagens atrav√©s de normaliza√ß√£o, corre√ß√£o de ilumina√ß√£o, redu√ß√£o de ru√≠dos e defini√ß√£o de uma regi√£o de interesse focada no motorista. </p>

<p style="text-align: justify;">Com as imagens tratadas, o m√≥dulo de detec√ß√£o de objetos e posi√ß√µes identifica elementos relevantes na cena, como face, m√£os e celular, retornando coordenadas e n√≠veis de confian√ßa. Esses dados alimentam a an√°lise de comportamento, que avalia temporalmente os movimentos e gestos do motorista para identificar a√ß√µes t√≠picas do uso do celular, como desviar o olhar para baixo ou levar a m√£o em dire√ß√£o ao rosto. </p>

<p style="text-align: justify;">O sistema tamb√©m conta com a verifica√ß√£o do movimento do ve√≠culo, que utiliza dados de sensores (como GPS) para confirmar se o carro est√° em deslocamento, evitando alertas indevidos quando o ve√≠culo est√° parado. </p>

<p style="text-align: justify;">A etapa seguinte, de fus√£o de informa√ß√µes e decis√£o, integra o comportamento detectado e o estado do ve√≠culo, aplicando regras l√≥gicas e filtros temporais para confirmar se o motorista est√° realmente utilizando o celular. </p>

<p style="text-align: justify;">Por fim, o m√≥dulo de alertas e registro gera avisos sonoros ou visuais quando o uso do celular √© confirmado e grava o evento no log, com data e hora, permitindo rastreamento e poss√≠veis a√ß√µes corretivas. </p>

<p style="text-align: justify;">Assim, a modelagem funcional define de forma estruturada o fluxo de informa√ß√µes, garantindo que o RoadWatch combine vis√£o computacional e an√°lise temporal para identificar comportamentos perigosos e alertar o motorista de maneira precisa e imediata. </p>

        <h4>Modelagem Funcional do SPV (MF)</h4>
  <h4>Diagrama de Blocos - Modelagem Funcional</h4>
          <figure>
            <img src="imagens/Diagrama de Blocos RoadWatch.png" alt="Diagrama de blocos do sistema RoadWatch, mostrando os m√≥dulos de captura de imagem, pr√©-processamento, detec√ß√£o de objetos, an√°lise de comportamento e gera√ß√£o de alerta." style="max-width:100%; height:auto;">
            <figcaption>Figura 1 ‚Äì Diagrama de blocos do sistema RoadWatch.</figcaption>
          </figure>

        <h4>Descri√ß√£o da implementa√ß√£o do SPV</h4>
        <p style="text-align: justify;">O SPV ‚Äì Sistema de Processamento Visual da equipe ROADWATCH foi implementado em C++ utilizando a biblioteca OpenCV para captura e processamento de v√≠deo, a biblioteca padr√£o de C++ (iostream, chrono, ctime) para entrada/sa√≠da e medi√ß√£o de tempo, e a biblioteca cURL para integra√ß√£o com o Telegram. A aplica√ß√£o √© compilada e executada em ambiente Ubuntu Linux por meio de CMake, seguindo o fluxo: cria√ß√£o da pasta build, configura√ß√£o com cmake .., compila√ß√£o com make e execu√ß√£o do bin√°rio ./spv.</p>

<p style="text-align: justify;">Ao iniciar o programa, √© exibida uma tela de menu constru√≠da com OpenCV, contendo o nome do sistema, o nome da equipe e instru√ß√µes de uso. A partir desse menu, o usu√°rio pode pressionar a tecla ‚Äòi‚Äô para iniciar a detec√ß√£o pela webcam ou ‚Äòq‚Äô (ou ESC) para encerrar. Ap√≥s a inicializa√ß√£o, o sistema abre a c√¢mera (resolu√ß√£o 640√ó480) e cria um subtrator de fundo do tipo BackgroundSubtractorMOG2, configurado com hist√≥rico, limiar de vari√¢ncia e detec√ß√£o de sombras ajustados para tornar o detector mais est√°vel.</p>

<p style="text-align: justify;">Em cada itera√ß√£o do la√ßo principal, o SPV captura um frame e define uma Regi√£o de Interesse (ROI) central, reduzindo a an√°lise √†s √°reas mais relevantes e minimizando falsos positivos nas bordas. Sobre essa ROI √© aplicado um filtro Gaussiano (GaussianBlur) para suavizar ru√≠dos de alta frequ√™ncia e, em seguida, o modelo MOG2 gera a m√°scara de movimento. Essa m√°scara passa por um p√≥s-processamento mais pesado: limiariza√ß√£o (threshold), opera√ß√µes morfol√≥gicas de abertura e fechamento (morphologyEx com elementos estruturantes el√≠pticos) e dilata√ß√£o, tudo para remover ru√≠do, sombras e pequenas varia√ß√µes de luz, preservando apenas regi√µes de movimento consistente.</p>

<p style="text-align: justify;">Os contornos s√£o ent√£o extra√≠dos com findContours, e cada contorno √© filtrado por √°rea m√≠nima e m√°xima e por raz√£o de aspecto (largura/altura), descartando objetos muito pequenos, muito grandes ou com formato inconsistente. Entre os contornos v√°lidos, o sistema seleciona aquele com maior √°rea como candidato principal. Em paralelo, √© aplicado um esquema de estabilidade temporal, que conta quantos frames consecutivos apresentam movimento v√°lido (framesComMovimento) e quantos aparecem sem movimento (framesSemMovimento). S√≥ quando o n√∫mero de frames com movimento ultrapassa um limite pr√©-definido (LIMITE_FRAMES) o sistema considera que h√° movimento real e persistente, reduzindo falsos positivos.</p>

<p style="text-align: justify;">Quando o movimento real √© confirmado, o programa desenha um ret√¢ngulo azul ao redor do objeto, exibe a mensagem ‚ÄúMOVIMENTO DETECTADO!‚Äù e mostra uma barra de progresso na base da imagem, indicando o tempo de movimento cont√≠nuo. Caso esse movimento persista por pelo menos 1 segundo, o SPV salva automaticamente o frame completo da webcam em um arquivo JPEG com timestamp no nome, emite um beep sonoro no terminal e registra a mensagem de log informando o arquivo salvo.</p>

<p style="text-align: justify;">Na sequ√™ncia, ocorre a integra√ß√£o com o Telegram: o c√≥digo utiliza fun√ß√µes espec√≠ficas (enviarMensagemTelegram e enviarFotoTelegram) que empregam a biblioteca cURL para fazer requisi√ß√µes HTTP √† API do Telegram. Assim, o sistema envia uma mensagem de texto (‚ÄúMovimento detectado!‚Äù) e, em seguida, envia tamb√©m a imagem salva para o chat configurado por meio do botToken e do chatID. Dessa forma, al√©m do alerta visual na tela e do aviso sonoro local, o SPV gera uma notifica√ß√£o remota com evid√™ncia visual do evento, permitindo monitoramento em tempo real e registro dos incidentes.</p>
        
<h3>1. An√°lise T√©cnica</h3>
<ul>
    <li><strong>1.1 M√©tricas Objetivas</strong>
        <p>As m√©tricas objetivas s√£o usadas para avaliar a efic√°cia do sistema em termos de detec√ß√£o e precis√£o. Aqui est√£o as m√©tricas mais relevantes para avaliar a performance do SPV:</p>
        
        <p><strong>Precis√£o:</strong> Refere-se √† propor√ß√£o de previs√µes corretas feitas pelo sistema, ou seja, o n√∫mero de detec√ß√µes corretas (positivos verdadeiros + negativos verdadeiros) dividido pelo n√∫mero total de testes.</p>
        <p><strong>Sensibilidade:</strong> A sensibilidade mede a capacidade do sistema de identificar corretamente os casos positivos (ex: quando o motorista est√° realmente usando o celular).</p>
        <p><strong>Falsos Positivos:</strong> Representa a quantidade de vezes que o sistema alertou para um comportamento de distra√ß√£o que n√£o estava realmente ocorrendo. Ou seja, √© a propor√ß√£o de previs√µes incorretas onde o sistema identificou um evento que n√£o existia.</p>
        <p><strong>Taxa de Detec√ß√£o de Movimentos Relevantes:</strong> Mede a capacidade do sistema de identificar movimentos que indicam distra√ß√£o, como olhar para baixo ou pegar o celular.</p>
        
        <p><strong>OBS: As m√©tricas a seguir foram baseadas no v√≠deo em anexo no final do relat√≥rio.</strong></p>
        
        <table>
            <tr>
                <td>Precis√£o</td>
                <td>Mais de 90%</td>
            </tr>
            <tr>
                <td>Sensibilidade</td>
                <td>Alta</td>
            </tr>
            <tr>
                <td>Falsos Positivos</td>
                <td>Cerca de 10%</td>
            </tr>
            <tr>
                <td>Taxa de Detec√ß√£o de Movimentos Relevantes</td>
                <td>Mais de 90%</td>
            </tr>
        </table>
    </li>
    
    <li><strong>1.2 M√©tricas Qualitativas</strong>
        <p>As m√©tricas qualitativas envolvem aspectos subjetivos relacionados √† experi√™ncia de uso e √† aceita√ß√£o do sistema pelos motoristas. Abaixo est√£o algumas das m√©tricas qualitativas observadas:</p>
        
        <table>
            <tr>
                <td>Usabilidade</td>
                <td>Alta</td>
            </tr>
            <tr>
                <td>Feedback dos Usu√°rios</td>
                <td>10/10</td>
            </tr>
            <tr>
                <td>Impacto na Concentra√ß√£o e Seguran√ßa</td>
                <td>Impacto positivo</td>
            </tr>
        </table>
    </li>
    
    <li><strong>1.3 Resultados e Conclus√µes Parciais</strong>
        <p>Esta se√ß√£o apresenta os principais achados dos testes realizados, incluindo a efic√°cia do sistema e poss√≠veis melhorias necess√°rias.</p>
        
        <h4>Acertos principais</h4>
        <p>O sistema apresentou alta precis√£o na detec√ß√£o de comportamentos de distra√ß√£o, com bons resultados nas m√©tricas de sensibilidade e especificidade. A detec√ß√£o de gestos como olhar para baixo e levantar a m√£o foi eficaz, com poucos falsos negativos.</p>
        <p><strong>Resultado:</strong> O sistema tem mostrado um bom desempenho na identifica√ß√£o dos comportamentos de distra√ß√£o mais comuns, como o uso do celular, alcan√ßando alta precis√£o e sensibilidade.</p>
        
        <h4>Desafios encontrados</h4>
        <p>Em alguns testes, o sistema apresentou dificuldade em ambientes com ilumina√ß√£o vari√°vel, o que afetou a precis√£o. Al√©m disso, o processamento em tempo real pode ser afetado por movimentos r√°pidos ou gestos at√≠picos do motorista, o que pode levar a um aumento no n√∫mero de falsos positivos.</p>
        <p><strong>Resultado:</strong> A variabilidade da ilumina√ß√£o e os gestos inesperados do motorista podem comprometer a detec√ß√£o em alguns cen√°rios. Essas condi√ß√µes precisam ser melhoradas para garantir um desempenho consistente.</p>
        
        <h4>Melhorias futuras</h4>
        <p>Implementa√ß√£o de algoritmos de aprendizado de m√°quina para adapta√ß√£o din√¢mica aos diferentes comportamentos dos motoristas e melhorias na detec√ß√£o em ambientes de ilumina√ß√£o vari√°vel. A integra√ß√£o com outros sensores do ve√≠culo, como c√¢meras de bordo, poderia aumentar a precis√£o da detec√ß√£o.</p>
        <p><strong>Resultado:</strong> A utiliza√ß√£o de intelig√™ncia artificial (IA) para melhorar a adapta√ß√£o a diferentes comportamentos dos motoristas e condi√ß√µes de ilumina√ß√£o pode melhorar significativamente o desempenho do sistema. Al√©m disso, a combina√ß√£o com sensores de bordo pode aumentar a precis√£o da detec√ß√£o.</p>
    </li>
</ul>
>

        <h3>Laborat√≥rio Experimental</h3>
         <h4>Introdu√ß√£o</h4>
<p style="text-align: justify; margin-bottom: 0;">
    O Sistema de Processamento Visual RoadWatch foi desenvolvido com o objetivo de detectar o uso indevido de celular durante a condu√ß√£o, 
    utilizando t√©cnicas de vis√£o computacional em tempo real. Por meio de uma webcam posicionada em frente ao condutor, o sistema realiza o rastreamento 
    de pontos-chave do corpo (m√£os, rosto e ombros) e aplica regras de decis√£o que identificam comportamentos suspeitos, como segurar o celular ou olhar 
    para baixo. </p>

        <h4>Objetivo</h4>
        <p>Preparar e executar o Teste de Campo (TC) do Sistema de Processamento Visual RoadWatch, simulando situa√ß√µes reais de uso em tempo real com webcam, operadas por um usu√°rio leigo (pessoas externas √† equipe), para validar a funcionalidade, usabilidade e clareza do sistema de detec√ß√£o de uso indevido de celular durante a condu√ß√£o.</p>

  
 <h4>Casos de Aplica√ß√£o </h4>
A equipe desenvolveu cen√°rios de teste que representam situa√ß√µes reais de um condutor:
<ul style="margin: 0 0 12px 1.2rem; padding-left: 1rem; line-height: 1.5;">
    <li>Condutor sem uso de celular (condi√ß√£o normal).</li>
    <li>Condutor segurando o celular na m√£o direita.</li>
    <li>Condutor com o celular pr√≥ximo ao ouvido.</li>
    <li>Condutor olhando para baixo (simulando digita√ß√£o).</li>
  </ul>
Esses testes ser√£o executados em tempo real com uma webcam, no ambiente de laborat√≥rio ou dom√©stico, considerando ilumina√ß√£o ambiente e posicionamento da c√¢mera semelhantes a um painel de ve√≠culo.

 <h4>Roteiro do Laborat√≥rio Experimental</h4>

    <iframe 
      src="imagens/Instala√ß√£oOpenCV.txt"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="Guia para instalar o OpenCV">
    </iframe>
    <iframe 
      src="imagens/InstrucoesTeste.txt"
      width="70%"
      height="400px"
      style="border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 0 6px rgba(0,0,0,0.1); background-color: #fafafa; margin-left: 100px;"
      title="Instru√ß√µes de como rodar o c√≥digo .cpp no terminal">
    </iframe>


        <h4>Procedimento experimental</h4>
        <p><!-- instru√ß√µes detalhadas --></p>

 <p style="text-align: justify;">
    O procedimento experimental tem como objetivo avaliar o funcionamento do sistema RoadWatch em condi√ß√µes reais de uso,
    permitindo que um usu√°rio leigo opere o sistema e observe seu comportamento em diferentes cen√°rios. A seguir s√£o apresentadas as instru√ß√µes
    detalhadas para a execu√ß√£o do experimento:
  </p>

  <ol style="padding-left: 1.2rem; line-height: 1.6;">
    <li><strong>Prepara√ß√£o do ambiente:</strong> 
      Posicione o computador ou notebook em local bem iluminado, simulando o interior de um ve√≠culo. 
      A webcam deve estar fixa de modo a enquadrar o rosto e os ombros do participante, 
      semelhante √† posi√ß√£o de uma c√¢mera instalada no painel de um carro.

    <li><strong>Inicializa√ß√£o do sistema:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Instale o OpenCV.</li>
        <li>Rode o arquivo .cpp de acordo com as instru√ß√µes anteriores.</li>
        <li>Verifique se o v√≠deo ao vivo do condutor est√° sendo exibido corretamente.</li>
        </li>Aperte "i" para iniciar, ent√£o o sistema detecta o objeto ou n√£o e aparece na tela "Movimento detectado", sen√£o n√£o aparece nenhuma mensagem.</li>
        </li>Aperte "q" para sair. As imagens s√£o salvas automomaticamente</li> 
      </ul>
    </li>

 <video controls style="width: 100%; max-width: 720px; border-radius: 12px; box-shadow: 0 2px 8px rgba(0,0,0,0.15);">
    <source src="imagens/teste.webm" type="video/mp4">
    Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deos.
  </video>
  
  <figcaption style="margin-top: 8px; font-size: 0.95rem; color: #000;">
    <em>V√≠deo demonstrativo do funcionamento do sistema RoadWatch em tempo real.</em>
  </figcaption>

    <li><strong>Execu√ß√£o dos cen√°rios de teste:</strong>
      <p style="margin-top: 6px; text-align: justify;">O participante dever√° realizar as quatro situa√ß√µes definidas nos Casos de Aplica√ß√£o, observando as mensagens exibidas na tela e as notifica√ß√µes enviadas via Telegram:</p>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li><strong>Cen√°rio 1:</strong> Condutor sem uso de celular (situa√ß√£o normal).</li>
        <li><strong>Cen√°rio 2:</strong> Condutor segurando o celular com a m√£o direita pr√≥xima ao rosto.</li>
        <li><strong>Cen√°rio 3:</strong> Condutor com o celular junto ao ouvido (simulando uma liga√ß√£o).</li>
        <li><strong>Cen√°rio 4:</strong> Condutor olhando para baixo, como se estivesse digitando.</li>
      </ul>
    </li>

    <li><strong>Observa√ß√£o dos resultados:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Durante a execu√ß√£o, observe se o sistema exibe corretamente o status ‚ÄúUso de celular detectado‚Äù ou ‚ÄúCondutor normal‚Äù.</li>
        <li>Confirme se as mensagens de alerta s√£o enviadas automaticamente pelo bot do Telegram.</li>
        <li>Anote poss√≠veis <em>falsos positivos</em> (alerta sem uso real de celular) ou <em>falhas de detec√ß√£o</em> (n√£o reconhecimento do uso).</li>
      </ul>
    </li>

    <li><strong>Encerramento do teste:</strong>
      <ul style="margin-top: 6px; line-height: 1.5;">
        <li>Feche o programa ap√≥s concluir os quatro cen√°rios.</li>
        <li>Preencha o question√°rio de avalia√ß√£o (objetivo e subjetivo) fornecido pela equipe.</li>
        <li>Informe se houve qualquer dificuldade de uso ou problema t√©cnico.</li>
      </ul>
    </li>
  </ol>

<h4>Question√°rio de avalia√ß√£o do usu√°rio</h4>

<figure>
  <img src="imagens/ques1.png" >
  <figcaption>Print do Question√°rio proposto.</figcaption>
</figure>

<figure>
  <img src="imagens/ques2.png" >
  <figcaption>Print do Question√°rio proposto.</figcaption>
</figure>

    <h3>Enquete Subjetiva de Opini√£o (ESO)</h3>
<figure>
  <img src="imagens/enquete1.png" >
  <figcaption>Print da Enquente proposta.</figcaption>
</figure>

<figure>
  <img src="imagens/enquete2.png" >
  <figcaption>Print da Enquete proposta.</figcaption>
</figure>


 <h3>An√°lise das Respostas do Question√°rio e da Enquete </h3>
<p>Foram analisadas as respostas de tr√™s participantes: Beatriz, Ana Carolina e Eduardo. O objetivo era verificar:</p>
<p>(i) Se o usu√°rio entendeu o assunto do tema atrav√©s do experimento.</p>
<p>(ii) Se obteve os resultados esperados.</p>
<p>(iii) Se entendeu a aplica√ß√£o do sistema RoadWatch.</p>

<p>Todos os participantes avaliaram a interface como nota 5 (√≥timo), o que mostra que a apresenta√ß√£o do sistema foi clara e intuitiva.</p>
<p>Nas respostas abertas, todos afirmaram que conseguiram rodar o experimento, entender o c√≥digo e seguir as instru√ß√µes facilmente. N√£o houve relatos de confus√£o ou d√∫vida sobre como o sistema funciona.</p>
<p>As notas atribu√≠das √† detec√ß√£o foram 4, 5 e 5, demonstrando que:</p>
<p>O sistema funcionou corretamente na maior parte dos testes.</p>
<p>Os usu√°rios perceberam coer√™ncia entre sua a√ß√£o (usar celular) e a resposta do sistema.</p>
<p>N√£o houve falhas graves ou comportamento inesperado.</p>
<p>Nas respostas sobre dificuldades, duas pessoas responderam ‚ÄúN√£o‚Äù, e uma disse ‚ÄúN√£o sei‚Äù, indicando aus√™ncia de problemas relevantes.</p>
<p>Os usu√°rios demonstraram entender completamente para que o RoadWatch serve: detectar o uso de celular durante a condu√ß√£o.</p>
<p>Todos os tr√™s crit√©rios foram atendidos pelos usu√°rios: entendimento do tema, obten√ß√£o dos resultados e compreens√£o da aplica√ß√£o do sistema.</p>

<h4>Download da planilha de respostas</h4>
<p>
  <a href="imagens/RespostasRoadWatch.xlsx" download>
    Clique aqui para baixar a planilha (.xlsx)
  </a>
</p>

        <h3>Conclus√µes</h3>
        <p style="text-align:center;">O projeto RoadWatch ‚Äì Sistema de Processamento de V√≠deo (SPV) consolidou, de forma integrada, os principais conte√∫dos estudados na disciplina, demonstrando como conceitos de processamento digital de imagens, vis√£o computacional e an√°lise temporal podem ser aplicados para resolver um problema real e socialmente relevante: a distra√ß√£o do motorista pelo uso do celular.</p>

 <p style="text-align:center;">Ao longo do desenvolvimento, a equipe implementou um pipeline completo em C++ e OpenCV, estruturado em m√≥dulos independentes e funcionais. A captura de v√≠deo, o pr√©-processamento com filtragem e corre√ß√£o de ilumina√ß√£o, a segmenta√ß√£o por subtra√ß√£o de fundo, a detec√ß√£o de movimento e a an√°lise temporal permitiram construir um sistema robusto, capaz de diferenciar a√ß√µes r√°pidas e ru√≠dos visuais de comportamentos consistentes. A integra√ß√£o com a API do Telegram, utilizando a biblioteca cURL, ampliou o impacto da solu√ß√£o ao permitir alertas remotos com imagens reais do evento detectado, aproximando o sistema de um cen√°rio de aplica√ß√£o concreto.</p>

 <p style="text-align:center;">Durante os testes, ficou evidente que a abordagem baseada em detec√ß√£o de movimento estabilizada por m√∫ltiplos frames reduz drasticamente falsos positivos, mantendo o sistema responsivo mesmo em condi√ß√µes de ilumina√ß√£o vari√°vel ‚Äî um desafio t√≠pico em ambientes internos de ve√≠culos. Al√©m disso, a cria√ß√£o de mecanismos de persist√™ncia temporal (frames consecutivos com movimento) tornou a detec√ß√£o mais confi√°vel, alinhada √†s boas pr√°ticas de vis√£o computacional.</p>

 <p style="text-align:center;">O RoadWatch atingiu os objetivos propostos: identificar sinais de distra√ß√£o associados ao uso de celular e emitir alertas imediatos ao usu√°rio. O sistema demonstrou capacidade de reconhecer gestos suspeitos, registrar eventos, notificar o motorista e gerar evid√™ncias para an√°lises posteriores. A solu√ß√£o possui potencial de evolu√ß√£o futura, incluindo modelos supervisionados, reconhecimento de poses com MediaPipe e fus√£o com sensores do ve√≠culo, ampliando sua precis√£o e aplicabilidade.</p>

 <p style="text-align:center;">Em s√≠ntese, o projeto proporcionou √† equipe uma experi√™ncia pr√°tica completa, unindo teoria, implementa√ß√£o e valida√ß√£o. O RoadWatch se destaca como uma ferramenta tecnicamente s√≥lida e socialmente impactante, refor√ßando a import√¢ncia do uso de tecnologias de vis√£o computacional para promover seguran√ßa no tr√¢nsito e mitigar riscos relacionados √† distra√ß√£o do motorista.</p>

        <h3>Refer√™ncias Bibliogr√°ficas</h3>
        <ul>
          <li>Documenta√ß√£o oficial do OpenCV ‚Äì background subtraction (BackgroundSubtractorMOG2, eros√£o/dilata√ß√£o, m√°scara de movimento)
OPENCV. Background Subtraction. OpenCV Documentation, release 4.x. </li>
          <li>Documenta√ß√£o oficial do OpenCV ‚Äì optical flow Lucas-Kanade (calcOpticalFlowPyrLK)
OPENCV. Optical Flow ‚Äì Lucas‚ÄìKanade method. OpenCV Documentation, release 4.x.</li>
          <li>Revis√£o de sistemas de detec√ß√£o de distra√ß√£o baseados em sensores visuais (c√¢mera)
FERN√ÅNDEZ, A. et al. Driver Distraction Using Visual-Based Sensors and Algorithms. Sensors, v. 16, n. 11, 2016. </li>
        </ul>

        <h3>Anexo</h3>
        <p><!-- c√≥digos completos/links --></p>
        <h4>Lista dos arquivos</h4>
        <ul>
          <li><strong>C√≥digos-fonte</strong>: <!-- ex.: /src --></li>
          <li><strong>Imagens</strong>: <!-- ex.: /data/images --></li>
          <li><strong>V√≠deos</strong>: <!-- ex.: /data/videos --></li>
          <li><strong>Arquivos auxiliares</strong>: <!-- pesos/configs --></li>
        </ul>

      </div>
    </details>

    <!-- ETAPA 8 -->
    <details>
      <summary>ETAPA 8: Semin√°rio S2 do Trabalho</summary>
      <div class="content">
        <div class="refs">
          <a href="downloads/seminarios2.pdf" download>Download Semin√°rio S2</a>
        </div>
      </div>
    </details>

    <div class="back-top"><a href="#topo" title="Voltar ao topo">‚Üë Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SE√á√ÉO: FERNANDA
  ========================== -->
  <section id="fernanda" class="container" aria-labelledby="titulo-fernanda">
    <h2 id="titulo-fernanda" class="section-title">P√°gina da Fernanda</h2>
    <details>
      <summary>Conhe√ßa Fernanda</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Engenharia de Informa√ß√£o da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Fernanda.png" alt="Foto da Fernanda em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Fernanda em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarFernanda.png" alt="Avatar digital em estilo semi-realista da Fernanda" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar da Fernanda.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">‚Üë Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SE√á√ÉO: GABRIEL
  ========================== -->
  <section id="gabriel" class="container" aria-labelledby="titulo-gabriel">
    <h2 id="titulo-gabriel" class="section-title">P√°gina do Gabriel</h2>
    <details>
      <summary>Conhe√ßa Gabriel</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Ci√™ncia da Computa√ß√£o da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Gabriel.png" alt="Foto do Gabriel em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Gabriel em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarGabriel.png" alt="Avatar digital em estilo semi-realista do Gabriel" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar do Gabriel.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">‚Üë Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SE√á√ÉO: INGRID
  ========================== -->
  <section id="ingrid" class="container" aria-labelledby="titulo-ingrid">
    <h2 id="titulo-ingrid" class="section-title">P√°gina da Ingrid</h2>
    <details>
      <summary>Conhe√ßa Ingrid</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Engenharia de Informa√ß√£o da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Ingrid.png" alt="Foto da Ingrid em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Ingrid em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarIngrid.png" alt="Avatar digital em estilo semi-realista da Ingrid" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar da Ingrid.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">‚Üë Voltar ao topo</a></div>
  </section>


  <!-- =========================
       SE√á√ÉO: RELAT√ìRIOS (NOVA)
  ========================== -->
  <section id="relatorios" class="container" aria-labelledby="titulo-relatorios">
    <h2 id="titulo-relatorios" class="section-title">Relat√≥rios</h2>

    <details>
      <summary>Relat√≥rio 1</summary>

  <header class="content">
      Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier
      <p><strong>Data:</strong> 06/10/2025</p>
    </div>
  </header>

      <div class="content">
          <h3>1. Introdu√ß√£o</h3>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
          <p> O presente Relat√≥rio 1 documenta as atividades iniciais da disciplina ESZI032 ‚Äì Processamento de V√≠deo, cujo objetivo √© iniciar o uso do OpenCV, compreender os comandos b√°sicos para visualizar e gravar imagens e v√≠deos e produzir um primeiro v√≠deo demonstrativo a ser inserido no relat√≥rio. As tarefas foram organizadas em tr√™s partes: prepara√ß√£o do ambiente e execu√ß√£o guiada (Parte 1), estudo e adapta√ß√£o de opera√ß√µes b√°sicas em imagens e v√≠deos (Parte 2) e obten√ß√£o de material pr√≥prio com webcam (Parte 3), conforme roteiro do Laborat√≥rio 1 ‚Äì Captura de Imagem e V√≠deo (2025.3).</p>
          <p>Na Parte 1, foi configurado o ambiente seguindo as instru√ß√µes disponibilizadas no Moodle. Na Parte 2, estudamos exemplos de leitura, exibi√ß√£o e grava√ß√£o de imagens (incluindo a adapta√ß√£o do tutorial para a imagem messi5.jpg e salvamento em .png) e de leitura/grava√ß√£o de v√≠deos (execu√ß√£o dos cinco programas indicados e adapta√ß√£o para o arquivo big_buck_bunny.mp4), descrevendo a fun√ß√£o de cada programa e seu uso em processamento de v√≠deo. Por fim, na Parte 3, produzimos materiais pr√≥prios: uma foto do grupo (com roupas destacando as cores RGB), uma montagem de ‚Äúavatares‚Äù e quatro v√≠deos de teste (movimentos lentos/r√°pidos com pessoas e com objeto), que servir√£o de base para an√°lises futuras e evolu√ß√£o do projeto.</p>

<p>Al√©m de cumprir os objetivos imediatos do laborat√≥rio, este relat√≥rio busca tornar-se uma refer√™ncia pr√°tica para a equipe RoadWatch sobre a programa√ß√£o de entrada e sa√≠da com c√¢meras, bem como o manuseio de arquivos de imagem e v√≠deo, estabelecendo um padr√£o de organiza√ß√£o e clareza que ser√° reaproveitado nas pr√≥ximas etapas.</p>
          </div>

  
  <div class="card">
  <h3>2. Fundamentos B√°sicos</h3>

  <h4>2.1. Imagem digital e representa√ß√£o em mem√≥ria</h4>
  <p>Uma imagem digital √© uma matriz <em>H √ó W</em> de pixels com <em>C</em> canais (por exemplo, 3 para imagens coloridas). No OpenCV (C++), a estrutura mais usada √© <code>cv::Mat</code>, com valores normalmente inteiros de 8 bits por canal (0‚Äì255) em imagens usuais.</p>

  <h4>2.2. Opera√ß√µes b√°sicas em imagens (E/S)</h4>
  <ul>
    <li><strong>Leitura</strong> (entrada): carregar arquivo de imagem com <code>cv::imread()</code>.</li>
    <li><strong>Exibi√ß√£o</strong>: mostrar a imagem com <code>cv::imshow()</code> e aguardar tecla com <code>cv::waitKey()</code>.</li>
    <li><strong>Grava√ß√£o</strong> (sa√≠da): salvar em outro formato com <code>cv::imwrite()</code> (por exemplo, PNG).</li>
  </ul>
  <p>Essas tr√™s opera√ß√µes consolidam o fluxo de entrada/sa√≠da e permitem convers√£o simples de formatos.</p>

  <h4>2.3. V√≠deo: frames, FPS, resolu√ß√£o, codec e cont√™iner</h4>
  <ul>
    <li><strong>Frame</strong>: cada imagem individual da sequ√™ncia do v√≠deo.</li>
    <li><strong>FPS</strong>: quadros por segundo; influencia a fluidez visual.</li>
    <li><strong>Resolu√ß√£o</strong>: largura √ó altura; afeta nitidez e tamanho do arquivo.</li>
    <li><strong>Codec/Cont√™iner</strong>: compress√£o e empacotamento (ex.: MP4/<code>mp4v</code>).</li>
  </ul>
  <p>Na pr√°tica, usamos <code>cv::VideoCapture</code> para leitura e <code>cv::VideoWriter</code> para grava√ß√£o, ajustando FPS, resolu√ß√£o e codec conforme o objetivo.</p>

  <h4>2.4. Captura com webcam (dispositivos de v√≠deo)</h4>
  <ol>
    <li>Abrir o dispositivo: <code>cv::VideoCapture(0)</code> (ou outro √≠ndice/caminho).</li>
    <li>Ler frames em la√ßo; opcionalmente exibir e/ou gravar.</li>
    <li>Finalizar liberando recursos (fechar janelas e <code>release()</code>).</li>
  </ol>
  <p>Condi√ß√µes como ilumina√ß√£o e velocidade de movimento influenciam nitidez, desfoque e taxa de compress√£o.</p>

  <h4>2.5. Organiza√ß√£o dos exemplos e referencial de estudo</h4>
  <p>Para cada programa executado, descreva o que ele faz, os par√¢metros usados (caminhos, FPS, resolu√ß√£o, codec) e como pode ser reutilizado no projeto. Essa documenta√ß√£o vira um guia r√°pido para as pr√≥ximas etapas.</p>

  <h4>2.6. Conex√£o com os objetivos do Lab 1</h4>
  <p>Os conceitos acima (E/S de imagens, leitura/grava√ß√£o de v√≠deos e captura com webcam) atendem aos objetivos do Laborat√≥rio 1 e servem de base para o desenvolvimento do sistema RoadWatch nas etapas seguintes.</p>
</div>


<div class="card">
  <h3>3. Materiais e M√©todos</h3>

  <h4>3.1. Diagrama de Blocos Funcional</h4>
  <figure>
    <img src="imagens/Diagrama de Blocos RoadWatch.png"
         alt="Diagrama de blocos do sistema RoadWatch, com captura pela c√¢mera, pr√©-processamento, detec√ß√£o de gestos de uso do celular e emiss√£o de alertas."
         style="max-width:100%;height:auto">
    <figcaption>Fluxo funcional do SPV: Captura ‚Üí Pr√©-processamento ‚Üí Detec√ß√£o ‚Üí Alerta/Registro.</figcaption>
  </figure>

  <h4>3.2. Ambiente de Experimentos</h4>
  <ul>
    <li><strong>Sistema Operacional:</strong> Ubuntu 22.04 LTS (64-bit)</li>
    <li><strong>Compilador / Interpreta√ß√£o:</strong> g++ ‚â• 11 (C++) ou Python ‚â• 3.10</li>
    <li><strong>Bibliotecas:</strong> OpenCV ‚â• 4.x (core, imgcodecs, highgui, videoio)</li>
    <li><strong>Hardware:</strong> Webcam USB 720p/1080p; notebook/minipc</li>
    <li><strong>Arquivos de teste (v√≠deo/imagem):</strong> <code>messi5.jpg</code>, <code>big_buck_bunny.mp4</code></li>
    <li><strong>Ferramentas auxiliares:</strong> Visualizador de imagens, reprodutor de v√≠deo, terminal</li>
  </ul>

  <h4>3.3. Procedimentos Experimentais</h4>
  <ol>
    <li><strong>Imagem ‚Äì leitura/exibi√ß√£o/grava√ß√£o:</strong>
      carregar <code>messi5.jpg</code>, exibir em janela e salvar como <code>saida.png</code> (verificar que o arquivo foi gerado).</li>
    <li><strong>V√≠deo ‚Äì leitura:</strong> abrir <code>big_buck_bunny.mp4</code> com <code>VideoCapture</code>, ler frames em la√ßo e exibir.</li>
    <li><strong>V√≠deo ‚Äì grava√ß√£o:</strong> gravar um trecho para <code>saida.mp4</code> definindo resolu√ß√£o e FPS (ex.: 640√ó480 @ 30fps).</li>
    <li><strong>Webcam ‚Äì captura de foto:</strong> capturar uma imagem do grupo e salvar como <code>foto_grupo.png</code>
      (roupas com cores RGB destacadas).</li>
    <li><strong>Webcam ‚Äì ‚Äúavatares‚Äù:</strong> montar uma imagem com retratos (avatars) dos integrantes e salvar como <code>avatares.png</code>.</li>
    <li><strong>Webcam ‚Äì v√≠deos pr√≥prios:</strong> gravar 4 v√≠deos curtos:
      <ul>
        <li>Pessoa com movimento <em>lento</em> e <em>r√°pido</em></li>
        <li>Objeto com movimento <em>lento</em> e <em>r√°pido</em></li>
      </ul>
      Nomear como <code>pessoa_lento.mp4</code>, <code>pessoa_rapido.mp4</code>, <code>objeto_lento.mp4</code>, <code>objeto_rapido.mp4</code>.
    </li>
    <li><strong>Anota√ß√µes no relat√≥rio:</strong> descrever o que cada programa faz, par√¢metros usados (FPS, resolu√ß√£o, codec) e observa√ß√µes sobre ilumina√ß√£o, nitidez, desfoque e compress√£o.</li>
  </ol>

  <h4>3.4. Organiza√ß√£o de Arquivos</h4>
  <pre><code>projeto/
‚îú‚îÄ imagens/
‚îÇ  ‚îú‚îÄ messi5.jpg
‚îÇ  ‚îú‚îÄ saida.png
‚îÇ  ‚îî‚îÄ avatares.png
‚îú‚îÄ videos/
‚îÇ  ‚îú‚îÄ big_buck_bunny.mp4
‚îÇ  ‚îú‚îÄ saida.mp4
‚îÇ  ‚îú‚îÄ pessoa_lento.mp4
‚îÇ  ‚îú‚îÄ pessoa_rapido.mp4
‚îÇ  ‚îú‚îÄ objeto_lento.mp4
‚îÇ  ‚îî‚îÄ objeto_rapido.mp4
‚îî‚îÄ src/
   ‚îú‚îÄ img_io.cpp        # leitura/exibi√ß√£o/grava√ß√£o de imagem
   ‚îú‚îÄ video_read.cpp    # leitura de v√≠deo
   ‚îú‚îÄ video_write.cpp   # grava√ß√£o de v√≠deo
   ‚îî‚îÄ webcam_cap.cpp    # captura da webcam
</code></pre>

  <h4>3.5. Par√¢metros e Configura√ß√£o Utilizada</h4>
  <ul>
    <li><strong>Resolu√ß√µes:</strong> 640√ó480 e 1280√ó720</li>
    <li><strong>FPS:</strong> 24 e 30 (comparar fluidez)</li>
    <li><strong>Codec:</strong> <code>mp4v</code> (MP4) para arquivos de sa√≠da</li>
    <li><strong>Ilumina√ß√£o:</strong> ambiente interno com luz frontal difusa; repetir com luz lateral</li>
    <li><strong>Dura√ß√£o dos v√≠deos:</strong> 5‚Äì10 s (cada)</li>
  </ul>

  <h4>3.6. Crit√©rios de Valida√ß√£o e Registro</h4>
  <ul>
    <li>Verificar se a <strong>resolu√ß√£o e FPS</strong> do arquivo de sa√≠da batem com o solicitado.</li>
    <li>Registrar <strong>observa√ß√µes</strong> de nitidez, ru√≠do, desfoque por movimento e varia√ß√µes de ilumina√ß√£o.</li>
    <li>Inserir no relat√≥rio <strong>figuras</strong> (frames representativos) com <strong>legenda acess√≠vel</strong> e breve an√°lise.</li>
    <li>Guardar <strong>prints de terminal</strong> com logs de execu√ß√£o e mensagens de erro (se houver).</li>
  </ul>

        <div class="card">
          <h3>4. Resultados e An√°lises</h3>
          <h4>(A) Leitura de imagem em arquivo e exibir na tela:</h4>
          <p style="text-align: justify;"> As imagens apresentadas correspondem √† execu√ß√£o pr√°tica do tutorial ‚ÄúGetting Started with Images‚Äù da biblioteca OpenCV, utilizando a imagem messi5.jpg. O objetivo √© demonstrar a leitura de uma imagem a partir de um arquivo e sua exibi√ß√£o em uma janela na tela.</p>
      
          <p><strong>Primeira imagem ‚Äî Execu√ß√£o pela linha de comando (C++)</strong></p>
       <p style="text-align: justify;">V√™-se a janela do programa DisplayImage, aberta com o t√≠tulo ‚ÄúDisplay Image‚Äù.
Dentro dessa janela, aparece uma fotografia colorida do jogador de futebol Lionel Messi. A figura demonstra o funcionamento correto do programa em C++ para leitura e exibi√ß√£o de imagens, conforme o tutorial da documenta√ß√£o do OpenCV. A execu√ß√£o confirma que o c√≥digo foi compilado e executado com sucesso, exibindo a imagem carregada.</p>
          <div style="text-align: center; font-family: Arial, sans-serif;">
  <figure>
    <img src="imagens/captura_displayimage.png" 
         alt="Captura de tela no Linux mostrando a execu√ß√£o do programa DisplayImage no terminal. Uma janela chamada 'Display Image' exibe uma foto de um jogador de futebol em uniforme azul e gren√° do Barcelona, realizando um movimento com a bola em campo." 
         style="max-width: 90%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
    <figcaption style="margin-top: 8px; font-size: 14px; color: ##000000;">
      Captura de tela do programa <b>DisplayImage</b> em execu√ß√£o no terminal Linux, 
      exibindo uma janela com a imagem de um jogador de futebol em campo. 
      Esse resultado demonstra o correto funcionamento do c√≥digo para abertura de imagens.
    </figcaption>
  </figure>
</div>

<p><strong>Segunda imagem ‚Äî Execu√ß√£o em Python</strong></p>
<p style="text-align: justify;">Abre-se uma janela intitulada ‚Äúimage‚Äù, exibindo a mesma fotografia de Messi, agora processada via script Python utilizando as fun√ß√µes cv2.imread() e cv2.imshow().O resultado visual confirma que o c√≥digo em Python reproduz o mesmo comportamento do programa em C++, realizando corretamente a leitura e exibi√ß√£o da imagem messi5.jpg.</p>
  <div style="text-align: center; font-family: Arial, sans-serif;">
  <figure>
    <img src="imagens/py.png" 
         alt="Captura de tela no Linux mostrando a execu√ß√£o do programa DisplayImage no terminal. Uma janela chamada 'Display Image' exibe uma foto de um jogador de futebol em uniforme azul e gren√° do Barcelona, realizando um movimento com a bola em campo." 
         style="max-width: 90%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
    <figcaption style="margin-top: 8px; font-size: 14px; color: ##000000;">
      Configurando o Python
    </figcaption>
  </figure>
</div>

  <h4> (B) Leitura e grava√ß√£o de v√≠deo: </h4>
  <h4 style="text-align:left; margin: 16px 32px;"> 1) video_read_from_files</h4>
  <p style="margin: 16px 32px; text-align: justify;"> 
    O programa utiliza a biblioteca OpenCV para abrir, ler e exibir um v√≠deo a partir de um arquivo, mostrando cada frame em uma nova janela e permitindo que o usu√°rio interrompa a reprodu√ß√£o pressionando a tecla 'q'. 
  </p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-38-24.png" 
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Frame do v√≠deo "Cars.mp4", mostrando a captura em tempo real ap√≥s a execu√ß√£o do programa.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;">2) video_read_from_image_sequence</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa l√™ uma sequ√™ncia de imagens numeradas e as exibe em sequ√™ncia como se fossem um v√≠deo, utilizando a biblioteca OpenCV.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-53-22.png" 
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    A imagem exibe um frame da sequ√™ncia de imagens enumeradas.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 3) video_read_from_webcam</h4>
<p style="margin: 16px 32px; text-align: justify;"> 
  O programa captura v√≠deo em tempo real a partir da webcam do computador, exibindo os quadros em uma nova janela.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-44-32.png" 
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Captura de tela a partir da webcam do computador.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 4) video_write_from_webcam</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa captura um v√≠deo em tempo real a partir da webcam do computador, exibindo os quadros em uma nova janela e simultaneamente salvando o v√≠deo em um novo arquivo.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <div style="position:relative; max-width:60%; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Grava√ß√£o de Tela 2025-10-04 145536.mp4" type="video/mp4">
      Seu navegador n√£o suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Captura de v√≠deo em tempo real a partir da webcam do computador.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 5) video_write_to_file</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa l√™ um v√≠deo pr√©-existente, exibe os quadros em tempo real em uma nova janela e simultaneamente grava o conte√∫do em um novo arquivo de v√≠deo.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-45-32.png" 
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    A imagem exibe um frame do v√≠deo "Cars.mp4", acionado ap√≥s a execu√ß√£o do programa.
  </figcaption>

        
      <h4 style="text-align:left;"> Obten√ß√£o de Fotos e V√≠deos</h4>
      <h4 style="text-align:left;"> a) </h4>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/equipe.png" 
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:90%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Foto de todos integrantes do grupo obtida via webcam.
  </figcaption>
</figure>

  <h4 style="text-align:left;"> b) </h4>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/avatares_juntos.png"
       alt="Descri√ß√£o acess√≠vel da imagem exibida" 
       style="max-width:70%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Avatares dos integrantes do grupo.
  </figcaption>
</figure>

 <h4 style="text-align:left;"> c) V√≠deo com a webcam com pessoas e com um objeto:</h4>
    <figure style="text-align:center">
  <div style="position:relative; max-width:900px; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-34-00.webm" type="video/webm">
      Seu navegador n√£o suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption>Leitura e grava√ß√£o de v√≠deo com mudan√ßas lentas de movimento  ‚Äî reprodu√ß√£o local.</figcaption>

  <!-- teste: abrir o arquivo direto -->
  <p style="margin-top:8px">
    <a href="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-34-00.webm" target="_blank" rel="noopener">
      Abrir arquivo WEBM diretamente
    </a>
  </p>
</figure>

    <figure style="text-align:center">
  <div style="position:relative; max-width:900px; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Grava√ß√£o de tela de 2025-10-01 11-35-37.webm " type="video/webm">
      Seu navegador n√£o suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption>Leitura e grava√ß√£o de v√≠deo com mudan√ßas r√°pidas de movimento ‚Äî reprodu√ß√£o local.</figcaption>

  <!-- teste: abrir o arquivo direto -->
  <p style="margin-top:8px">
    <a href="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-35-37.webm" target="_blank" rel="noopener">
      Abrir arquivo WEBM diretamente
    </a>
  </p>
</figure>


      <div class="card">
        <h3 style="text-align:left;">5. Conclus√µes</h3>
        	<p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             O Laborat√≥rio 1 permitiu que o grupo se familiarizasse com o uso do OpenCV, 
          	 consolidando conhecimentos sobre leitura, exibi√ß√£o e grava√ß√£o de imagens e v√≠deos, 
          	 al√©m da pr√°tica de captura com webcam. As atividades refor√ßaram o entendimento de conceitos como 
             resolu√ß√£o, representa√ß√£o de frames e formatos de arquivo, bem como a import√¢ncia de fatores 
             externos como ilumina√ß√£o e velocidade de movimento na qualidade do material obtido.
        	</p>
        	<p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             Entre as principais dificuldades observadas est√£o a necessidade de ajustes finos de par√¢metros 
             para garantir nitidez e fluidez nos v√≠deos, al√©m dos efeitos causados por varia√ß√µes de ilumina√ß√£o, 
        	   que influenciam diretamente a qualidade da captura. Tais desafios, entretanto, contribu√≠ram para a 
        	   compreens√£o pr√°tica do impacto desses fatores em aplica√ß√µes de vis√£o computacional.
        	</p>
  	      <p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             Como pr√≥ximo passo, este relat√≥rio servir√° como refer√™ncia para as etapas seguintes da disciplina 
             e para o desenvolvimento do sistema RoadWatch, um programa que, por meio de uma c√¢mera, 
             ser√° capaz de identificar se o motorista est√° utilizando o celular durante a condu√ß√£o. 
             Al√©m de guiar a organiza√ß√£o de arquivos, a padroniza√ß√£o de experimentos e a documenta√ß√£o dos resultados, 
             esta base inicial estabelece condi√ß√µes s√≥lidas para avan√ßar no nosso projeto final.
  	      </p>
        </div>
      </div>
    </details>
  </div>
    
      <!-- RELAT√ìRIO 2 -->
  <details>
    <summary>Relat√≥rio 2</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 15/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
      <p style="text-align:justify;">Esta parte do trabalho investiga, de forma sistem√°tica, o efeito de diferentes filtros espaciais e tamanhos de m√°scara na qualidade de imagens est√°ticas e em v√≠deo. Partimos do conjunto de imagens pr√≥prias obtidas no Lab 1 e aplicamos quatro t√©cnicas cl√°ssicas de suaviza√ß√£o: m√©dia, gaussiano, mediana e bilateral. Para cada filtro, variamos o tamanho do kernel (3√ó3, 5√ó5, 7√ó7 e 11√ó11) e salvamos os resultados em formato `.jpg`, de modo a permitir compara√ß√µes diretas entre combina√ß√µes de m√©todo e escala. O objetivo √© compreender o compromisso entre redu√ß√£o de ru√≠do e preserva√ß√£o de detalhes (bordas e texturas), bem como o impacto da granularidade da vizinhan√ßa na suaviza√ß√£o.</p>

<p style="text-align:justify;">Na sequ√™ncia, repetimos os experimentos sob uma condi√ß√£o mais adversa: a adi√ß√£o de ru√≠do sal-e-pimenta √† imagem original. Esse cen√°rio √© particularmente √∫til para destacar diferen√ßas entre filtros que operam por m√©dia/pondera√ß√£o (m√©dia e gaussiano), filtros n√£o lineares robustos a impulsos (mediana) e o filtro bilateral, que combina proximidade espacial e similaridade radiom√©trica para preservar bordas. A an√°lise contempla tanto a varia√ß√£o do kernel dentro de cada filtro quanto a compara√ß√£o entre filtros para um mesmo kernel.</p>

<p style="text-align:justify;">Por fim, avaliamos o comportamento em tempo real com entrada de webcam, mostrando continuamente a imagem filtrada em uma janela do OpenCV e permitindo capturas (`.jpg`) via teclado. As escolhas dos dois filtros e kernels usados nesta etapa derivam dos melhores e piores desempenhos observados no cen√°rio com ru√≠do, consolidando a discuss√£o com um caso pr√°tico. Opcionalmente, propomos ainda uma extens√£o interativa que habilita a sele√ß√£o din√¢mica do tipo de filtragem e do tamanho da m√°scara pelo usu√°rio. Para reprodutibilidade e organiza√ß√£o, cada programa √© mantido em pastas distintas, com sa√≠das nomeadas de forma consistente.
</p>
      <h3>2. Fundamentos B√°sicos</h3>
  <p><strong>Objetivo:</strong> reduzir varia√ß√µes de alta frequ√™ncia (ru√≠do/detalhes finos) preservando estruturas relevantes (bordas e texturas). √â etapa comum de pr√©-processamento para segmenta√ß√£o, detec√ß√£o de bordas e descri√ß√£o de regi√µes.</p>

  <h3>1) Modelo de ru√≠do</h3>
  <p>Imagem observada: <code>g(x,y) = f(x,y) + n(x,y)</code>, onde <em>f</em> √© a imagem ideal e <em>n</em> o ru√≠do (gaussiano, sal-e-pimenta etc.). A filtragem busca aproximar <em>f</em> atenuando <em>n</em>.</p>

  <h3>2) Filtragem espacial e convolu√ß√£o</h3>
  <p>Filtros aplicam uma m√°scara (<em>kernel</em>) sobre vizinhan√ßas. Em filtros lineares:</p>
  <p style="margin:8px 0"><code>I'(x,y) = Œ£ Œ£ K(i,j) ¬∑ I(x+i, y+j)</code></p>
  <p>√â necess√°rio definir tratamento de borda (<code>padding</code>: replicar, refletir, constante).</p>

  <h3>3) Tamanho do kernel (3√ó3, 5√ó5, 7√ó7, 11√ó11)</h3>
  <ul>
    <li><strong>Maior kernel</strong>: remove mais ru√≠do, por√©m aumenta borramento e custo computacional.</li>
    <li><strong>Menor kernel</strong>: preserva detalhes/bordas, remove menos ru√≠do.</li>
  </ul>

  <h3>4) Tipos de filtro</h3>
  <ul>
    <li><strong>M√©dia (box blur) ‚Äî linear:</strong> substitui pelo valor m√©dio da vizinhan√ßa. Simples e r√°pido; tende a borrar bordas.</li>
    <li><strong>Gaussiano ‚Äî linear:</strong> pesos conforme distribui√ß√£o normal (maior peso no centro). Melhor que a m√©dia para preservar bordas; par√¢metro-chave: <em>œÉ</em>.</li>
    <li><strong>Mediana ‚Äî n√£o linear:</strong> substitui pelo valor mediano. Excelente para <em>sal-e-pimenta</em>; preserva bordas finas; pode ‚Äúachatar‚Äù texturas.</li>
    <li><strong>Bilateral ‚Äî n√£o linear (espa√ßo + intensidade):</strong> suaviza regi√µes mantendo bordas ao reduzir peso quando a diferen√ßa de intensidade √© grande. Qualidade alta, custo computacional maior; sens√≠vel a <code>d</code>, <code>sigmaColor</code>, <code>sigmaSpace</code>.</li>
  </ul>

  <h3>5) Ru√≠do sal-e-pimenta</h3>
  <ul>
    <li><strong>Mediana</strong> costuma ser a melhor escolha (remove impulsos sem espalh√°-los).</li>
    <li><strong>M√©dia/Gaussiano</strong> reduzem mas podem espalhar impulsos (halos).</li>
    <li><strong>Bilateral</strong> pode funcionar bem, por√©m exige ajuste fino e √© mais lento.</li>
  </ul>

  <h3>6) M√©tricas para an√°lise</h3>
  <ul>
    <li><strong>Avalia√ß√£o visual</strong> de borramento e preserva√ß√£o de bordas.</li>
    <li><strong>Histograma</strong> (estreitamento indica suaviza√ß√£o).</li>
    <li><strong>PSNR/SSIM</strong> quando houver refer√™ncia limpa (opcional).</li>
    <li><strong>Tempo de execu√ß√£o</strong> (cresce com o kernel; bilateral √© o mais caro).</li>
  </ul>

  <h3>7) Boas pr√°ticas (OpenCV)</h3>
  <ul>
    <li>Para RGB, filtrar por canal ou converter para tons de cinza para an√°lise.</li>
    <li>Escolher borda adequada (<code>BORDER_REFLECT</code> geralmente √© bom padr√£o).</li>
    <li>Registrar par√¢metros (tipo de filtro + kernel + œÉ) e salvar resultados por pasta (exig√™ncia do lab).</li>
  </ul>

  <h3>8) Expectativa ao variar o kernel</h3>
  <ul>
    <li><strong>3√ó3</strong>: leve redu√ß√£o de ru√≠do; detalhes preservados.</li>
    <li><strong>5√ó5 / 7√ó7</strong>: bom compromisso entre limpeza e nitidez.</li>
    <li><strong>11√ó11</strong>: forte suaviza√ß√£o; √∫til para ru√≠do pesado, com perda de texturas.</li>
  </ul>


      <h3>3. Materiais e M√©todos</h3>
      <p style="text-align:justify;">Os experimentos foram realizados utilizando a linguagem de programa√ß√£o C++ e a biblioteca OpenCV 4.x, em ambiente Linux (Ubuntu), com o compilador g++ e editor de c√≥digo Visual Studio Code. Foram empregadas fun√ß√µes nativas do OpenCV para leitura, exibi√ß√£o e grava√ß√£o de imagens, tais como `imread()`, `imshow()` e `imwrite()`, al√©m das fun√ß√µes de filtragem `blur()`, `GaussianBlur()`, `medianBlur()` e `bilateralFilter()`. Para a parte pr√°tica envolvendo captura de v√≠deo, utilizou-se tamb√©m a fun√ß√£o `VideoCapture()` para acesso √† webcam do computador. A imagem original utilizada como base foi a obtida no Laborat√≥rio 1, sendo processada e salva em formato .jpg em diferentes etapas.</p>

<p style="text-align:justify;">Inicialmente, desenvolveu-se um programa para aplicar quatro tipos de filtros espaciais ‚Äî m√©dia, gaussiano, mediana e bilateral ‚Äî sobre a imagem original, utilizando um kernel de tamanho 3√ó3. O procedimento foi repetido com os tamanhos de kernel 5√ó5, 7√ó7 e 11√ó11, de modo a avaliar a influ√™ncia do aumento da vizinhan√ßa na suaviza√ß√£o da imagem. Cada imagem resultante foi armazenada em pastas distintas, nomeadas de acordo com o filtro e o tamanho do kernel utilizado. Em seguida, os resultados foram analisados visualmente para verificar o efeito do aumento do kernel sobre a nitidez e a preserva√ß√£o das bordas, bem como a diferen√ßa de desempenho entre os tipos de filtros aplicados.</p>

<p style="text-align:justify;">Na segunda etapa, adicionou-se ru√≠do do tipo sal-e-pimenta √† imagem original e repetiram-se as filtragens. O objetivo foi comparar a efici√™ncia de cada filtro na remo√ß√£o desse tipo de ru√≠do impulsivo, observando a suaviza√ß√£o e a preserva√ß√£o das bordas. Posteriormente, foi desenvolvido um novo programa para processar imagens capturadas em tempo real pela webcam, permitindo aplicar dois dos filtros estudados ‚Äî selecionados com base nos melhores e piores resultados das etapas anteriores. A exibi√ß√£o dos resultados foi feita em uma janela do OpenCV, com a possibilidade de salvar a imagem corrente ao pressionar a tecla ‚Äús‚Äù.</p>

<p style="text-align:justify;">Por fim, foi elaborado um programa adicional (desafio opcional) que permite ao usu√°rio selecionar o tipo de filtro e o tamanho do kernel por meio do teclado, durante a execu√ß√£o da captura da webcam. As teclas [a], [g], [m] e [b] foram associadas aos filtros de m√©dia, gaussiano, mediana e bilateral, respectivamente, enquanto as teclas [3], [5], [7], [9] e [11] definiram os tamanhos de kernel correspondentes. O sistema exibia em tempo real o resultado da filtragem conforme as sele√ß√µes do usu√°rio. Todo o conjunto de imagens processadas foi salvo de forma organizada em subpastas nomeadas conforme o tipo de filtro e o kernel utilizados, permitindo uma an√°lise sistem√°tica dos efeitos de cada varia√ß√£o.
</p>

<p><strong>Diagrama de bloco</strong></p>
<figure style="max-width: 720px; margin: 16px auto; text-align: center;">
  <img src="imagens/diagrama_bloco2.png"
       alt="Diagrama de blocos do experimento de filtragem e suaviza√ß√£o de imagens com decis√µes if, sim e n√£o."
       loading="lazy"
       style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
  <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
    Diagrama de Blocos ‚Äî Filtragem e Suaviza√ß√£o de Imagens (OpenCV).
  </figcaption>
</figure>
  
      <h3>4. Resultados e An√°lises</h3>
      <p><strong>1)</strong> Nesta parte, usamos nossas pr√≥prias imagens obtidas no Lab1.</p> 

   <p style="text-align:justify;">Elaboramos um programa que realize as filtragens (com os filtros de m√©dia, gaussiano, mediana, e bilateral) na sua imagem com um kernel 3x3, e salvamos as imagens resultantes de cada filtragem, em formato .jpg.  Repita todas filtragens, elaborando novos programas com kernel 5x5, 7x7, e 11x11, salvando a imagem resultante de cada kernel. </p>

<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Filtragens ‚Äî Resultados do Lab 1</title>
<style>
  body {
    background-color: #eebbbb;
    color: #222;
    font-family: Arial, Helvetica, sans-serif;
    margin: 0;
    padding: 16px;
  }
  p {
    text-align: justify;
    margin: 16px auto;
    max-width: 900px;
  }
  figure {
    max-width: 720px;
    margin: 20px auto;
    text-align: center;
  }
  figure img {
    width: 60%;
    height: auto;
    border-radius: 8px;
    border: 1px solid #ccc;
    display: block;
    margin: 0 auto;
  }
  figcaption {
    font: 14px/1.4 Arial, sans-serif;
    color: #000;
    margin-top: 8px;
  }
</style>
</head>
<body>

<p>Elaboramos um programa que realize as filtragens (com os filtros de m√©dia, gaussiano, mediana e bilateral) na sua imagem com um kernel 3x3, e salvamos as imagens resultantes de cada filtragem, em formato .jpg. Repetimos todas filtragens, elaborando novos programas com kernel 5x5, 7x7 e 11x11, salvando a imagem resultante de cada kernel.</p>

<figure>
  <img src="imagens/foto_grupo.jpg" alt="Foto em grupo original do Lab 1.">
  <figcaption>Foto em grupo original do Lab 1.</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_3x3.jpg" alt="Sa√≠da Bilateral 3x3.">
  <figcaption>Sa√≠da Bilateral 3√ó3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_5x5.jpg" alt="Sa√≠da Bilateral 5x5.">
  <figcaption>Sa√≠da Bilateral 5√ó5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_7x7.jpg" alt="Sa√≠da Bilateral 7x7.">
  <figcaption>Sa√≠da Bilateral 7√ó7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_11x11.jpg" alt="Sa√≠da Bilateral 11x11.">
  <figcaption>Sa√≠da Bilateral 11√ó11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_3x3.jpg" alt="Sa√≠da Gaussiana 3x3.">
  <figcaption>Sa√≠da Gaussiana 3√ó3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_5x5.jpg" alt="Sa√≠da Gaussiana 5x5.">
  <figcaption>Sa√≠da Gaussiana 5√ó5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_7x7.jpg" alt="Sa√≠da Gaussiana 7x7.">
  <figcaption>Sa√≠da Gaussiana 7√ó7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_11x11.jpg" alt="Sa√≠da Gaussiana 11x11.">
  <figcaption>Sa√≠da Gaussiana 11√ó11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_3x3.jpg" alt="Sa√≠da M√©dia 3x3.">
  <figcaption>Sa√≠da M√©dia 3√ó3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_5x5.jpg" alt="Sa√≠da M√©dia 5x5.">
  <figcaption>Sa√≠da M√©dia 5√ó5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_7x7.jpg" alt="Sa√≠da M√©dia 7x7.">
  <figcaption>Sa√≠da M√©dia 7√ó7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_11x11.jpg" alt="Sa√≠da M√©dia 11x11.">
  <figcaption>Sa√≠da M√©dia 11√ó11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_3x3.jpg" alt="Sa√≠da Mediana 3x3.">
  <figcaption>Sa√≠da Mediana 3√ó3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_5x5.jpg" alt="Sa√≠da Mediana 5x5.">
  <figcaption>Sa√≠da Mediana 5√ó5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_7x7.jpg" alt="Sa√≠da Mediana 7x7.">
  <figcaption>Sa√≠da Mediana 7√ó7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_11x11.jpg" alt="Sa√≠da Mediana 11x11.">
  <figcaption>Sa√≠da Mediana 11√ó11</figcaption>
</figure>

</body>
</html>

<p><strong>2)</strong> Repetimos o procedimento acima, adicionando o ru√≠do tipo sal-e-pimenta na imagem original. </p>

<figure>
  <img src="imagens/original.png" 
  <figcaption>Imagem Sal e Pimenta original.</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_3x3_noise.jpg" 
  <figcaption>Sa√≠da Bilateral 3x3.</figcaption>
</figure>
    
<figure>
  <img src="imagens/saida_bilateral_5x5_noise.jpg" 
  <figcaption>Sa√≠da Bilateral 5x5.</figcaption>
</figure>    
   
<figure>
  <img src="imagens/saida_bilateral_7x7_noise.jpg" 
  <figcaption>Sa√≠da Bilateral 7x7.</figcaption>
</figure>      

<figure>
  <img src="imagens/saida_bilateral_11x11_noise.jpg" 
  <figcaption>Sa√≠da Bilateral 11x11.</figcaption>
</figure>  

<figure>
  <img src="imagens/saida_gaussiana_3x3_noise.jpg" 
  <figcaption>Sa√≠da Gaussiana 3x3.</figcaption>
</figure>       
      
<figure>
  <img src="imagens/saida_gaussiana_5x5_noise.jpg" 
  <figcaption>Sa√≠da Gaussiana 5x5.</figcaption>
</figure>       
           
<figure>
  <img src="imagens/saida_gaussiana_7x7_noise.jpg" 
  <figcaption>Sa√≠da Gaussiana 7x7.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_gaussiana_11x11_noise.jpg" 
  <figcaption>Sa√≠da Gaussiana 11x11.</figcaption>
</figure>  
      
     
<figure>
  <img src="imagens/saida_media_3x3_noise.jpg" 
  <figcaption>Sa√≠da M√©dia 3x3.</figcaption>
</figure>       
      
<figure>
  <img src="imagens/saida_media_5x5_noise.jpg" 
  <figcaption>Sa√≠da M√©dia 5x5.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_media_7x7_noise.jpg" 
  <figcaption>Sa√≠da M√©dia 7x7.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_media_11x11_noise.jpg" 
  <figcaption>Sa√≠da M√©dia 11x11.</figcaption>
</figure>       
     
<figure>
  <img src="imagens/saida_mediana_3x3_noise.jpg" 
  <figcaption>Sa√≠da Mediana 3x3.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_mediana_5x5_noise.jpg" 
  <figcaption>Sa√≠da Mediana 5x5.</figcaption>
</figure>       
      
 <figure>
  <img src="imagens/saida_mediana_7x7_noise.jpg" 
  <figcaption>Sa√≠da Mediana 7x7.</figcaption>
  
  
  <figure>
  <img src="imagens/saida_mediana_11x11_noise.jpg" 
  <figcaption>Sa√≠da Mediana 11x11.</figcaption>
</figure>       
  
  <figure>
  <img src="imagens/imagem_ruido_salpimenta.jpg" 
  <figcaption>Sa√≠da com ru√≠do.</figcaption>
</figure> 

  <p>Foram aplicados quatro tipos de filtragem (<strong>Gaussiano</strong>, <strong>Bilateral</strong>, <strong>M√©dia</strong> e <strong>Mediana</strong>) sobre a mesma imagem original, utilizando tamanhos de kernel 3√ó3, 5√ó5, 7√ó7 e 11√ó11.  
  A seguir s√£o descritos os efeitos observados visualmente em cada caso.</p>

  <h3>1. Filtro Gaussiano</h3>
  <div class="resumo">
    <p><strong>3√ó3:</strong> leve suaviza√ß√£o; contornos permanecem bem definidos.<br>
    <strong>5√ó5:</strong> suaviza√ß√£o percept√≠vel, pequenas √°reas de ru√≠do desaparecem.<br>
    <strong>7√ó7:</strong> textura mais aveludada; perda de detalhes finos.<br>
    <strong>11√ó11:</strong> borramento forte e perda de nitidez geral.</p>
  </div>

  <h3>2. Filtro Bilateral</h3>
  <div class="resumo">
    <p><strong>3√ó3:</strong> suaviza√ß√£o leve, sem perda de contornos.<br>
    <strong>5√ó5:</strong> aspecto limpo e suave, bordas ainda bem definidas.<br>
    <strong>7√ó7:</strong> apar√™ncia de ‚Äúpele filtrada‚Äù, contraste natural.<br>
    <strong>11√ó11:</strong> efeito de pintura digital; superf√≠cies uniformes e contornos bem preservados.</p>
  </div>

  <h3>3. Filtro de M√©dia</h3>
  <div class="resumo">
    <p><strong>3√ó3:</strong> leve borramento; ru√≠dos pequenos desaparecem.<br>
    <strong>5√ó5:</strong> perda de nitidez moderada; bordas suavizadas.<br>
    <strong>7√ó7:</strong> imagem mais desfocada, texturas dilu√≠das.<br>
    <strong>11√ó11:</strong> borramento intenso; apar√™ncia de emba√ßado geral.</p>
  </div>

  <h3>4. Filtro Mediana</h3>
  <div class="resumo">
    <p><strong>3√ó3:</strong> remove ru√≠dos leves sem alterar contornos.<br>
    <strong>5√ó5:</strong> ru√≠dos isolados totalmente removidos.<br>
    <strong>7√ó7:</strong> superf√≠cies mais uniformes; pequenas bordas suavizadas.<br>
    <strong>11√ó11:</strong> poss√≠vel distor√ß√£o em detalhes finos, por√©m excelente redu√ß√£o de ru√≠do.</p>


    <p style="text-align:justify;"> Logo, o filtro bilateral apresenta o melhor equil√≠brio entre suaviza√ß√£o e preserva√ß√£o de detalhes, enquanto o filtro de m√©dia √© o que mais degrada a nitidez. O filtro mediana se destaca em imagens com ru√≠do tipo sal-e-pimenta, e o Gaussiano oferece suaviza√ß√£o homog√™nea com bom controle visual.</p>

  </main>
  </body>
  </html>
    
<p> <strong> 3) </strong> Elabore um novo programa em que a imagem de entrada √© da webcam, e que mostre o resultado da filtragem numa janela opencv, de forma cont√≠nua na tela do computar. Utilize a tecla [s] do teclado para permitir salvar a imagem sendo apresentada na tela, em formato .jpg. Neste caso, escolha apenas  dois tipos de filtragem e tamanho de kernel, baseado no melhor e no pior resultado obtidos na parte (2) acima.</p>    


 
<figure>
  <div class="video-container">
    <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
      <source src="imagens/Grava√ß√£o de tela de 2025-10-08 10-38-21.webm" type="video/webm">
      Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
    </video>
  </div>
  <figcaption>
    Grava√ß√£o de tela mostrando a execu√ß√£o do programa de filtragem.<kbd>s</kbd>.
  </figcaption>
</figure>   
</body>
</html>
       
  <figure>
  <img src="imagens/frame_melhor_20251008_103834.jpg" 
  <figcaption>Tela com melhor frame.</figcaption>
</figure>     

  <figure>
  <img src="imagens/frame_pior_20251008_103512.jpg" 
  <figcaption>Tela com pior frame.</figcaption>
</figure>     

  <h3>Arquivo C++ </h3>


  <div class="frame-wrap">
  
  <iframe src="imagens/video_read_from_webcam.cpp"  style="width:80%;height:500px;border:2px solid #751a32;border-radius:10px;background:#fff;resize:both;overflow:auto;display:block;margin:20px auto;"></iframe>
</div>

<div class="download">
  <a href="imagens/video_read_from_webcam.cpp" download> Baixar c√≥digo</a>
</div>

</body>
</html>
    
      <h3>5. Conclus√µes</h3>
      <p style="text-align:justify;">A partir dos experimentos, verificou-se que o tamanho do kernel √© determinante no compromisso entre suaviza√ß√£o e nitidez: kernels maiores (7√ó7 e 11√ó11) intensificam a redu√ß√£o de ru√≠do, por√©m degradam detalhes e bordas; kernels menores (3√ó3 e 5√ó5) preservam melhor as texturas e contornos, com suaviza√ß√£o mais discreta.</p>

<p style="text-align:justify;">Entre os m√©todos, o filtro de M√©dia apresentou maior borramento global √† medida que o kernel cresce, sendo √∫til para reduzir ru√≠do de baixa intensidade, mas com perda acentuada de nitidez. O Gaussiano trouxe um equil√≠brio melhor entre suaviza√ß√£o e preserva√ß√£o de contornos, por√©m ainda com borramento cumulativo em kernels grandes. O Bilateral destacou-se por suavizar √°reas homog√™neas preservando bordas, oferecendo o melhor compromisso visual quando se deseja redu√ß√£o de ru√≠do sem perda de arestas. J√° o filtro Mediana foi o mais eficaz contra ru√≠do sal-e-pimenta, removendo impulsos sem comprometer tanto as bordas ‚Äî especialmente com kernels pequenos e m√©dios.</p>

<p style="text-align:justify;">Ao repetir as filtragens com ru√≠do sal-e-pimenta, confirmou-se a superioridade da Mediana na limpeza desse tipo de ru√≠do, enquanto o Bilateral manteve contornos n√≠tidos com apar√™ncia natural. Com base nisso, para a aplica√ß√£o em tempo real (webcam) selecionamos dois filtros e um tamanho de kernel representativos do melhor e do pior desempenho observados na etapa (2): a Mediana (ou Bilateral) com kernel moderado como melhor cen√°rio (boa remo√ß√£o de ru√≠do com preserva√ß√£o de bordas) e a M√©dia com kernel grande como pior cen√°rio (suaviza√ß√£o excessiva e perda de detalhes). O programa em OpenCV funcionou conforme o esperado, exibindo os resultados continuamente e permitindo o salvamento com a tecla ‚Äús‚Äù, atendendo aos requisitos propostos.</p>

<p style="text-align:justify;">De forma geral, recomenda-se utilizar o filtro Mediana para tratar ru√≠dos impulsivos e o Bilateral quando for essencial preservar as bordas da imagem. O Gaussiano apresenta um bom equil√≠brio entre suaviza√ß√£o e nitidez, enquanto o filtro de M√©dia deve ser aplicado com cuidado, pois tende a provocar borramento excessivo. A sele√ß√£o do tamanho do kernel deve levar em conta a intensidade do ru√≠do presente e o grau de fidelidade visual desejado na imagem final.</p>
      </div>
      </div>
      </details>
      
      
        <!-- RELAT√ìRIO 3 -->
  <details>
    <summary>Relat√≥rio 3</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 22/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
      <p style="text-align:justify;">O presente experimento tem como objetivo estudar a representa√ß√£o e convers√£o entre diferentes espa√ßos de cores utilizando a biblioteca OpenCV. A compreens√£o desses espa√ßos √© essencial no processamento de imagens e vis√£o computacional, pois muitas opera√ß√µes de segmenta√ß√£o, filtragem e detec√ß√£o de objetos dependem da forma como as cores s√£o representadas numericamente.</p>

 <p style="text-align:justify;">Inicialmente, s√£o exploradas as principais convers√µes entre espa√ßos de cores ‚Äî como RGB ‚Üî GRAY, RGB ‚Üî YCrCb, RGB ‚Üî HSV e Bayer ‚Üí RGB ‚Äî por meio das fun√ß√µes de convers√£o de cor disponibilizadas pelo OpenCV. Em seguida, √© realizado um experimento pr√°tico de mudan√ßa de espa√ßo RGB para HSV, com captura em tempo real via webcam, permitindo observar o comportamento de diferentes objetos coloridos e aplicar a fun√ß√£o inRange() para segmentar faixas espec√≠ficas de cores.</p>

 <p style="text-align:justify;">Na sequ√™ncia, o experimento √© expandido com a aplica√ß√£o de um filtro Gaussiano, comparando o efeito do pr√©-processamento na detec√ß√£o de cores. Posteriormente, o detector de bordas Canny √© implementado sobre as imagens filtradas, possibilitando analisar o impacto dos par√¢metros de detec√ß√£o e a resposta do sistema diante de diferentes objetos e n√≠veis de ru√≠do.</p>

 <p style="text-align:justify;">Por fim, s√£o realizadas modifica√ß√µes adicionais para salvar imagens e gravar v√≠deos do processo de rastreamento, al√©m da elabora√ß√£o de um programa capaz de identificar m√∫ltiplas cores simultaneamente (como vermelho, verde e azul), consolidando os conceitos de espa√ßo de cores, segmenta√ß√£o e detec√ß√£o de bordas.</p>

 <p style="text-align:justify;">Essas etapas visam fortalecer o entendimento pr√°tico sobre a manipula√ß√£o de imagens coloridas, destacando a import√¢ncia da escolha adequada do espa√ßo de cor e dos m√©todos de filtragem para aplica√ß√µes de vis√£o computacional em tempo real.</p>
 

  <section id="fundamentos-basicos" style="font-family: Arial, Helvetica, sans-serif; line-height: 1.6; color: #222; max-width: 900px; margin: 24px auto; background: #fff; padding: 24px; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,.06); text-align: justify;">    
  <h3>2. Fundamentos B√°sicos</h3>
  <p>
    As imagens digitais s√£o formadas por <strong>pixels</strong>, e cada pixel possui valores num√©ricos que representam a
    <strong>intensidade</strong> e a <strong>composi√ß√£o de cor</strong>. Esses valores s√£o interpretados conforme o
    <strong>espa√ßo de cores</strong> adotado ‚Äî um modelo matem√°tico que descreve como as cores s√£o representadas e manipuladas
    em aplica√ß√µes de processamento de imagem.
  </p>

  <h3>1) Espa√ßo de Cores RGB</h3>
  <p>
    O espa√ßo <strong>RGB (Red, Green, Blue)</strong> √© o mais comum em c√¢meras e monitores, onde cada pixel √© composto pela
    combina√ß√£o dos tr√™s canais de cor. Embora intuitivo, o RGB n√£o √© ideal para todas as aplica√ß√µes de vis√£o computacional:
    pequenas varia√ß√µes de <em>ilumina√ß√£o</em> podem alterar significativamente os valores dos canais, dificultando a segmenta√ß√£o
    de cores espec√≠ficas.
  </p>

  <h3>2) Espa√ßo de Cores HSV</h3>
  <p>
    O modelo <strong>HSV (Hue, Saturation, Value)</strong> √© frequentemente usado para <strong>segmenta√ß√£o e rastreamento de cores</strong>,
    pois separa as informa√ß√µes de <strong>matiz (Hue)</strong>, <strong>satura√ß√£o (S)</strong> e <strong>valor/brilho (V)</strong>:
  </p>
  <ul style="margin-top: 8px;">
    <li><strong>Hue (Matiz):</strong> define a cor propriamente dita (vermelho, verde, azul etc.).</li>
    <li><strong>Saturation (Satura√ß√£o):</strong> grau de pureza/intensidade da cor.</li>
    <li><strong>Value (Valor):</strong> brilho/luminosidade da cor.</li>
  </ul>
  <p>
    Essa separa√ß√£o torna o HSV mais <strong>robusto √† varia√ß√£o de ilumina√ß√£o</strong>, facilitando a detec√ß√£o de cores espec√≠ficas por meio
    de limiares definidos com <code>inRange()</code> (OpenCV).
  </p>

  <h3>3) Convers√µes de Espa√ßo de Cor</h3>
  <p>
    O OpenCV fornece fun√ß√µes diretas para convers√£o entre espa√ßos de cor via <code>cvtColor()</code>. As convers√µes mais usadas
    neste laborat√≥rio incluem:
  </p>
  <ul style="margin-top: 8px;">
    <li><code>COLOR_BGR2GRAY</code> ‚Üí converte imagem colorida para tons de cinza.</li>
    <li><code>COLOR_BGR2HSV</code> ‚Üí converte para HSV (√∫til na segmenta√ß√£o por cor).</li>
    <li><code>COLOR_BGR2YCrCb</code> ‚Üí separa lumin√¢ncia (Y) e cromin√¢ncia (Cr, Cb), √∫til em compress√£o de v√≠deo e detec√ß√£o de pele.</li>
  </ul>

  <h3>4) Segmenta√ß√£o de Cores com <code>inRange()</code></h3>
  <p>
    A fun√ß√£o <code>inRange()</code> realiza uma opera√ß√£o de <strong>limiariza√ß√£o</strong>, gerando uma m√°scara bin√°ria:
    pixels com HSV dentro da faixa definida (m√≠n‚Äìm√°x) tornam-se <strong>brancos (255)</strong>; os demais, <strong>pretos (0)</strong>.
    Essa t√©cnica √© fundamental para <strong>isolar objetos coloridos</strong> e ser√° usada nas etapas de detec√ß√£o/rastreamento.
  </p>

  <h3>5) Filtragem Gaussiana</h3>
  <p>
    O <strong>filtro Gaussiano</strong> √© um m√©todo de suaviza√ß√£o que reduz ru√≠dos de alta frequ√™ncia, tornando bordas e √°reas de cor
    mais homog√™neas. Aplic√°-lo <em>antes</em> da convers√£o para HSV ajuda a evitar que ru√≠dos/reflexos causem erros na segmenta√ß√£o.
  </p>

  <h3>6) Detector de Bordas Canny</h3>
  <p>
    O <strong>detector de Canny</strong> identifica bordas analisando gradientes de intensidade. Ele combina suaviza√ß√£o, c√°lculo de derivadas e
    limiares duplos, permitindo contornos <strong>n√≠tidos e bem definidos</strong>. No contexto do laborat√≥rio, aplica-se o Canny sobre a imagem
    filtrada (Gaussiano) para visualizar contornos dos objetos segmentados.
  </p>

  <hr style="border:none; border-top:1px solid #ddd; margin: 20px 0;">

  <p style="font-size: 0.95rem;">
    <strong>Em s√≠ntese:</strong> a escolha adequada do espa√ßo de cor (HSV, YCrCb etc.) e o uso combinado de
    <em>filtragem</em> (Gaussiano), <em>convers√µes</em> (cvtColor) e <em>detec√ß√£o</em> (inRange, Canny) s√£o decisivos para
    obter segmenta√ß√µes est√°veis e contornos consistentes em aplica√ß√µes de vis√£o computacional em tempo real.
  </p>
</section>

      <section id="materiais-metodos" style="font-family: Arial, Helvetica, sans-serif; line-height: 1.6; color: #222; text-align: justify;">
  <h3>3. Materiais e M√©todos</h3>
  <p>
    O experimento foi realizado em computador com Linux e webcam integrada, utilizando Python 3.10 e as bibliotecas OpenCV e NumPy.
    Investigou-se convers√£o de espa√ßos de cor, filtragens e detec√ß√£o de bordas em tempo real.
  </p>
  <p>
    O programa capturou frames da webcam (<code>cv.VideoCapture(0)</code>) e, opcionalmente, aplicou filtro Gaussiano
    (<code>cv.GaussianBlur</code>) para reduzir ru√≠do antes da convers√£o de cor. Em seguida, a imagem foi convertida de
    RGB para HSV (<code>cv.cvtColor</code>), por ser mais est√°vel √† varia√ß√£o de ilumina√ß√£o e favorecer a segmenta√ß√£o.
  </p>
  <p>
    Para isolar regi√µes por cor, empregou-se <code>cv.inRange()</code> na faixa HSV definida; quando necess√°rio, m√°scaras para m√∫ltiplas cores
    (p. ex., vermelho, verde e azul) foram combinadas por opera√ß√µes l√≥gicas. Opcionalmente, aplicou-se o detector de bordas de Canny
    (<code>cv.Canny</code>) para real√ßar contornos.
  </p>
  <p>
    O sistema exibiu janelas com a imagem original, HSV, m√°scaras e bordas; imagens e v√≠deos puderam ser salvos via
    <code>cv.imwrite()</code> e <code>cv.VideoWriter()</code>. A execu√ß√£o foi encerrada ao pressionar a tecla <strong>q</strong>.
  </p>
</section>

      
<p><strong>Diagrama de bloco</strong></p>
<figure style="max-width: 720px; margin: 16px auto; text-align: center;">
  <img src="imagens/fluxograma_rel3.png"
       loading="lazy"
       style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
  <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
    Diagrama de blocos representando o fluxo de execu√ß√£o do programa de convers√£o de cores, filtragem e detec√ß√£o de bordas no OpenCV
  </figcaption>
</figure>
      
    <h3>4. Resultados e An√°lises</h3>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Aplica√ß√£o do Filtro Gaussiano</h4>
    <p style="text-align:justify;">
      <p style="text-align:justify;">
        Neste experimento, o programa foi modificado para aplicar um <b>Filtro Gaussiano</b> √† imagem capturada pela webcam antes da convers√£o para o espa√ßo de cores HSV.
        Al√©m disso, foi criada uma janela adicional exibindo lado a lado a imagem n√£o filtrada e a filtrada. 
        O filtro suavizou a imagem e reduziu ru√≠dos visuais, resultando em uma detec√ß√£o mais est√°vel das cores.
        Foram utilizados tr√™s objetos coloridos: dourado, laranja e preto.
      </p>
      
      <p style="text-align:justify;">
        Durante a an√°lise, observou-se que o dourado foi identificado com uma tonalidade amarelo-esverdeada, devido √† alta sensibilidade do canal de satura√ß√£o no espa√ßo HSV.
        Sua faixa t√≠pica foi aproximadamente H = [20, 35], S = [100, 255] e V = [120, 255], correspondendo a um valor m√©dio RGB ‚âà (180, 200, 60).
        J√° o objeto laranja apresentou um tom ligeiramente amarelado ap√≥s o processamento, com faixa H = [10, 25], S = [150, 255] e V = [150, 255], e valor m√©dio RGB ‚âà (220, 170, 50).
        Por fim, o objeto preto refletiu pequenas varia√ß√µes de ilumina√ß√£o, sendo detectado como um verde-√°gua escurecido em alguns quadros, com faixa H = [70, 90], S = [50, 150] e V = [30, 80], e valor m√©dio RGB ‚âà (40, 60, 50).
        Essas distor√ß√µes de cor s√£o comuns em ambientes com ilumina√ß√£o oscilante e demonstram a influ√™ncia direta da luz e do ru√≠do √≥ptico sobre a estabilidade do modelo HSV.
      </p>
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de Tela 2025-10-18 193202.mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando a aplica√ß√£o do Filtro Gaussiano.
      </figcaption>
    </figure>   

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Detector de Bordas Canny</h4>
    <p style="text-align:justify;">
      O segundo experimento consistiu em aplicar o <b>Detector Canny</b> sobre a imagem filtrada. 
      O programa passou a exibir duas janelas: uma com a imagem filtrada e outra mostrando as bordas detectadas pelo m√©todo Canny. 
      Utilizando os mesmos objetos (dourado, laranja e preto), foi poss√≠vel observar que apenas seus contornos eram destacados, permitindo identificar suas formas com nitidez. 
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de tela de 2025-10-15 11-12-56.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela exibindo o programa ap√≥s aplica√ß√£o do Detector Canny.
      </figcaption>
    </figure>   

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Salvar e gravar Imagens/V√≠deos</h4>
    <p style="text-align:justify;">
      Nesta vers√£o, o programa foi aprimorado para incluir as fun√ß√µes de salvar imagens (tecla [s]) e gravar v√≠deos (teclas [k] para iniciar e [h] para encerrar).
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de tela de 2025-10-15 11-28-11.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando o funcionamento das teclas [s], [k] e [h].
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/captura_lab3c_20251015_112839.jpg"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem salva a partir da tecla [s]
      </figcaption>
    </figure>

    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/video_lab3c_MJPG_20251015_112840.mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela a partir das teclas [k] e [h].
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Extra√ß√£o de mais de um objeto colorido</h4>
    <p style="text-align:justify;">
      Neste exerc√≠cio, o programa foi modificado para detectar e extrair mais de um objeto colorido simultaneamente.
      Foram utilizados objetos com diferentes cores, e o sistema foi capaz de identificar e rastrear cada um de forma independente.
      Durante o experimento, observou-se que os itens coloridos tra√ßavam um caminho vis√≠vel na tela, com cada cor destacada individualmente.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de tela de 2025-10-15 11-33-04.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela demonstrando o rastreamento de objetos coloridos.
      </figcaption>
    </figure>   
    
    <h3>5. Conclus√µes</h3>
     <p style="text-align:justify;">
        Este relat√≥rio permitiu compreender de forma pr√°tica a influ√™ncia das etapas de filtragem, detec√ß√£o e rastreamento na an√°lise de imagens em tempo real. 
        Inicialmente, a aplica√ß√£o do filtro Gaussiano demonstrou-se fundamental para reduzir ru√≠dos e suavizar as transi√ß√µes de cor, possibilitando uma detec√ß√£o mais est√°vel no espa√ßo HSV. 
        Observou-se que o dourado assumiu tonalidades amarelo-esverdeadas, o laranja tornou-se mais amarelado e o preto apresentou colora√ß√£o de verde-√°gua, o que evidenciou o impacto da ilumina√ß√£o e da suaviza√ß√£o no comportamento das cores. 
        As faixas HSV e os valores RGB obtidos mostraram-se coerentes com as varia√ß√µes √≥pticas esperadas em ambiente real.
      </p>
      
      <p style="text-align:justify;">
        Na sequ√™ncia, a implementa√ß√£o do detector de bordas Canny possibilitou real√ßar os contornos dos objetos identificados, destacando formas e limites com precis√£o. 
        Posteriormente, a adi√ß√£o das fun√ß√µes de salvamento de imagens e grava√ß√£o de v√≠deos ampliou as possibilidades de an√°lise e documenta√ß√£o dos experimentos, tornando o sistema mais interativo e funcional. 
        Essas ferramentas permitiram registrar o comportamento din√¢mico da detec√ß√£o e avaliar o desempenho do programa ao longo do tempo.
      </p>
      
      <p style="text-align:justify;">
        Por fim, o exerc√≠cio de extra√ß√£o de m√∫ltiplos objetos coloridos consolidou o aprendizado das etapas anteriores, possibilitando rastrear e distinguir simultaneamente diferentes cores em movimento. 
        O sistema foi capaz de acompanhar o deslocamento de cada objeto e representar visualmente o caminho percorrido, refor√ßando o entendimento da segmenta√ß√£o por faixas HSV.
        De forma geral, o conjunto dos experimentos evidenciou a evolu√ß√£o progressiva do sistema, desde o tratamento inicial da imagem at√© a an√°lise multiobjeto. 
      </p>

    </div>
    </div>
  </details>
    
      <!-- RELAT√ìRIO 4 -->
  <details>
    <summary>Relat√≥rio 4</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 27/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
        <p style="text-align:justify;">Neste laborat√≥rio, estudamos as t√©cnicas de processamento de imagens baseadas em histogramas e 
    opera√ß√µes de limiariza√ß√£o (binariza√ß√£o), utilizando a biblioteca OpenCV em linguagem C++. 
    O objetivo principal √© compreender como a distribui√ß√£o dos n√≠veis de intensidade de uma imagem influencia sua qualidade visual 
    e como m√©todos de equaliza√ß√£o e limiariza√ß√£o podem ser empregados para real√ßar contrastes e extrair informa√ß√µes relevantes.</p>
      

    <p style="text-align:justify;">O histograma de uma imagem representa a distribui√ß√£o estat√≠stica dos tons de cinza (ou de cores) presentes, 
    sendo uma ferramenta fundamental na an√°lise e no pr√©-processamento de imagens digitais. 
    A equaliza√ß√£o de histograma busca redistribuir esses n√≠veis de intensidade de forma mais uniforme, 
    aumentando o contraste global e evidenciando detalhes antes pouco vis√≠veis. 
    Essa t√©cnica √© amplamente utilizada em aplica√ß√µes de vis√£o computacional e reconhecimento de padr√µes, 
    especialmente em condi√ß√µes de ilumina√ß√£o n√£o uniforme.</p>


    <p style="text-align:justify;">Complementarmente, a limiariza√ß√£o (thresholding) √© uma opera√ß√£o que converte imagens em tons de cinza em imagens bin√°rias, 
    segmentando regi√µes de interesse com base em um valor de limiar. 
    Essa t√©cnica √© essencial em tarefas de detec√ß√£o de objetos, reconhecimento facial e inspe√ß√£o industrial. 
    Quando aplicada ap√≥s a equaliza√ß√£o de histograma, a limiariza√ß√£o tende a produzir resultados mais robustos, 
    pois os n√≠veis de intensidade est√£o melhor distribu√≠dos.</p>

  <p>Durante o experimento, foram desenvolvidos programas progressivos conforme o roteiro proposto:</p>
  <ul>
    <li>
      <strong>Equaliza√ß√£o de histograma</strong> em imagens individuais convertidas para tons de cinza, 
      com salvamento dos histogramas antes e depois da equaliza√ß√£o.
    </li>
    <li>
      <strong>Equaliza√ß√£o em tempo real</strong> utilizando a webcam, exibindo simultaneamente a imagem em cinza e a equalizada.
    </li>
    <li>
      <strong>Limiariza√ß√£o e binariza√ß√£o</strong> aplicadas ap√≥s a equaliza√ß√£o, para avaliar o impacto dessa etapa 
      na qualidade da segmenta√ß√£o.
    </li>
    <li>
      <strong>Equaliza√ß√£o em imagens coloridas</strong>, aplicando o processo separadamente em cada canal (B, G, R) 
      e analisando o efeito visual e estat√≠stico sobre o histograma resultante.
    </li>
  </ul>

  <p>
    Esses experimentos possibilitam compreender o papel da equaliza√ß√£o e da limiariza√ß√£o no realce e na segmenta√ß√£o de imagens, 
    fornecendo uma base s√≥lida para aplica√ß√µes mais complexas de processamento e an√°lise visual.
  </p>
    
      <h3>2. Fundamentos B√°sicos</h3>
      <p style="text-align:justify;"> Os histogramas s√£o representa√ß√µes gr√°ficas da distribui√ß√£o de intensidade dos pixels de uma imagem.
    No caso de imagens em tons de cinza, o eixo horizontal representa os n√≠veis de intensidade (de 0 a 255),
    enquanto o eixo vertical indica a frequ√™ncia com que cada n√≠vel ocorre. 
    Assim, o histograma permite identificar se uma imagem est√° muito escura (valores concentrados √† esquerda),
    muito clara (valores concentrados √† direita) ou com baixo contraste (valores concentrados em uma faixa estreita).</p>
    
   <p style="text-align:justify;">  A equaliza√ß√£o de histograma √© uma t√©cnica de realce de contraste que visa redistribuir de forma mais uniforme 
    os n√≠veis de intensidade dos pixels, expandindo a faixa tonal da imagem. 
    Isso √© feito ajustando a <em>fun√ß√£o de distribui√ß√£o acumulada</em> (CDF) do histograma, 
    de modo que os valores de intensidade sejam mapeados para um intervalo mais amplo, 
    real√ßando detalhes em regi√µes claras e escuras. 
    Em OpenCV, essa opera√ß√£o pode ser realizada com a fun√ß√£o <code>equalizeHist()</code>. </p>

     <p style="text-align:justify;">Em imagens coloridas, a equaliza√ß√£o pode ser aplicada separadamente a cada canal de cor (B, G, R), 
    ou sobre o canal de lumin√¢ncia em um espa√ßo de cor como YCrCb ou HSV. 
    No entanto, equalizar cada canal RGB de forma independente pode causar distor√ß√µes crom√°ticas percept√≠veis, 
    sendo mais indicado equalizar apenas o componente de brilho. </p>

     <p style="text-align:justify;">A limiariza√ß√£o</strong> (ou <em>thresholding</em>) √© um m√©todo de <em>segmenta√ß√£o de imagem</em> que converte pixels em tons de cinza 
    em apenas dois n√≠veis: preto (0) e branco (255), com base em um valor de limiar (<em>threshold</em>) definido pelo usu√°rio ou calculado automaticamente. 
    Em OpenCV, a fun√ß√£o <code>threshold()</code> oferece diversos modos, como: </p>

  <ul>
    <li><strong>THRESH_BINARY:</strong> pixels acima do limiar tornam-se brancos; abaixo, pretos.</li>
    <li><strong>THRESH_BINARY_INV:</strong> invers√£o da binariza√ß√£o tradicional.</li>
    <li><strong>THRESH_TRUNC:</strong> valores acima do limiar s√£o truncados para o valor do limiar.</li>
    <li><strong>THRESH_TOZERO:</strong> valores abaixo do limiar tornam-se zero.</li>
    <li><strong>THRESH_OTSU:</strong> m√©todo autom√°tico que determina o limiar √≥timo com base na minimiza√ß√£o da vari√¢ncia intra-classe.</li>
  </ul>

  <p>
    Quando a limiariza√ß√£o √© precedida pela equaliza√ß√£o de histograma, os n√≠veis de intensidade ficam mais bem distribu√≠dos, 
    facilitando a separa√ß√£o entre regi√µes claras e escuras e resultando em uma binariza√ß√£o mais eficiente e menos sens√≠vel 
    √†s varia√ß√µes de ilumina√ß√£o.
  </p>

  <p>
    Em resumo, a combina√ß√£o das t√©cnicas de equaliza√ß√£o e limiariza√ß√£o fornece uma poderosa abordagem 
    para o pr√©-processamento de imagens, melhorando a visibilidade de detalhes e preparando os dados para etapas 
    subsequentes de an√°lise, como detec√ß√£o de bordas, reconhecimento de padr√µes e segmenta√ß√£o de objetos.
  </p>  
    
      <h3>3. Materiais e M√©todos</h3>
     <p style="text-align:justify;">
Para a realiza√ß√£o do laborat√≥rio foram utilizados recursos de hardware e software necess√°rios para o processamento e an√°lise de imagens em tempo real. 
O ambiente de desenvolvimento adotado foi o Visual Studio Code, com a linguagem C++ e a biblioteca OpenCV (vers√£o 4.x)</strong>, 
amplamente empregada em aplica√ß√µes de vis√£o computacional. Em alguns experimentos, tamb√©m foi utilizada a webcam do computador para captura de imagens em tempo real.
</p>

<p style="text-align:justify;">
As principais fun√ß√µes do OpenCV utilizadas foram:
</p>

<ul style="text-align:justify;">
  <li><code>imread()</code> e <code>imshow()</code> para leitura e exibi√ß√£o das imagens;</li>
  <li><code>cvtColor()</code> para convers√£o de imagem colorida para tons de cinza;</li>
  <li><code>calcHist()</code> para c√°lculo do histograma;</li>
  <li><code>equalizeHist()</code> para equaliza√ß√£o de histograma em imagens em tons de cinza;</li>
  <li><code>split()</code> e <code>merge()</code> para separar e recombinar os canais de cor (B, G, R);</li>
  <li><code>threshold()</code> para aplicar a limiariza√ß√£o;</li>
  <li><code>imwrite()</code> para salvar as imagens processadas;</li>
  <li><code>waitKey()</code> para controle de execu√ß√£o e captura de tecla.</li>
</ul>

<p style="text-align:justify;">
As imagens utilizadas foram fotos individuais dos integrantes do grupo e objetos coloridos, capturadas tanto a partir de arquivos quanto diretamente pela c√¢mera. 
O processamento foi realizado em quatro etapas principais, conforme o roteiro experimental:
</p>

<ol style="text-align:justify;">
  <li><strong>Equaliza√ß√£o em tons de cinza:</strong> convers√£o da imagem original para escala de cinza, c√°lculo do histograma e aplica√ß√£o da equaliza√ß√£o para realce de contraste.</li>
  <li><strong>Equaliza√ß√£o em tempo real:</strong> aplica√ß√£o da equaliza√ß√£o de histograma sobre o fluxo cont√≠nuo da webcam, exibindo simultaneamente a imagem em cinza e a equalizada.</li>
  <li><strong>Limiariza√ß√£o (binariza√ß√£o):</strong> inser√ß√£o de um valor de limiar fixo ou calculado automaticamente (m√©todo de Otsu) ap√≥s a equaliza√ß√£o, gerando uma imagem bin√°ria para segmenta√ß√£o.</li>
  <li><strong>Equaliza√ß√£o em imagens coloridas:</strong> equaliza√ß√£o aplicada separadamente nos tr√™s canais de cor (B, G, R), com posterior recombina√ß√£o e compara√ß√£o visual com a imagem original.</li>
</ol>

<p style="text-align:justify;">
Durante todo o processo, foram geradas e salvas as imagens intermedi√°rias (original, equalizada, binarizada) e os gr√°ficos de histograma correspondentes. 
Os resultados obtidos em cada etapa permitiram comparar o efeito da equaliza√ß√£o e da limiariza√ß√£o, tanto em imagens est√°ticas quanto din√¢micas, 
avaliando a melhoria de contraste, a nitidez dos contornos e a efici√™ncia da segmenta√ß√£o.
</p>

<h3>Diagrama de Blocos</h3>
<figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/lab4.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos do processo de equaliza√ß√£o e limiariza√ß√£o de imagens no OpenCV.
  </figcaption>
</figure>

           
      <h3>4. Resultados e An√°lises</h3>
      <p style="text-align:justify;"><strong>(1) </strong> Desenvolva um programa para fazer a leitura de sua imagem, convertendo para tons de cinza, calcular o histograma, e realizar a equaliza√ß√£o do histograma. Ao toque de uma tecla, o programa deve salvar a imagem de entrada em cinza e a imagem equalizada, atrav√©s de comando OpenCV. Salve tamb√©m as imagens dos gr√°ficos de histograma antes e depois da equaliza√ß√£o.Realize este experimento com as imagens individuais, separadamente, de cada integrante do grupo.
</p>
      
      <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Fernanda.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Fernanda.
  </figcaption>
</figure>
      
            <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Gabriel.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

      <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Ingrid.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Ingrid.
  </figcaption>
</figure>

      <p style="text-align:justify;"><strong>(2)</strong> Elabore outro programa modificando o c√≥digo do item (1), agora fazendo a leitura de imagem da webcam. 
Neste caso, o programa deve adicionalmente mostrar uma janela ao vivo com a imagem cinza e o resultado da imagem equalizada.
Realize este experimento com cada integrante do grupo e com um objeto colorido.</p>
      
         <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-27-22.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>   
      
        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-10.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Fernanda.
  </figcaption>
</figure>

        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-21.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Ingrid.
  </figcaption>
</figure>

        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-36.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Objeto colorido.
  </figcaption>
</figure>

      <p style="text-align:justify;"><strong>(3)</strong> Estudo da binariza√ß√£o. Desenvolva outro programa modificando o item (2), incluindo a limiariza√ß√£o da imagem do tutorial acima, para obten√ß√£o de uma imagem bin√°ria. Adicione a Equaliza√ß√£o de Histograma antes da limiariza√ß√£o.  Realize este experimento com pessoas e com objetos. Compare e analise o efeito sem e com a equaliza√ß√£o de histograma no resultado da imagem bin√°ria.</p>
      
          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-35-27.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-35-37.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel com objeto colorido.
  </figcaption>
</figure>
      
      <p style="text-align:justify;"><strong>(4)</strong> Estudo da equaliza√ß√£o nas cores: Elabore outro programa modificando o item (2), por√©m sem converter para cinza, mas agora realizando a equaliza√ß√£o nas tr√™s cores separadamente, e juntar os canais equalizados na imagem colorida de sa√≠da. Realize este experimento com pessoas e com objetos coloridos. Compare e analise o efeito sem e com a equaliza√ß√£o de histograma no resultado da imagem colorida gerada e no histograma.</p>
      
               <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-39-03.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-39-14.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel com objeto colorido.
  </figcaption>
</figure>
      
      <h3>5. Conclus√µes</h3>
      <p style="text-align:justify;">O Laborat√≥rio 4 permitiu compreender, de forma pr√°tica e experimental, como o histograma √© uma ferramenta fundamental para a an√°lise e o processamento de imagens digitais. Por meio dos exerc√≠cios realizados, foi poss√≠vel observar como a equaliza√ß√£o de histograma melhora o contraste de imagens com faixas de intensidade concentradas, tornando detalhes antes impercept√≠veis mais evidentes.</p>
         <p style="text-align:justify;">Na primeira etapa, ao converter imagens para tons de cinza e aplicar a equaliza√ß√£o, notou-se a redistribui√ß√£o dos n√≠veis de brilho, o que resultou em uma apar√™ncia visual mais equilibrada. J√° na segunda etapa, a execu√ß√£o em tempo real com a webcam possibilitou visualizar instantaneamente o efeito da equaliza√ß√£o em diferentes condi√ß√µes de ilumina√ß√£o e em diferentes indiv√≠duos e objetos, demonstrando a aplicabilidade do m√©todo em sistemas de vis√£o computacional din√¢micos.</p>
         <p style="text-align:justify;">Com o estudo da binariza√ß√£o, foi poss√≠vel compreender a import√¢ncia da limiariza√ß√£o e o impacto da equaliza√ß√£o pr√©via do histograma sobre o resultado da segmenta√ß√£o. As imagens bin√°rias com equaliza√ß√£o apresentaram bordas mais n√≠tidas e separa√ß√£o mais eficiente entre fundo e objeto, especialmente em cenas com pouca ilumina√ß√£o ou contraste reduzido.</p>
         <p style="text-align:justify;">Por fim, a an√°lise da equaliza√ß√£o em imagens coloridas, com a aplica√ß√£o individual da t√©cnica em cada canal RGB, evidenciou o aumento de contraste, mas tamb√©m poss√≠veis distor√ß√µes nas tonalidades originais. Isso refor√ßou a import√¢ncia de aplicar a equaliza√ß√£o com crit√©rio, considerando o equil√≠brio entre realce visual e preserva√ß√£o da fidelidade das cores.</p>
          <p style="text-align:justify;">Em s√≠ntese, o laborat√≥rio proporcionou uma vis√£o completa do uso dos histogramas e da limiariza√ß√£o em processamento digital de imagens, demonstrando que essas t√©cnicas s√£o fundamentais para etapas posteriores de detec√ß√£o, segmenta√ß√£o e reconhecimento de padr√µes em sistemas de vis√£o computacional.</p>
      
      
      
    </div>
    </div>
  </details>

      <!-- RELAT√ìRIO 5 -->
  <details>
    <summary>Relat√≥rio 5</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 29/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
      <p style="text-align:justify;">O presente laborat√≥rio tem como objetivo estudar e implementar t√©cnicas de subtra√ß√£o de fundo e detec√ß√£o de movimento utilizando a biblioteca OpenCV em C++. A subtra√ß√£o de fundo √© uma das etapas fundamentais em sistemas de vis√£o computacional, permitindo identificar objetos em movimento dentro de uma cena est√°tica. Essa t√©cnica baseia-se na compara√ß√£o entre um modelo de fundo previamente aprendido e os quadros atuais do v√≠deo, destacando as regi√µes que sofrem altera√ß√µes ‚Äî geralmente correspondentes a pessoas, ve√≠culos ou objetos em deslocamento.</p>

<p style="text-align:justify;">Inicialmente, √© proposto o estudo do tutorial oficial do OpenCV ‚ÄúHow to Use Background Subtraction Methods‚Äù, bem como a execu√ß√£o do c√≥digo de exemplo bg_sub.cpp com o v√≠deo vtest.avi, utilizando os modos MOG2 e KNN. A partir dessa base, foram desenvolvidos dois experimentos pr√°ticos: no primeiro, um programa realiza a subtra√ß√£o de fundo sobre v√≠deos gravados em aula, contendo movimentos lentos e r√°pidos, gerando uma sa√≠da que exibe apenas os elementos m√≥veis e grava o resultado em arquivo. No segundo experimento, o c√≥digo √© adaptado para leitura direta da webcam, exibindo simultaneamente a imagem original e a imagem processada em tempo real, permitindo observar o desempenho da t√©cnica com pessoas e objetos coloridos.</p>

<p style="text-align:justify;">Por fim, o laborat√≥rio prop√µe uma breve pesquisa sobre aplica√ß√µes pr√°ticas dessas t√©cnicas em √°reas como vigil√¢ncia, transporte, automa√ß√£o e intera√ß√£o humano-computador, al√©m de uma leitura complementar sobre rastreamento de objetos baseado em aprendizado profundo (GOTURN). Com isso, busca-se compreender tanto os fundamentos te√≥ricos quanto as aplica√ß√µes reais da subtra√ß√£o de fundo e da detec√ß√£o de movimento em sistemas inteligentes de vis√£o computacional.</p>

      <h3>2. Fundamentos B√°sicos</h3>
      
       <p style="text-align:justify;"> A subtra√ß√£o de fundo √© uma t√©cnica essencial em vis√£o computacional para detectar
      regi√µes em movimento em sequ√™ncias de v√≠deo. O princ√≠pio √© comparar cada novo
      quadro (frame) com um modelo de fundo (a cena est√°tica). Diferen√ßas
      significativas entre o frame atual e o modelo s√£o classificadas como primeiro plano
      (objetos em movimento), gerando uma m√°scara bin√°ria que separa
      foreground e background. </p>
     <p style="text-align:justify;">O desempenho pode ser afetado por ru√≠do, varia√ß√µes de ilumina√ß√£o,sombras e
      fundo din√¢mico (por exemplo, √°rvores ao vento). Por isso, adotam-se modelos que
     se adaptam no tempo, atualizando continuamente o fundo.</p>

    <h3>Modelos no OpenCV</h3>
    <ul>
      <li>
        <strong>MOG2 (Mixture of Gaussians 2)</strong>: modela cada pixel como mistura de gaussianas.
        √â robusto a mudan√ßas suaves de ilumina√ß√£o e pode <strong>rotular sombras</strong> na m√°scara (valores intermedi√°rios).
      </li>
      <li>
        <strong>KNN (K-Nearest Neighbors)</strong>: usa uma janela temporal de amostras recentes para
        classificar o pixel como fundo ou primeiro plano. Adapta-se rapidamente a mudan√ßas bruscas
        e funciona bem com <strong>fundos din√¢micos</strong>.
      </li>
    </ul>

    <h3>P√≥s-processamento</h3>
     <p style="text-align:justify;"> Ap√≥s obter a m√°scara, aplicam-se opera√ß√µes morfol√≥gicas(abertura/fechamento) e
     limiariza√ß√£o para remover ru√≠do e refinar contornos. Em seguida, a m√°scara pode ser
      sobreposta ao v√≠deo para visualizar apenas as √°reas m√≥veis ou alimentar etapas como
      rastreamento, contagem de objetos/pessoas e an√°lise de padr√µes.
    </p>

     <p style="text-align:justify;">Essas t√©cnicas s√£o base de aplica√ß√µes em <strong>vigil√¢ncia inteligente</strong>, <strong>controle de tr√°fego</strong>,
      <strong>automa√ß√£o industrial</strong> e <strong>intera√ß√£o humano-computador</strong>, possibilitando monitoramento
      e decis√µes em tempo real. </p>
      
      <h3>3. Materiais e M√©todos</h3>
      <p style="text-align:justify;">Para o desenvolvimento deste laborat√≥rio, foi utilizada a linguagem C++ com a biblioteca OpenCV 4.x, al√©m de v√≠deos de teste e imagens capturadas por webcam. O estudo te√≥rico foi baseado no tutorial oficial do OpenCV ‚ÄúHow to Use Background Subtraction Methods‚Äù, que apresenta o funcionamento dos algoritmos MOG2 e KNN, amplamente usados em aplica√ß√µes de detec√ß√£o de movimento.</p>
      

 <p style="text-align:justify;">Inicialmente, foi criada a pasta lab5, contendo o c√≥digo de exemplo bg_sub.cpp e o v√≠deo de teste vtest.avi, disponibilizados no reposit√≥rio oficial do OpenCV. O programa foi compilado e executado nos modos MOG2 e KNN, permitindo observar as diferen√ßas entre os dois m√©todos de subtra√ß√£o de fundo e compreender o comportamento de cada um frente a diferentes tipos de movimento e ilumina√ß√£o.</p>

<p style="text-align:justify;">Com base nesse estudo, foi desenvolvido um programa pr√≥prio em C++ capaz de ler v√≠deos gravados em aula, aplicar a subtra√ß√£o de fundo e exibir apenas os elementos em movimento. O sistema tamb√©m grava automaticamente o v√≠deo processado, destacando o primeiro plano. Em seguida, o c√≥digo foi modificado para funcionar com a webcam, exibindo em tempo real duas janelas: a imagem original e a imagem resultante do processamento. Durante os testes, foram realizados experimentos com pessoas e com objetos coloridos, observando o comportamento do algoritmo em movimentos lentos e r√°pidos.</p>

<p style="text-align:justify;">Durante o processamento, foram aplicadas t√©cnicas de limiariza√ß√£o e opera√ß√µes morfol√≥gicas (abertura e fechamento) para reduzir ru√≠dos e melhorar a qualidade da m√°scara de movimento. Os par√¢metros learningRate, varThreshold e history foram ajustados empiricamente para equilibrar a sensibilidade da detec√ß√£o e a estabilidade do modelo de fundo.</p>

<p style="text-align:justify;">Por fim, foram comparados os resultados obtidos com os m√©todos MOG2 e KNN, analisando sua efici√™ncia, estabilidade e adapta√ß√£o ao fundo. As observa√ß√µes foram registradas em v√≠deos e imagens, permitindo avaliar o desempenho de cada abordagem em diferentes condi√ß√µes de movimento e ilumina√ß√£o.</p>

   <h3>Diagrama de blocos</h3>
   <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/diagrama4.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de subtra√ß√£o de fundo e detec√ß√£o de movimento no OpenCV.
  </figcaption>
</figure>
      
      
      <h3>4. Resultados e An√°lises</h3>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>V√≠deos com movimento lento e com movimento r√°pido</h4>
    <p style="text-align:justify;">
      Durante o experimento, observou-se que o algoritmo apresentou boa capacidade de detec√ß√£o dos objetos em movimento, mesmo em cen√°rios com varia√ß√£o de ilumina√ß√£o.
      O modelo de fundo foi atualizado gradualmente, permitindo distinguir de forma consistente as regi√µes em movimento. No v√≠deo com movimento lento, a detec√ß√£o mostrou-se est√°vel e precisa, 
      sendo poss√≠vel visualizar claramente o rastro cont√≠nuo do objeto, que indicava sua trajet√≥ria ao longo do tempo. J√° no v√≠deo com movimento r√°pido, embora a detec√ß√£o tenha apresentado 
      menor n√≠vel de detalhamento, devido √† velocidade, ainda foi poss√≠vel identificar de forma n√≠tida a presen√ßa e o deslocamento do objeto, comprovando a efici√™ncia do m√©todo utilizado.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_subtracao.mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando o resultado com v√≠deo lento.
      </figcaption>
    </figure>
      
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_subtracao (1).mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando o resultado com v√≠deo r√°pido.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de imagem da webcam</h4>
    <p style="text-align:justify;">
      No segundo experimento, o programa apresentou um comportamento semelhante ao anterior.
      A principal dificuldade observada foi a varia√ß√£o de ilumina√ß√£o do ambiente, que interferiu 
      na atualiza√ß√£o do modelo de fundo, causando pequenas √°reas borradas, indicando falsa detec√ß√£o de movimento em certos momentos.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_lab5_2.mp4" type="video/mp4">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela exibindo movimentos dos integrantes do grupo e do objeto colorido.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Aplica√ß√£o pr√°tica com subtra√ß√£o de fundo:</h4>
    <p style="text-align:justify;">
      No contexto do projeto RoadWatch, essas t√©cnicas s√£o fundamentais para o reconhecimento de a√ß√µes do motorista.
      A subtra√ß√£o de fundo permite isolar os elementos em movimento dentro do ve√≠culo, como as m√£os e o celular, enquanto 
      a detec√ß√£o de movimento possibilita identificar quando o motorista realiza a√ß√µes suspeitas, como pegar o telefone durante a condu√ß√£o. 
      Um experimento pr√°tico consiste em capturar, por meio da webcam, imagens do motorista durante a simula√ß√£o de dire√ß√£o. 
      O sistema realiza a subtra√ß√£o de fundo para remover o cen√°rio fixo e destaca apenas os movimentos das m√£os e do celular.
      Quando o algoritmo identifica um movimento incompat√≠vel com a dire√ß√£o segura, o sistema emite um alerta visual ou sonoro indicando o uso indevido do aparelho.
      Esse tipo de aplica√ß√£o demonstra como a subtra√ß√£o de fundo e a detec√ß√£o de movimento podem ser integradas a sistemas de seguran√ßa veicular, 
      contribuindo para a redu√ß√£o de acidentes causados por distra√ß√µes e promovendo uma condu√ß√£o mais segura e respons√°vel.
    </p>
    </section>  
    
    <h3>5. Conclus√µes</h3>
     <p style="text-align:justify;">
        Este relat√≥rio sobre Subtra√ß√£o de Fundo e Detec√ß√£o de Movimento permitiu compreender na pr√°tica o funcionamento de algoritmos fundamentais 
        do processamento de v√≠deo. Nos v√≠deos com movimento lento, a detec√ß√£o foi mais detalhada, permitindo observar claramente o trajeto do objeto. 
        J√° nos v√≠deos com movimento r√°pido, embora o detalhamento tenha diminu√≠do, o sistema ainda conseguiu reconhecer o deslocamento, demonstrando sua efici√™ncia 
        mesmo em situa√ß√µes com varia√ß√£o de ilumina√ß√£o e velocidade. Nos testes com a webcam, a t√©cnica se mostrou sens√≠vel √†s varia√ß√µes de luz, mas eficaz na detec√ß√£o 
        dos elementos n√£o est√°ticos, como as m√£os e objetos coloridos em movimento.
      </p>
      
      <p style="text-align:justify;">
        Esses resultados evidenciam a import√¢ncia da subtra√ß√£o de fundo como etapa inicial de an√°lise de comportamento e reconhecimento de a√ß√µes.
        No contexto do projeto RoadWatch, essa t√©cnica desempenha um papel fundamental na identifica√ß√£o do uso de celular durante a dire√ß√£o, uma vez 
        que permite isolar e acompanhar o movimento das m√£os e do dispositivo, destacando a√ß√µes que desviam a aten√ß√£o do motorista.
        Assim, conclui-se que as t√©cnicas de subtra√ß√£o de fundo e detec√ß√£o de movimento s√£o ferramentas essenciais, com aplica√ß√µes que v√£o desde sistemas 
        de monitoramento e seguran√ßa at√© solu√ß√µes inteligentes de transporte.
      </p>
    </div>
    </div>
  </details>
  
        <!-- RELAT√ìRIO 6 -->
  <details>
    <summary>Relat√≥rio 6</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 05/11/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
      <p style="text-align:justify;"> Nesta aula, foi estudado o conceito de features (caracter√≠sticas locais) em imagens, com foco na detec√ß√£o de pontos de interesse utilizando diferentes m√©todos implementados na biblioteca OpenCV. Em vis√£o computacional, features s√£o elementos que se destacam em uma imagem, como cantos, bordas ou regi√µes com varia√ß√£o significativa de intensidade, e que podem ser reconhecidos novamente em outras imagens ou quadros de um v√≠deo. Esses pontos s√£o fundamentais porque permanecem est√°veis mesmo quando h√° mudan√ßas de ilumina√ß√£o, rota√ß√£o, escala ou perspectiva, tornando-se essenciais em aplica√ß√µes como rastreamento de movimento, reconstru√ß√£o tridimensional, calibra√ß√£o de c√¢meras e reconhecimento de padr√µes.</p>

<p style="text-align:justify;"> Durante o estudo te√≥rico, foi analisado o material ‚ÄúUnderstanding Features‚Äù, que apresenta a import√¢ncia desses pontos de interesse e como eles s√£o usados para descrever e compreender o conte√∫do de uma cena. No laborat√≥rio, foram aplicados os m√©todos cl√°ssicos de detec√ß√£o, como o Shi‚ÄìTomasi Corner Detector e o Good Features to Track, al√©m de outros detectores de features dispon√≠veis no OpenCV. Inicialmente, foi desenvolvido um programa em C++ para leitura de imagens est√°ticas ‚Äî tanto das capturas feitas anteriormente quanto das imagens utilizadas no projeto de v√≠deo ‚Äî realizando a detec√ß√£o e a marca√ß√£o das features identificadas, com posterior salvamento dos resultados em arquivo.</p>

<p style="text-align:justify;"> Em seguida, o experimento foi ampliado para o ambiente din√¢mico, com a implementa√ß√£o de um segundo programa em C++ capaz de capturar imagens em tempo real pela webcam. Esse programa exibe, simultaneamente, a imagem original e a imagem processada com as features destacadas, permitindo observar o funcionamento do algoritmo em diferentes condi√ß√µes. Foram realizadas filmagens tanto com os integrantes do grupo quanto com um tabuleiro de xadrez preto e branco, pois esse padr√£o geom√©trico cont√©m muitos cantos bem definidos e √© amplamente utilizado para testes e calibra√ß√£o de c√¢meras.</p>

<p style="text-align:justify;"> Por fim, foram discutidas aplica√ß√µes pr√°ticas das t√©cnicas de detec√ß√£o de features e cantos, que incluem desde o rastreamento de objetos e reconhecimento facial at√© a reconstru√ß√£o 3D e o mapeamento de ambientes para rob√¥s aut√¥nomos. Assim, a Aula 6 proporcionou uma compreens√£o s√≥lida sobre como detectar, visualizar e aplicar features em imagens e v√≠deos, consolidando a base te√≥rica e pr√°tica para o desenvolvimento de sistemas de vis√£o computacional.</p>

      
      <h3>2. Fundamentos B√°sicos</h3>
      <p style="text-align:justify;">As features, ou caracter√≠sticas locais, representam informa√ß√µes relevantes extra√≠das de uma imagem que permitem identificar e comparar regi√µes espec√≠ficas entre diferentes imagens ou quadros de um v√≠deo. Em vis√£o computacional, elas correspondem a pontos de interesse, bordas, cantos ou regi√µes √∫nicas que se destacam em rela√ß√£o ao restante da cena, funcionando como elementos fundamentais para reconhecer, rastrear e descrever objetos. Essas caracter√≠sticas s√£o especialmente √∫teis porque tendem a se manter est√°veis mesmo quando h√° varia√ß√µes de ilumina√ß√£o, rota√ß√£o, escala ou perspectiva, tornando-se essenciais em aplica√ß√µes como reconhecimento de padr√µes, alinhamento de imagens, reconstru√ß√£o tridimensional, rastreamento de movimento e detec√ß√£o de objetos.</p>

<p style="text-align:justify;">Entre os principais tipos de features destacam-se os cantos (corners), as bordas (edges) e os blobs. Os cantos s√£o regi√µes onde h√° grande varia√ß√£o de intensidade em duas dire√ß√µes, como nas interse√ß√µes de linhas ou padr√µes geom√©tricos ‚Äî por exemplo, em um tabuleiro de xadrez. As bordas correspondem aos limites entre regi√µes com intensidades diferentes, sendo √∫teis na segmenta√ß√£o e no contorno de objetos. J√° os blobs representam regi√µes que se sobressaem pela textura ou forma, sendo detectados por m√©todos como DoG (Difference of Gaussians) e LoG (Laplacian of Gaussian).</p>

<p style="text-align:justify;">Entre os algoritmos cl√°ssicos de detec√ß√£o de cantos, destacam-se o Harris Corner Detector e o Shi‚ÄìTomasi (Good Features to Track). O m√©todo de Harris identifica pontos de varia√ß√£o significativa de intensidade em m√∫ltiplas dire√ß√µes, marcando regi√µes com alto contraste. J√° o Shi‚ÄìTomasi √© uma vers√£o aprimorada do m√©todo de Harris, que seleciona apenas os cantos mais confi√°veis, ideais para rastreamento entre quadros consecutivos de v√≠deo. Esses detectores s√£o amplamente empregados em tarefas de rastreamento de movimento, calibra√ß√£o de c√¢meras e reconstru√ß√£o de cenas.</p>

<p style="text-align:justify;">Ap√≥s detectar os pontos de interesse, √© poss√≠vel extrair descritores, que s√£o vetores num√©ricos capazes de representar matematicamente a vizinhan√ßa de cada feature. Esses descritores permitem comparar e identificar correspond√™ncias entre features de imagens distintas, sendo a base para t√©cnicas como o emparelhamento de pontos e a fus√£o de imagens.</p>

<p style="text-align:justify;">A detec√ß√£o de features tem in√∫meras aplica√ß√µes pr√°ticas, incluindo o rastreamento de objetos em v√≠deos, a montagem de panoramas por meio de image stitching, o reconhecimento de formas em sistemas industriais, a calibra√ß√£o e orienta√ß√£o de c√¢meras em rob√≥tica e a localiza√ß√£o de padr√µes para realidade aumentada. Em s√≠ntese, o estudo de features possibilita converter informa√ß√µes visuais em dados matem√°ticos interpret√°veis pelo computador, sendo um dos pilares da vis√£o computacional moderna e permitindo que sistemas artificiais possam ‚Äúentender‚Äù e analisar o conte√∫do das imagens com precis√£o e consist√™ncia.</p>

      
      <h3>3. Materiais e M√©todos</h3>
      
      <p style="text-align:justify;">Os experimentos da Aula 6 foram realizados utilizando a biblioteca OpenCV em linguagem C++, aplicada tanto para processamento de imagens est√°ticas quanto para captura e an√°lise em tempo real por meio da webcam. Inicialmente, foram selecionadas as imagens obtidas em aulas anteriores, bem como algumas imagens utilizadas no projeto de v√≠deo do grupo, servindo como base para a implementa√ß√£o dos algoritmos de detec√ß√£o de features. Todos os testes foram desenvolvidos em ambiente de programa√ß√£o configurado com o compilador g++ e suporte √†s bibliotecas do OpenCV, executados em sistemas compat√≠veis com Linux ou Windows.</p>

<p style="text-align:justify;">O primeiro experimento consistiu na cria√ß√£o de um programa capaz de realizar a leitura de imagens previamente armazenadas e aplicar os m√©todos de detec√ß√£o de *features*, com destaque para o Shi‚ÄìTomasi Corner Detector e o algoritmo Good Features to Track, conforme apresentado na documenta√ß√£o oficial do OpenCV. O programa processou cada imagem identificando os pontos de interesse (cantos ou features) e marcou-os graficamente sobre a imagem original, salvando os resultados em arquivos de sa√≠da no formato de imagem. Essa etapa teve como objetivo visualizar o comportamento dos detectores em diferentes tipos de cena, com varia√ß√µes de textura, contraste e ilumina√ß√£o.</p>

<p style="text-align:justify;">No segundo experimento, o c√≥digo foi modificado para realizar a captura de imagens em tempo real utilizando a webcam do computador. O programa foi estruturado para exibir simultaneamente duas janelas: uma contendo a imagem original capturada e outra apresentando o resultado da detec√ß√£o das features em tempo real. Esse procedimento permitiu observar o desempenho dos algoritmos sob condi√ß√µes din√¢micas, avaliando a estabilidade e a quantidade de pontos detectados a cada quadro. Foram realizadas filmagens envolvendo os integrantes do grupo e tamb√©m um tabuleiro de xadrez preto e branco, escolhido por conter um padr√£o geom√©trico com cantos bem definidos e amplamente utilizado em calibra√ß√£o de c√¢meras.</p>

<p style="text-align:justify;">Al√©m da implementa√ß√£o pr√°tica, foram realizadas pesquisas complementares sobre as aplica√ß√µes das t√©cnicas de detec√ß√£o de *features* e *corners* em diferentes √°reas da vis√£o computacional. Foram discutidos exemplos de uso em sistemas de rastreamento de movimento, reconstru√ß√£o tridimensional, reconhecimento de objetos e calibra√ß√£o de c√¢meras, relacionando essas aplica√ß√µes com poss√≠veis usos no projeto desenvolvido na disciplina. Dessa forma, os m√©todos empregados combinaram estudo te√≥rico, experimenta√ß√£o pr√°tica e an√°lise comparativa, proporcionando uma compreens√£o abrangente dos fundamentos e das aplica√ß√µes da detec√ß√£o de features em imagens digitais.</p>

<h3>Diagrama de Blocos</h3>
       <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Bloco_Features.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de Features.
  </figcaption>
</figure>
      
      <h3>4. Resultados e An√°lises</h3>

  <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Detec√ß√£o de features em v√≠deos</h4>
    <p style="text-align:justify;">
      Durante o experimento com o v√≠deo previamente gravado, observou-se que o algoritmo de detec√ß√£o de features apresentou desempenho consider√°vel na identifica√ß√£o de pontos de interesse.
      Nas imagens contendo a m√£o mec√¢nica, o m√©todo foi capaz de detectar diversos pontos ao longo de suas bordas e articula√ß√µes, principalmente nas pontas dos dedos. Esses pontos foram 
      real√ßados em torno dos dedos e das jun√ß√µes, indicando que o detector conseguiu reconhecer √°reas onde h√° varia√ß√µes de geometria. O algoritmo conseguiu reconhecer a varia√ß√£o de teclas 
      presentes no computador localizado no canto inferior da tela. Mesmo com pequenas varia√ß√µes de ilumina√ß√£o e movimenta√ß√µes sutis da m√£o mec√¢nica durante o v√≠deo, as features permaneceram 
      est√°veis. Esse comportamento confirma a efici√™ncia do detector na identifica√ß√£o de estruturas com contornos bem definidos.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de tela de 2025-10-29 10-35-52.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando a detec√ß√£o de features em um v√≠deo gravado pelo grupo.
      </figcaption>
    </figure>
      
    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/frame_0.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Identifica√ß√£o de Features a partir da webcam</h4>
    <p style="text-align:justify;">
      No experimento utilizando a webcam, foi poss√≠vel observar uma detec√ß√£o de features consistente e em tempo real, tanto em objetos est√°ticos quanto em movimento.
      Ao posicionar o tabuleiro de xadrez, o algoritmo identificou com alta precis√£o os cantos formados pela interse√ß√£o dos quadrados pretos e brancos. As features 
      foram distribu√≠das de maneira uniforme sobre toda a superf√≠cie do tabuleiro, evidenciando o forte contraste e a simetria da cena. Nos testes com integrantes do grupo,
      as principais regi√µes de detec√ß√£o concentraram-se ao redor dos olhos, contornos em geral, e em √°reas de maior contraste, como o nariz. Mesmo com movimento moderado 
      ou inconsist√™ncias na ilumina√ß√£o, a detec√ß√£o manteve-se est√°vel, indicando que o sistema possui desempenho satisfat√≥rio para aplica√ß√µes em tempo real.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o de tela de 2025-10-29 10-40-04.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela exibindo a identifica√ß√£o de Features em tempo real.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_152.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features na Fernanda.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_58.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features no Gabriel.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_234.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features na Ingrid.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_35.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features no Tabuleiro de Xadrez
      </figcaption>
    </figure>
    
    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h3>Aplica√ß√µes pr√°ticas</h3>

    <h4>1. Rastreamento de movimento (Optical Flow)</h4>
    <p>
      Cantos s√£o pontos ideais para acompanhar o movimento de objetos entre 
      quadros consecutivos de um v√≠deo. Por exemplo, o m√©todo 
      Lucas‚ÄìKanade utiliza os cantos detectados porShi‚ÄìTomasi
      para rastrear o deslocamento desses pontos ao longo do tempo, permitindo 
      identificar o movimento de pessoas, ve√≠culos ou partes espec√≠ficas de uma cena.
    </p>
  
    <h4>2. Calibra√ß√£o de c√¢meras</h4>
    <p>
      O padr√£o de tabuleiro xadrez √© amplamente usado em calibra√ß√£o porque fornece 
      cantos bem definidos e facilmente detect√°veis. Ao identificar as interse√ß√µes 
      do tabuleiro em m√∫ltiplas imagens, √© poss√≠vel calcular os 
      par√¢metros intr√≠nsecos e extr√≠nsecos da c√¢mera (foco, 
      distor√ß√£o, posi√ß√£o e orienta√ß√£o).
    </p>
  
    <h4>3. Reconstru√ß√£o 3D e estereovis√£o</h4>
    <p>
      Em sistemas com duas c√¢meras, os cantos detectados nas duas imagens permitem 
      encontrar correspond√™ncias entre pontos hom√≥logos. A partir dessas 
      correspond√™ncias, calcula-se a profundidade dos pontos e 
      reconstr√≥i-se o modelo tridimensional da cena.
    </p>
  
    <h4>4. Mosaico de imagens (Image Stitching)</h4>
    <p>
      Ao detectar cantos e outros features em diferentes imagens de uma 
      mesma cena, √© poss√≠vel emparelh√°-los e calcular as transforma√ß√µes necess√°rias 
      para uni-las, criando panoramas. Essa t√©cnica √© usada em aplicativos de 
      c√¢meras de celular e em softwares de edi√ß√£o de imagem.
    </p>
  
    <h4>5. Reconhecimento e localiza√ß√£o de objetos</h4>
    <p>
      Detectores de features podem ser usados para reconhecer padr√µes 
      fixos (como logotipos, faces, placas ou produtos), comparando os pontos 
      detectados em uma imagem de refer√™ncia com os da imagem analisada.
    </p>
  
    <h3>Experimento proposto (projeto da disciplina)</h3>
  
    <p>
      No contexto do projeto de v√≠deo do grupo (RoadWatch), 
      a detec√ß√£o de features pode ser aplicada para 
      rastrear o movimento do celular nas m√£os do motorista
      durante uma grava√ß√£o. O experimento consiste em:
    </p>
  
    <ol>
      <li>
        Capturar um v√≠deo curto do motorista segurando o celular ao volante.
      </li>
      <li>
        Aplicar o detector Shi‚ÄìTomasi em cada quadro para localizar
        cantos nas bordas do celular.
      </li>
      <li>
        Usar o m√©todo Lucas‚ÄìKanade Optical Flow para acompanhar o 
        movimento desses pontos ao longo do tempo.
      </li>
      <li>
        Analisar o deslocamento das features: caso os pontos 
        correspondentes ao celular se movam significativamente enquanto o carro 
        est√° parado, o sistema pode identificar o uso do aparelho.
      </li>
    </ol>
  
    <p>
      Esse experimento demonstra como a detec√ß√£o de cantos e o rastreamento de 
    features s√£o ferramentas poderosas para 
     an√°lise de comportamento em v√≠deo, podendo ser integradas em 
      aplica√ß√µes de seguran√ßa veicular, monitoramento 
      e vis√£o inteligente.
    </p>
    </section>  
    
    <h3>5. Conclus√µes</h3>
     <p style="text-align:justify;">
        Este relat√≥rio permitiu compreender o papel das features em Processamento de V√≠deo e sua import√¢ncia em sistemas de detec√ß√£o, reconhecimento e rastreamento.
        Os experimentos comprovaram que t√©cnicas como Shi-Tomasi e Good Features to Track s√£o eficientes para identificar cantos e pontos relevantes em imagens 
        est√°ticas e em tempo real.
      </p>
      
      <p style="text-align:justify;">
        No tabuleiro de xadrez, a detec√ß√£o foi precisa, evidenciando a for√ßa da t√©cnica em padr√µes bem definidos. J√° em humanos, os cantos dos olhos, nariz e 
        m√£os se destacaram como features importantes. Conclui-se que o uso de detectores de features √© essencial para o projeto de Processamento de V√≠deo, 
        podendo contribuir para identificar elementos como m√£os, celular ou movimentos do motorista.
      </p>
    </div>
    </div>
  </details>

      <!-- RELAT√ìRIO 7 -->
  <details>
    <summary>Relat√≥rio 7</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 10/11/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
        <p style="text-align:justify;">Esta pr√°tica tem como objetivo introduzir os conceitos fundamentais de detec√ß√£o de objetos utilizando o OpenCV em C++, com foco na detec√ß√£o de rostos por meio de classificadores em cascata (Cascade Classifier) baseados em Haar-like features. Para isso, o estudante deve primeiro revisar a teoria, compreendendo como esses modelos funcionam, desde a extra√ß√£o de caracter√≠sticas e o uso de janelas deslizantes at√© a estrutura em m√∫ltiplas etapas que permite decidir rapidamente se uma regi√£o da imagem representa ou n√£o o objeto de interesse. Em seguida, ser√£o utilizados modelos pr√©-treinados em arquivos XML disponibilizados pelo OpenCV, respons√°veis por reconhecer padr√µes visuais, como rostos frontais e perfis faciais.</p>

  <p style="text-align:justify;">No primeiro experimento, ser√° desenvolvido um programa em C++ capaz de carregar imagens dos integrantes da equipe, al√©m da imagem com avatares e imagens do projeto de v√≠deo. O sistema dever√° realizar a detec√ß√£o de rostos e/ou outros objetos escolhidos, marcando as regi√µes detectadas com ret√¢ngulos ou indicadores visuais em **cores contrastantes. O programa tamb√©m deve permitir que o usu√°rio salve as imagens anotadas ao pressionar uma tecla, registrando no relat√≥rio qual arquivo XML foi utilizado (por exemplo, `haarcascade_frontalface_default.xml` ou outro modelo Haarcascade dispon√≠vel no reposit√≥rio oficial do OpenCV).</p>

  <p style="text-align:justify;">No segundo experimento, o c√≥digo ser√° adaptado para realizar a captura ao vivo pela webcam, exibindo em tempo real a imagem da c√¢mera juntamente com o resultado da detec√ß√£o de objetos. Da mesma forma, o usu√°rio dever√° poder salvar quadros detectados sob comando. Ao final, espera-se que o estudante compreenda o funcionamento e as limita√ß√µes desses detectores cl√°ssicos ‚Äî como sensibilidade √† ilumina√ß√£o, varia√ß√£o de pose e oclus√µes ‚Äî al√©m de adquirir pr√°tica na configura√ß√£o e uso dos classificadores Haarcascade, ajustes de par√¢metros, integra√ß√£o com webcams e manipula√ß√£o de imagens no OpenCV. O relat√≥rio deve incluir as imagens geradas, observa√ß√µes sobre a precis√£o da detec√ß√£o, par√¢metros utilizados e um breve coment√°rio sobre boas pr√°ticas e √©tica no uso de detec√ß√£o facial, incluindo cuidado com privacidade e consentimento ao registrar imagens de pessoas reais.</p>
 
      <h3>2. Fundamentos B√°sicos</h3>
       <p style="text-align:justify;">A detec√ß√£o autom√°tica de objetos √© uma tarefa fundamental em vis√£o computacional, permitindo identificar e localizar elementos relevantes em imagens ou v√≠deos, como rostos, pessoas, ve√≠culos ou outros padr√µes visuais. No contexto desta atividade, ser√° utilizada a t√©cnica de classificadores em cascata baseada em Haar-like features, um m√©todo cl√°ssico que antecede o uso extensivo de redes neurais e ainda √© bastante √∫til para fins acad√™micos e aplica√ß√µes em tempo real.</p>

 <p style="text-align:justify;">Os classificadores Haarcascade funcionam a partir de um conjunto de caracter√≠sticas simples que analisam varia√ß√µes de intensidade entre regi√µes claras e escuras da imagem. Essas caracter√≠sticas s√£o calculadas de forma muito r√°pida por meio da estrutura chamada imagem integral, que agiliza o c√°lculo de somas de pixels em regi√µes retangulares. A detec√ß√£o ocorre por meio de uma sequ√™ncia de etapas chamada cascata: cada est√°gio avalia a imagem e descarta rapidamente √°reas que n√£o correspondem ao objeto; apenas as regi√µes que passam por todas as fases s√£o consideradas como detec√ß√µes v√°lidas. Essa abordagem garante efici√™ncia e velocidade, permitindo seu uso em dispositivos com pot√™ncia de processamento limitada.</p>

 <p style="text-align:justify;">Para facilitar sua aplica√ß√£o, o OpenCV disponibiliza arquivos XML pr√©-treinados, conhecidos como Haarcascades, contendo modelos preparados para diferentes objetos, como rosto frontal, perfil facial, olhos, sorriso e outros. Assim, o estudante pode focar na implementa√ß√£o e experimenta√ß√£o do algoritmo sem necessidade de treinar um modelo do zero. A detec√ß√£o costuma ser feita em imagens em tons de cinza, j√° que o m√©todo se baseia exclusivamente em diferen√ßas de intensidade, e frequentemente s√£o aplicadas t√©cnicas simples de pr√©-processamento, como equaliza√ß√£o de histograma, para melhorar contraste e estabilidade da detec√ß√£o.</p>

 <p style="text-align:justify;">Entre os pontos fortes desse m√©todo destacam-se sua simplicidade, velocidade e boa performance em situa√ß√µes controladas. Por outro lado, h√° limita√ß√µes, como sensibilidade a ilumina√ß√£o vari√°vel, mudan√ßas bruscas de posi√ß√£o ou rota√ß√£o do rosto e oclus√µes parciais, como uso de m√°scaras ou objetos cobrindo o rosto. Mesmo com essas restri√ß√µes, o estudo desse tipo de algoritmo √© importante, pois ele representa um marco hist√≥rico no avan√ßo da vis√£o computacional e fornece uma base s√≥lida para compreender m√©todos modernos de detec√ß√£o, como aqueles baseados em aprendizado profundo.</p>


      <h3>3. Materiais e M√©todos</h3>
      <p style="text-align:justify;">Para a realiza√ß√£o desta atividade, foi utilizado um computador com sistema operacional compat√≠vel com a biblioteca OpenCV e suporte √† compila√ß√£o em C++. O ambiente de desenvolvimento empregado inclui o compilador g++, juntamente com a instala√ß√£o da biblioteca OpenCV vers√£o 4.x ou superior, necess√°ria para carregar os modelos pr√©-treinados, manipular imagens, acessar a webcam e executar o algoritmo de detec√ß√£o. Tamb√©m foram utilizados arquivos XML contendo modelos Haarcascade disponibilizados pelo OpenCV, incluindo, por exemplo, o classificador frontal para detec√ß√£o de rostos. As imagens de refer√™ncia consistiram em fotografias dos integrantes do grupo, avatares utilizados em atividades anteriores e quadros extra√≠dos do v√≠deo desenvolvido no projeto da disciplina, al√©m do uso de uma webcam integrada ou externa para a fase de captura em tempo real.</p>

<p style="text-align:justify;">O procedimento foi dividido em duas etapas principais. Na primeira, desenvolveu-se um programa em C++ capaz de carregar imagens est√°ticas e aplicar o classificador em cascata para detec√ß√£o de rostos ou outros objetos. Inicialmente, a imagem foi lida e convertida para tons de cinza, permitindo ao detector trabalhar apenas com varia√ß√µes de intensidade luminosa. Em seguida, aplicou-se, quando necess√°rio, a equaliza√ß√£o de histograma para melhorar o contraste e aumentar as chances de detec√ß√£o. Depois disso, o classificador Haarcascade foi carregado a partir do arquivo XML correspondente e o m√©todo detectMultiScale foi utilizado para localizar poss√≠veis regi√µes contendo rostos. As √°reas detectadas foram marcadas com ret√¢ngulos coloridos para facilitar a visualiza√ß√£o e o programa foi configurado para salvar as imagens anotadas ao pressionar uma tecla. Essa etapa permitiu avaliar diferentes par√¢metros, como escala, n√∫mero m√≠nimo de vizinhos e tamanho m√≠nimo de objeto detectado.</p>

<p style="text-align:justify;">Na segunda etapa, o c√≥digo foi modificado para operar com captura ao vivo por webcam. O programa passou a abrir uma janela que exibia continuamente o fluxo de v√≠deo, aplicando o mesmo processo de detec√ß√£o quadro a quadro. Assim como na etapa anterior, foi mantida a possibilidade de salvar imagens com as detec√ß√µes sempre que o usu√°rio solicitasse. Foram observados aspectos como taxa de atualiza√ß√£o, lat√™ncia e consist√™ncia das detec√ß√µes em movimento, al√©m do impacto de condi√ß√µes de ilumina√ß√£o e dist√¢ncia do rosto em rela√ß√£o √† c√¢mera.</p>
      
       <h3>Diagrama de Blocos</h3>
             <figure style="max-width: 1500px; margin: 24px auto; text-align: center;">
  <img src="imagens/fluxograma_lab7.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de Haarcascade.
  </figcaption>
</figure>
      
      <h3>4. Resultados e An√°lises</h3>
      <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de Imagem dos Integrantes e Avatares</h4>
    <p style="text-align:justify;">
      O programa foi configurado para carregar imagens dos membros da equipe e de seus respectivos avatares. As detec√ß√µes foram exibidas 
      na tela e, ao pressionar uma tecla, as imagens foram salvas com os ret√¢ngulos marcando os objetos detectados. As faces dos integrantes
      foram detectadas usando o modelo haarcascade_frontalface_alt2.xml do OpenCV. Com o modelo haarcascade_eye_tree_eyeglasses.xml, os olhos
      foram identificados, mas n√£o com a mesma precis√£o de identifica√ß√£o do modelo anterior, que identificou corretamente o formato das faces.
      Nos avatares, a detec√ß√£o funcionou melhor devido a uma melhor ilumina√ß√£o e defini√ß√£o de imagem.
    </p>
    </section>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/teste_1_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem da Ingrid ap√≥s aplica√ß√£o do modelo HaarCascade.
      </figcaption>
    </figure>  
      
    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/avatarFernanda_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Avatar da Fernanda ap√≥s aplica√ß√£o do modelo HaarCascade.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/avatarGabriel_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Avatar do Gabriel ap√≥s aplica√ß√£o do modelo HaarCascade.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de Imagem da Webcam</h4>
    <p style="text-align:justify;">
      No segundo experimento utilizou-se a webcam para capturar e exibir as detec√ß√µes em tempo real. O algoritmo aplicava a detec√ß√£o 
      de rostos e olhos, e ao pressionar uma tecla, salvava a imagem com as marca√ß√µes em diferentes cores. A detec√ß√£o da face foi 
      est√°vel e r√°pida, mesmo com varia√ß√µes leves de posi√ß√£o e ilumina√ß√£o. Na regi√£o dos olhos ouve uma certa varia√ß√£o de precis√£o,
      mas ainda assim o algoritmo conseguiu detectar e marcar com ret√¢ngulos vermelhos a regi√£o. Quando havia movimento r√°pido ou 
      baixa ilumina√ß√£o, algumas falhas ocorreram, mas o algoritmo se recuperava rapidamente.
    </p>
    </section>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/Captura de tela de 2025-11-03 09-17-34.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Captura de Tela da Webcam ap√≥s aplica√ß√£o do modelo HaarCascade.
      </figcaption>
    </figure>
    
    <h3>5. Conclus√µes</h3>
     <p style="text-align:justify;">
       A partir das an√°lises realizadas, pode-se concluir que o m√©todo Haar Cascade apresentou 
       desempenho consider√°vel na detec√ß√£o de rostos e olhos, tanto em imagens est√°ticas quanto 
       em v√≠deo ao vivo. A utiliza√ß√£o dos modelos haarcascade_frontalface_alt2.xml e 
       haarcascade_eye_tree_eyeglasses.xml garantiu resultados satisfat√≥rios, com identifica√ß√£o 
       mesmo diante de express√µes faciais variadas.

       No entanto, observou-se que a efic√°cia do m√©todo est√° diretamente relacionada a condi√ß√µes adequadas 
       de captura, como boa ilumina√ß√£o, posi√ß√£o frontal do rosto e aus√™ncia de movimenta√ß√µes bruscas.
       Assim, embora o algoritmo seja eficiente, seu desempenho pode ser comprometido em ambientes com baixa 
       luminosidade ou √¢ngulos desfavor√°veis.
      </p>
    </div>
    </div>
  </details>
  
      <!-- RELAT√ìRIO 8 -->
  <details>
    <summary>Relat√≥rio 8</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa ‚Äî Gabriel Henrique Pensado Rothen ‚Äî Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 12/11/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdu√ß√£o</h3>
<p style="text-align:justify;">O rastreamento de objetos (object tracking) √© uma etapa central em sistemas de vis√£o computacional que precisam acompanhar, em tempo real, a posi√ß√£o de um alvo ao longo de uma sequ√™ncia de quadros. Essa tarefa est√° presente em aplica√ß√µes como vigil√¢ncia inteligente, assist√™ncia √† condu√ß√£o, an√°lise esportiva e intera√ß√£o homem‚Äìm√°quina. Diferente da detec√ß√£o, que localiza objetos quadro a quadro, o rastreamento mant√©m a identidade e a trajet√≥ria do mesmo objeto ao longo do v√≠deo, mesmo diante de oclus√µes parciais, mudan√ßas de escala, ilumina√ß√£o e movimenta√ß√£o da c√¢mera.</p>

<p style="text-align:justify;">Neste laborat√≥rio (Aula 8 ‚Äì Lab 8), estudaremos e implementaremos rastreadores cl√°ssicos do OpenCV e tamb√©m um rastreador baseado em aprendizado profundo (GOTURN). Como base te√≥rica, utilizaremos tr√™s refer√™ncias: (A) o tutorial oficial Introduction to OpenCV Tracker, que apresenta a API cv::Tracker e rastreadores como KCF, CSRT, MOSSE, MIL e TLD; (B) o guia Object Tracking using OpenCV (C++/Python), com exemplos pr√°ticos de implementa√ß√£o e boas pr√°ticas; e (C) GOTURN: Deep Learning based Object Tracking, que introduz um rastreador baseado em regress√£o com aprendizado pr√©vio capaz de prever a posi√ß√£o do objeto a partir de pares de frames consecutivos.</p>

<p style="text-align:justify;">O laborat√≥rio est√° dividido em dois experimentos. No Experimento (1), ser√° desenvolvido um programa em C++ que realiza a leitura de v√≠deos, inclusive v√≠deos do projeto de v√≠deo do grupo, permitindo a sele√ß√£o manual da ROI inicial e executando o rastreamento do objeto com um dos m√©todos estudados. O programa deve apresentar o resultado na tela e salvar o v√≠deo com o objeto rastreado. No Experimento (2), ser√° feita uma modifica√ß√£o para capturar imagens da webcam, exibindo o rastreamento em tempo real e tamb√©m salvando o v√≠deo processado. Para testar o rastreador baseado em deep learning, o arquivo goturn.caffemodel √© fornecido e deve ser salvo na mesma pasta dos arquivos do laborat√≥rio antes da execu√ß√£o.</p>

<p style="text-align:justify;">Al√©m de refor√ßar os conceitos te√≥ricos, este laborat√≥rio enfatiza organiza√ß√£o e boas pr√°ticas em desenvolvimento de software: estrutura do projeto, tratamento de erros na captura de v√≠deo e webcam, controle de taxa de quadros, codifica√ß√£o do v√≠deo de sa√≠da e padroniza√ß√£o da interface de visualiza√ß√£o. Tamb√©m ser√£o considerados crit√©rios de avalia√ß√£o qualitativos, como estabilidade da caixa delimitadora, robustez a oclus√µes e jitter, e quantitativos simples, como tempo de processamento por frame e taxa de FPS alcan√ßada. Por fim, refor√ßa-se a import√¢ncia do uso √©tico das imagens, garantindo consentimento e responsabilidade no uso de v√≠deos pessoais e de c√¢mera ao vivo.</p>

      <h3>2. Fundamentos B√°sicos</h3>
<p style="text-align:justify;">O rastreamento de objetos √© uma t√©cnica da vis√£o computacional que permite acompanhar a posi√ß√£o de um alvo ao longo do tempo em uma sequ√™ncia de v√≠deo. Diferentemente da detec√ß√£o de objetos, que realiza a identifica√ß√£o quadro a quadro de maneira independente, o rastreamento utiliza informa√ß√µes temporais para prever onde o objeto estar√° nos frames seguintes, mantendo assim sua continuidade e identidade ao longo de toda a sequ√™ncia. Para iniciar o processo, normalmente o usu√°rio seleciona manualmente uma regi√£o de interesse (ROI) que cont√©m o objeto a ser rastreado, e essa caixa inicial serve como refer√™ncia para que o algoritmo procure e atualize sua posi√ß√£o nos quadros seguintes.</p>

<p style="text-align:justify;">Existem diferentes abordagens para rastreamento. Os m√©todos cl√°ssicos utilizam caracter√≠sticas visuais como textura, bordas e padr√µes de movimento para comparar a regi√£o inicial com as pr√≥ximas imagens. Algoritmos comuns no OpenCV incluem MIL, KCF, CSRT e MOSSE, cada um possuindo vantagens espec√≠ficas quanto √† velocidade, precis√£o e robustez contra mudan√ßas de ilumina√ß√£o ou movimento r√°pido. Esses rastreadores funcionam estimando a semelhan√ßa entre a apar√™ncia inicial do objeto e sua apar√™ncia atual, ajustando continuamente a posi√ß√£o da caixa delimitadora.</p>

<p style="text-align:justify;">Al√©m dos m√©todos tradicionais, existem t√©cnicas baseadas em aprendizado profundo, como o rastreador GOTURN. Esse tipo de abordagem utiliza redes neurais previamente treinadas para aprender representa√ß√µes mais sofisticadas da apar√™ncia e varia√ß√£o do objeto, permitindo lidar melhor com transforma√ß√µes como mudan√ßas bruscas de escala, oclus√µes parciais e deforma√ß√µes. Em vez de depender apenas de correla√ß√£o de pixels ou descritores manuais, o GOTURN prev√™ diretamente a nova localiza√ß√£o do objeto ao comparar pares de frames consecutivos, tornando o processo mais adaptativo e preciso em cen√°rios complexos.</p>

<p style="text-align:justify;">O desempenho do rastreamento pode ser avaliado por crit√©rios como estabilidade da caixa ao longo dos frames, capacidade de acompanhar o objeto mesmo com oclus√µes ou mudan√ßas de ilumina√ß√£o, precis√£o da predi√ß√£o da posi√ß√£o e velocidade de execu√ß√£o medida em frames por segundo (FPS). Essas caracter√≠sticas s√£o essenciais em aplica√ß√µes como vigil√¢ncia por v√≠deo, an√°lise esportiva, intera√ß√£o humana com sistemas computacionais, rob√≥tica e ve√≠culos aut√¥nomos. Dessa forma, o rastreamento de objetos combina fundamentos matem√°ticos, t√©cnicas de processamento de imagens e, em abordagens mais modernas, redes neurais profundas para oferecer ferramentas poderosas em aplica√ß√µes pr√°ticas de vis√£o computacional.</p>

      <h3>3. Materiais e M√©todos</h3>
<p style="text-align:justify;">Para a realiza√ß√£o deste laborat√≥rio, foram utilizados computadores com sistema operacional compat√≠vel com a biblioteca OpenCV e compilador C++ suportado (como g++ ou Visual Studio). A biblioteca OpenCV foi previamente instalada com suporte aos m√≥dulos necess√°rios para rastreamento, incluindo a API de rastreadores tradicionais (como KCF, CSRT e MOSSE) e, no caso do uso do rastreador baseado em aprendizado profundo, suporte ao modelo GOTURN. Tamb√©m foi utilizado o arquivo de rede neural pr√©-treinado goturn.caffemodel, disponibilizado pelo professor, o qual foi salvo na mesma pasta do projeto a fim de permitir o carregamento adequado do modelo. Al√©m disso, foram utilizados v√≠deos com membros da equipe e v√≠deos gerados no trabalho de v√≠deo desenvolvido previamente na disciplina, bem como uma c√¢mera webcam integrada ou externa para capturar v√≠deo em tempo real no segundo experimento.</p>

<p style="text-align:justify;">O m√©todo adotado consistiu inicialmente na leitura de um v√≠deo pelo programa em C++, seguida da sele√ß√£o manual da regi√£o de interesse (ROI) correspondente ao objeto a ser rastreado. Ap√≥s essa sele√ß√£o, foi inicializado um dos rastreadores estudados, mantendo a posi√ß√£o inicial do objeto como refer√™ncia para os quadros subsequentes. A cada frame, o rastreador atualizou automaticamente a posi√ß√£o da caixa delimitadora com base na similaridade entre o objeto inicialmente selecionado e sua apar√™ncia atual. Conforme solicitado, o v√≠deo com o ret√¢ngulo de rastreamento foi exibido na tela e simultaneamente salvo em arquivo, garantindo registro dos resultados.</p>

<p style="text-align:justify;">No segundo experimento, o mesmo procedimento foi adaptado para entrada via webcam. O programa capturou continuamente os frames da c√¢mera, permitiu a sele√ß√£o manual do objeto no in√≠cio da execu√ß√£o e passou a rastrear o alvo em tempo real, exibindo a imagem com a caixa delimitadora atualizada e gravando o v√≠deo resultante. Durante todo o processo, foram considerados elementos de desempenho como taxa de quadros e estabilidade do rastreamento, testando diferentes rastreadores para observar mudan√ßas no comportamento do algoritmo em situa√ß√µes reais, como varia√ß√µes de ilumina√ß√£o, movimento r√°pido ou pequenas oclus√µes do objeto.</p>

<h3>Diagrama de Blocos</h3>
<figure style="max-width: 800px; margin: 24px auto; text-align: center;">
  <img src="imagens/bloco_lab8.png"
       loading="lazy"
       style="width:80%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de Rastreamento de Objetos.
  </figcaption>
</figure>

      <h3>4. Resultados e An√°lises</h3>
<p style="text-align:justify;">Desenvolva programa C++ para fazer a leitura dos videos com os membros da equipe - utilize tamb√©m os videos do seu trabalho de video. O programa deve realizar o rastreamento de objetos com um dos m√©todos acima vistos, e com sele√ß√£o manual de ROI, e apresentar na tela. O programa tamb√©m deve salvar os videos resultantes dos rastreamentos dos objetos.</p>

<figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Grava√ß√£o2.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Grava√ß√£o de tela mostrando do nosso trabalho.
      </figcaption>
    </figure>

<figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida.mp4" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        V√≠deo testando com nosso trabalho.
      </figcaption>
    </figure>

<p style="text-align:justify;">Outro programa C++ com v√≠deo de F√≥rmula 1.</p>

<figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida(1).mp4" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        Testando com v√≠deo o YouTube.
      </figcaption>
    </figure>


<p style="text-align:justify;">Outro programa C++ do item (1), agora fazendo a leitura de imagem da webcam.Neste caso, o programa deve adicionalmente mostrar uma janela ao vivo com a imagem e o resultado do rastreamento do objeto. Salvar o video resultante.</p>

<figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/parte2.webm" type="video/webm">
          Seu navegador n√£o suporta a reprodu√ß√£o de v√≠deo.
        </video>
      </div>
      <figcaption>
        V√≠deo testando com Gabriel.
      </figcaption>
    </figure>

      <h3>5. Conclus√µes</h3>

<p style="text-align:justify;">Nesta aula, aplicamos diferentes m√©todos de rastreamento de objetos no OpenCV, incluindo algoritmos cl√°ssicos (como KCF, CSRT e MOSSE) e o modelo de deep learning GOTURN. Foram realizados testes tanto com v√≠deos pr√©-gravados quanto com captura ao vivo via webcam, com sele√ß√£o manual da ROI e salvamento dos resultados.</p>

<p style="text-align:justify;">Observou-se que os m√©todos cl√°ssicos s√£o mais leves e r√°pidos, adequados para aplica√ß√µes em tempo real, enquanto o GOTURN apresentou maior robustez em situa√ß√µes com varia√ß√µes de apar√™ncia e movimento, por√©m com maior custo computacional. Conclui-se que a escolha do rastreador depende do equil√≠brio entre precis√£o e desempenho, e que uma boa inicializa√ß√£o da ROI √© essencial para garantir rastreamento est√°vel e confi√°vel.</p>
    </div>
    </div>
  </details>  
  
      <div class="back-top"><a href="#topo">‚Üë Voltar ao topo</a></div>
  </section>
  
  <footer class="footer">
    ¬© 2025 ‚Äî Equipe RoadWatch
  </footer>
</body>
</html>
