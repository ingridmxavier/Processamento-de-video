<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Processamento de Vídeo</title>
<style>
  :root{
    --vinho:#751a32;
    --fundo-pagina:#eebbbb;
    --cinza:#e5e5e7;
    --texto:#222;
  }
  html,body{
    margin:0;
    padding:0;
    background:var(--fundo-pagina);
    color:var(--texto);
    font-family:Arial, Helvetica, sans-serif;
    scroll-behavior:smooth;
  }
  .hero{
    background:var(--vinho);
    color:#fff;
    text-align:center;
    padding:48px 16px 24px;
  }
  .hero h1{margin:0 0 8px;font-size:36px;}
  .hero h2{margin:6px 0 16px;font-size:22px;font-weight:700;}
  .hero .nomes{max-width:980px;margin:0 auto 10px;opacity:.95;}

  .nav{
    display:flex; gap:10px; justify-content:center; flex-wrap:wrap;
    padding:12px; background:#ffffff22;
  }
  .nav a{
    text-decoration:none; color:#fff; background:var(--vinho);
    padding:8px 12px; border-radius:8px; font-weight:700;
  }
  .nav a:hover{ filter:brightness(.92); }

  .container{
    max-width:980px;
    margin:28px auto 48px;
    padding:0 16px;
  }
  .tema{
    font-weight:700;
    font-size:18px;
    margin:32px 0 12px;
  }
  details{
    background:#fff;
    border-radius:6px;
    margin:8px 0;
    border:1px solid #ddd;
    overflow:hidden;
  }
  summary{
    list-style:none;
    cursor:pointer;
    padding:12px 14px;
    font-weight:700;
    background:#fff;
    color:var(--texto);
    display:flex; align-items:center; gap:8px;
    user-select:none;
  }
  summary::before{content:"►";font-weight:900;color:var(--texto);}
  details[open] summary::before{content:"▼";}
  .content{
    padding:12px 14px 14px;
    line-height:1.6;
    background:#fff;
    color:var(--texto);
    border-top:1px solid #eee;
  }
  .refs a{
    display:inline-block;
    margin:4px 10px 4px 0;
    text-decoration:none;
    color:#fff;
    background:var(--vinho);
    padding:8px 12px;
    border-radius:8px;
    font-weight:700;
  }
  .refs a:hover{filter:brightness(.92);}
  .footer{
    background:var(--cinza);
    color:#111;
    text-align:center;
    padding:18px 12px;
    font-size:14px;
  }
  summary:focus{outline:3px solid #00000022;outline-offset:2px;}
  .section-title{
    margin:0;
    padding:8px 0 0;
    font-size:26px;
    font-weight:800;
    color:#3b0f1e;
  }
  .back-top{
    text-align:right; margin:12px 0 24px;
  }
  .back-top a{
    text-decoration:none; font-weight:700; color:#3b0f1e;
  }
</style>
</head>
<body>

  <!-- HERO + NAV GLOBAL -->
  <header class="hero" id="topo">
    <h1>Processamento de Vídeo</h1>
    <h2>Equipe RoadWatch</h2>
    <div class="nomes">
      Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier
    </div>
    <nav class="nav" aria-label="Navegação principal">
      <a href="#equipe">Página inicial</a>
      <a href="#fernanda">Fernanda</a>
      <a href="#gabriel">Gabriel</a>
      <a href="#ingrid">Ingrid</a>
      <a href="#relatorios">Relatórios</a><!-- NOVA ABA -->
    </nav>
  </header>

  <!-- =========================
       SEÇÃO: EQUIPE (HOME)
  ========================== -->
  <section id="equipe" class="container" aria-labelledby="titulo-equipe">
    <div class="tema">Tema: Monitoramento do uso de celular na direção veicular.</div>

    <!-- ETAPA 1 -->
    <details>
      <summary>ETAPA 1: Contexto e Cenário de Aplicação (CA)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
        <p><strong>Data:</strong> 01/10/2025</p>

        <h3>Introdução</h3>
        <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
          <p>A distração ao volante é um dos principais fatores que elevam o risco de acidentes de trânsito nos dias atuais, especialmente com o crescimento do uso de celulares. Segundo pesquisas da OMS e de outras organizações, o uso do celular enquanto dirige pode aumentar o risco de colisões em até 400%. No Brasil, essa prática já figura como a terceira maior causa de mortes no trânsito, assim já podemos perceber o perigoso efeito de dirigir distraído mexendo ou olhando no celular pode impactar.</p>
          <p>Em termos de infrações entre 2023 e 2024, mais de 50 mil condutores foram autuados por uso de celular ao volante — o que corresponde a uma média de quase 150 flagrantes por dia. Agora, olhando pelo  ponto de vista internacional, os dados mostram essa tendência de risco elevado, como por exemplo: nos Estados Unidos, em 2023, 3.275 pessoas morreram em acidentes em que a distração estava envolvida. Além disso, cerca de 12% dos acidentes fatais ao redor do mundo, praticamente todos estavam relacionados a distrações envolvendo o uso de telefone celular enquanto dirigem. Ainda, diversos estudos mostram ainda que tarefas visuais e manuais (como digitar ou deslizar no celular) estão entre as que mais aumentam o risco de acidentes de trânsito.</p>

          <p>O uso de celular ao volante é uma das principais causas de distração e acidentes, pois reduz o tempo de reação e aumenta o risco de colisões, colocando em perigo motoristas, passageiros e pedestres. Entrevistas empáticas com condutores e acompanhantes confirmaram que a distração pelo celular é frequente e que há consenso sobre a necessidade de um sistema de alerta rápido que mantenha a privacidade do usuário. Diante disso, este trabalho propõe um Sistema de Processamento de Vídeo (SPV), desenvolvido em C++ com a biblioteca OpenCV, que capta e analisa em tempo real as imagens do motorista. Ao identificar gestos típicos de uso do celular, como segurar o aparelho ou olhar para baixo, o sistema emite alerta sonoro e visual, incentivando o condutor a retomar a atenção. Além de aumentar a segurança e a conscientização, o projeto aplica conceitos da disciplina, como filtragem de imagens, processamento de cores, equalização de histograma, subtração de fundo e detecção de objetos, demonstrando aplicação prática e relevância social.</p>
        </div>

        <h3>Etapas de desenvolvimento</h3>
        <h4>(A) Problema a ser abordado</h4>
        <p>Dentro do conteúdo da disciplina de Processamento de Vídeo, a RoadWatch, será um aplicativo cujo a sua principal função é detectar automaticamente o uso do celular enquanto o motorista dirige. </p>

        <p><strong>Justificativa:</strong> Desenvolver um sistema que identifique automaticamente o uso do celular durante a condução contribui para a redução de acidentes, atende às recomendações de segurança viária e aproveita técnicas de processamento de vídeo e visão computacional abordadas na disciplina.</p>

        <h4>(B) Objetivo</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <p>Criar um Sistema de Processamento de Vídeo (SPV) capaz de:</p>
          <ul>
            <li><strong>Captura de vídeo:</strong> o RoadWatch utiliza a câmera frontal do veículo (ou do próprio smartphone fixado no painel ou no para‐brisa) para captar imagens contínuas do rosto, mãos e ambiente à frente do condutor.</li>
            <li><strong>Pré-processamento:</strong> as imagens são filtradas (redução de ruído, ajuste de brilho/contraste, normalização) e recortadas nas regiões de interesse (por exemplo, face, mãos, volante).</li>
            <li><strong>Detecção de objetos / poses:</strong> algoritmos identificam, nos frames, os elementos-chave — como a mão segurando um smartphone, a face do motorista e a posição da cabeça — por meio de técnicas de detecção baseadas em redes neurais ou modelos clássicos.</li>
            <li><strong>Classificação de ação / comportamento:</strong> com base em sequências de frames e características extraídas (ângulos de flexão de dedos, deslocamentos da mão, direção do olhar, tempo de fixação), o sistema classifica comportamentos em “uso do celular” ou “comportamento seguro”.</li>
            <li><strong>Alerta em tempo real:</strong> quando o sistema detecta um padrão de risco, ele emite um alerta audiovisual ou vibratório.</li>
            <li><strong>Registro e logging:</strong> o app registra os eventos detectados (tempo, tipo de distração, duração) para posterior análise.</li>
          </ul>
        </div>

        <h4>(C) Funcionamento do Sistema</h4>
        <p>O sistema inicia seu funcionamento assim que o motorista liga o veículo e aciona o programa instalado em um computador de bordo, notebook ou minipc conectado a uma webcam posicionada próxima ao retrovisor...</p>

        <h4>Exemplo de Uso Prático</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <p>Imagine que um motorista está trafegando a 60 km/h em via urbana e recebe uma mensagem de texto...</p>
          <ul>
            <li>movimento da mão em direção ao painel;</li>
            <li>presença do smartphone na mão;</li>
            <li>desvio do olhar;</li>
            <li>padrão temporal de digitação.</li>
          </ul>
        </div>

        <h4>(D) Benefícios esperados</h4>
        <div style="font-family: Arial, sans-serif; line-height: 1.5;">
          <ul>
            <li><strong>Segurança:</strong> reduz riscos de acidentes ao alertar sobre distrações.</li>
            <li><strong>Conscientização:</strong> o relatório de uso reforça hábitos de direção segura.</li>
            <li><strong>Baixo custo e simplicidade:</strong> usa câmera e processamento local.</li>
            <li><strong>Integração acadêmica:</strong> conecta os conceitos da disciplina a um caso real.</li>
          </ul>
        </div>

        <h4>Considerações práticas e limitações:</h4>
        <ul>
          <li>Qualidade da câmera impacta a precisão.</li>
          <li>Condições adversas dificultam a classificação.</li>
          <li>Latência precisa ser mínima.</li>
          <li>Privacidade e consentimento se gravar imagens.</li>
        </ul>

        <h3>Referências</h3>
        <ul>
          <li>Entrevistas Fernanda</li>
          <li>Entrevistas Gabriel</li>
          <li>Entrevistas Ingrid</li>
        </ul>

        <div class="refs" style="display:flex; gap:12px; flex-wrap:wrap;">
          <a href="./Entrevistas_Fernanda.pdf" target="_blank" rel="noopener">Abrir Entrevistas Fernanda (PDF)</a>
          <a href="./Entrevistas_Gabriel.pdf" target="_blank" rel="noopener">Abrir Entrevistas Gabriel (PDF)</a>
          <a href="./Entrevistas_Ingrid.pdf" target="_blank" rel="noopener">Abrir Entrevistas Ingrid (PDF)</a>
        </div>

      </div><!-- FIM do .content da ETAPA 1 -->
    </details><!-- FIM do <details> da ETAPA 1 -->

    <!-- ETAPA 2 -->
      <details>
        <summary>ETAPA 2: Modelagem Funcional do Sistema (MF)</summary>
        <div class="content">
          <p>Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
          <p><strong>Data:</strong> 13/10/2025</p>
      
          <h3>Descrição Geral</h3>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <p>
              O RoadWatch é um sistema de monitoramento inteligente que tem como objetivo 
              identificar, em tempo real, se o motorista está utilizando o celular enquanto dirige. 
              A proposta busca reduzir acidentes causados por distrações no trânsito, 
              detectando o comportamento do condutor através de uma câmera instalada no veículo. 
              O sistema analisa continuamente o vídeo capturado, verificando padrões como 
              posição das mãos, olhar do motorista e presença de um celular em cena.
            </p>
            <p>
              Durante a modelagem funcional, foram definidos os blocos principais que compõem 
              o funcionamento do sistema, bem como as entradas, saídas e processamentos de cada um. 
              Essa concepção funcional orienta o desenvolvimento de cada módulo de software e 
              auxilia na integração entre câmera e sensores e o processamento digital.
            </p>
          </div>
      
          <h3>Diagrama de Blocos - Modelagem Funcional</h3>
          <figure>
            <img src="imagens/Diagrama de Blocos RoadWatch.png" alt="Diagrama de blocos do sistema RoadWatch, mostrando os módulos de captura de imagem, pré-processamento, detecção de objetos, análise de comportamento e geração de alerta." style="max-width:100%; height:auto;">
            <figcaption>Figura 1 – Diagrama de blocos do sistema RoadWatch.</figcaption>
          </figure>
      
          <h4>Descrição dos Blocos Funcionais</h4>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <ol>
              <li>
                <strong>Bloco 1 – Captura de Imagem</strong><br>
                Entrada: vídeo em tempo real da câmera frontal voltada ao motorista.<br>
                Processamento: coleta contínua de quadros (frames) com resolução suficiente para detecção de rosto e mãos, 
                ajustando taxa de quadros e brilho automaticamente conforme as condições de iluminação.<br>
                Saída: sequência de frames de vídeo prontos para pré-processamento.
              </li><br>
      
              <li>
                <strong>Bloco 2 – Pré-processamento de Imagem</strong><br>
                Entrada: frames capturados pelo módulo anterior.<br>
                Processamento: redimensionamento, normalização, correção de iluminação, 
                e definição de uma Região de Interesse focada na cabeça e mãos do motorista. 
                Essa etapa reduz ruídos e o custo computacional da análise.<br>
                Saída: imagens tratadas e otimizadas para detecção.
              </li><br>
      
              <li>
                <strong>Bloco 3 – Detecção de Objetos e Posições</strong><br>
                Entrada: imagens pré-processadas.<br>
                Processamento: aplicação de modelo de detecção baseado em redes neurais, identificando a localização da face, das mãos e do celular. 
                Cada detecção retorna uma coordenada e uma pontuação de confiança.<br>
                Saída: mapa de detecções e suas respectivas probabilidades.
              </li><br>
      
              <li>
                <strong>Bloco 4 – Análise de Comportamento do Motorista</strong><br>
                Entrada: resultados de detecção dos frames e histórico recente de posições.<br>
                Processamento: análise temporal dos frames para verificar gestos ou ações típicas 
                do uso de celular, como olhar para baixo ou segurar o telefone. 
                Pode ser utilizada uma rede de classificação de ações ou um filtro temporal.<br>
                Saída: rótulo de comportamento (ex.: “atento”, “usando celular”, “mão próxima ao rosto”).
              </li><br>
      
              <li>
                <strong>Bloco 5 – Verificação de Movimento do Veículo</strong><br>
                Entrada: dados de sensores como o GPS.<br>
                Processamento: determina se o veículo está em movimento acima de uma velocidade mínima 
                (ex.: 5 km/h), evitando que o alerta seja acionado com o carro parado.<br>
                Saída: estado do veículo (“em movimento” / “parado”).
              </li><br>
      
              <li>
                <strong>Bloco 6 – Fusão de Informações e Decisão</strong><br>
                Entrada: resultados da análise de comportamento e estado do veículo.<br>
                Processamento: combina os resultados, aplicando regras lógicas e filtros temporais 
                (por exemplo, exigir que o comportamento seja detectado em 3 frames consecutivos 
                para confirmar o evento).<br>
                Saída: decisão final (“alerta” ou “seguro”).
              </li><br>
      
              <li>
                <strong>Bloco 7 – Geração de Alertas e Registro</strong><br>
                Entrada: sinal de alerta gerado pelo módulo de decisão.<br>
                Processamento: aciona o alerta sonoro e/ou visual no painel do veículo 
                e registra o evento no log com data, hora e tipo de distração.<br>
                Saída: feedback imediato ao motorista e registro do incidente.
              </li>
            </ol>
          </div>
      
          <h4>Conclusão</h4>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
            <p>
              A modelagem funcional do RoadWatch define claramente as etapas envolvidas 
              na identificação de distrações do motorista, desde a captura das imagens até 
              a emissão do alerta. A decomposição em blocos facilita o desenvolvimento modular, 
              a validação individual de cada componente e a futura integração com hardware real.
            </p>
            <p>
              Essa abordagem garante que o sistema possa evoluir de forma estruturada, 
              permitindo substituição ou aprimoramento de módulos específicos 
              (por exemplo, trocar o modelo de detecção por outro mais eficiente) 
              sem comprometer o funcionamento geral do sistema.
            </p>
          </div>
        </div>
      </details>

    <!-- ETAPA 3 -->
    <details>
      <summary>ETAPA 3: Seminário S1 do Trabalho</summary>
      <div class="content">
        <div class="refs">
          <a href="imagens/seminários1.pdf" download>Download Seminário S1</a>
        </div>
      </div>
    </details>

    <!-- ETAPA 4 -->
    <details>
      <summary>ETAPA 4: Desenvolvimento do Sistema de Processamento da Visão (SPV)</summary>
      <div class="content">Coloque aqui o conteúdo da Etapa 4.</div>
    </details>

    <!-- ETAPA 5 -->
    <details>
      <summary>ETAPA 5: Desenvolvimento do laboratório experimental (LEx)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier </p>
        <p><strong>Data:</strong> 12/11/2025</p>

        <h3>Objetivo</h3>
        <p>Preparar o Teste de Campo (TC) do SPV por meio do LEx...</p>

        <h3>Roteiro do Laboratório Experimental (para cada aplicação)</h3>
        <ul>
          <li>Roteiro de procedimento experimental...</li>
          <li>Figuras/imagens explicativas...</li>
        </ul>

        <h3>Introdução</h3>
        <p><!-- explicar funcionamento --></p>

        <h3>Procedimento experimental</h3>
        <p><!-- instruções detalhadas --></p>

        <h3>Questionário de avaliação do usuário</h3>
        <ul>
          <li>(i) Entendeu o assunto?</li>
          <li>(ii) Obteve resultados esperados?</li>
          <li>(iii) Entendeu a aplicação?</li>
        </ul>

        <h3>Notas e planilha de médias</h3>
        <ul>
          <li>Atribuir nota por questão; calcular média...</li>
          <li>Enviar a planilha pelo Moodle...</li>
        </ul>

        <h3>Enquete Subjetiva de Opinião (ESO)</h3>
        <ul>
          <li>Perguntas abertas.</li>
          <li>Perguntas com escala 1–5.</li>
        </ul>
      </div>
    </details>

    <!-- ETAPA 6 -->
    <details>
      <summary>ETAPA 6: Teste de Campo do SPV (TC)</summary>
      <div class="content">Coloque aqui o conteúdo da Etapa 6.</div>
    </details>

    <!-- ETAPA 7 -->
    <details>
      <summary>ETAPA 7: Relatório Final do Trabalho (RFT)</summary>
      <div class="content">
        <p>Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier </p>
        <p><strong>Data:</strong> 24/11/2025</p>

        <h3>Introdução</h3>
        <ul>
          <li><strong>Objetivos do trabalho</strong> — <!-- objetivos --></li>
        </ul>

        <h3>Cenário de Aplicação (CA)</h3>
        <p><!-- contexto --></p>

        <h3>Fundamentação teórica</h3>
        <p><!-- teorias e referências --></p>

        <h3>Materiais e métodos</h3>

        <h4>Modelagem Funcional do SPV (MF)</h4>
        <p><!-- diagrama e papéis --></p>

        <h4>Descrição da implementação do SPV</h4>
        <p><!-- algoritmos e pipeline --></p>

        <h4>Lista dos arquivos</h4>
        <ul>
          <li><strong>Códigos-fonte</strong>: <!-- ex.: /src --></li>
          <li><strong>Imagens</strong>: <!-- ex.: /data/images --></li>
          <li><strong>Vídeos</strong>: <!-- ex.: /data/videos --></li>
          <li><strong>Arquivos auxiliares</strong>: <!-- pesos/configs --></li>
        </ul>

        <h4>Análise técnica</h4>
        <ul>
          <li><strong>Métricas objetivas</strong> — precisão, sensibilidade...</li>
          <li><strong>Métricas qualitativas</strong> — usabilidade, feedback...</li>
          <li><strong>Resultados e conclusões parciais</strong> — achados principais...</li>
        </ul>

        <h3>Laboratório Experimental</h3>
        <h4>Roteiro do LEx</h4>
        <p><!-- passos e condições --></p>

        <h4>Análise dos Resultados do TC</h4>
        <ul>
          <li><strong>Experimentos realizados</strong> — ...</li>
          <li><strong>Critérios de avaliação</strong> — ...</li>
          <li><strong>Médias dos alunos</strong> — ...</li>
          <li><strong>Opiniões subjetivas</strong> — ...</li>
        </ul>

        <h3>Conclusões</h3>
        <p><!-- objetivos atingidos; pontos positivos/negativos --></p>

        <h3>Referências Bibliográficas</h3>
        <ul>
          <li><!-- AUTOR. Título. Editora/Conferência, ano. DOI/URL. --></li>
        </ul>

        <h3>Anexo</h3>
        <p><!-- códigos completos/links --></p>
      </div>
    </details>

    <!-- ETAPA 8 -->
    <details>
      <summary>ETAPA 8: Seminário S2 do Trabalho</summary>
      <div class="content">
        <div class="refs">
          <a href="downloads/seminarios2.pdf" download>Download Seminário S2</a>
        </div>
      </div>
    </details>

    <div class="back-top"><a href="#topo" title="Voltar ao topo">↑ Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SEÇÃO: FERNANDA
  ========================== -->
  <section id="fernanda" class="container" aria-labelledby="titulo-fernanda">
    <h2 id="titulo-fernanda" class="section-title">Página da Fernanda</h2>
    <details>
      <summary>Conheça Fernanda</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Engenharia de Informação da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Fernanda.png" alt="Foto da Fernanda em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Fernanda em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarFernanda.png" alt="Avatar digital em estilo semi-realista da Fernanda" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar da Fernanda.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">↑ Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SEÇÃO: GABRIEL
  ========================== -->
  <section id="gabriel" class="container" aria-labelledby="titulo-gabriel">
    <h2 id="titulo-gabriel" class="section-title">Página do Gabriel</h2>
    <details>
      <summary>Conheça Gabriel</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Ciência da Computação da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Gabriel.png" alt="Foto do Gabriel em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Gabriel em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarGabriel.png" alt="Avatar digital em estilo semi-realista do Gabriel" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar do Gabriel.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">↑ Voltar ao topo</a></div>
  </section>

  <!-- =========================
       SEÇÃO: INGRID
  ========================== -->
  <section id="ingrid" class="container" aria-labelledby="titulo-ingrid">
    <h2 id="titulo-ingrid" class="section-title">Página da Ingrid</h2>
    <details>
      <summary>Conheça Ingrid</summary>
      <div class="content">
        <p style="text-align:center;">Estudante de Engenharia de Informação da UFABC</p>

        <div style="display:flex; justify-content:center; align-items:center; gap:24px; flex-wrap:wrap; margin-top:16px;">
          <figure style="text-align:center; margin:0;">
            <img src="imagens/Ingrid.png" alt="Foto da Ingrid em sala de aula" style="max-width:250px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Ingrid em sala de aula.</figcaption>
          </figure>

          <figure style="text-align:center; margin:0;">
            <img src="imagens/avatarIngrid.png" alt="Avatar digital em estilo semi-realista da Ingrid" style="max-width:200px; height:auto; display:block; margin:0 auto;">
            <figcaption style="margin-top:6px; font-size:14px; color:#000;">Avatar da Ingrid.</figcaption>
          </figure>
        </div>
      </div>
    </details>
    <div class="back-top"><a href="#topo">↑ Voltar ao topo</a></div>
  </section>


  <!-- =========================
       SEÇÃO: RELATÓRIOS (NOVA)
  ========================== -->
  <section id="relatorios" class="container" aria-labelledby="titulo-relatorios">
    <h2 id="titulo-relatorios" class="section-title">Relatórios</h2>

    <details>
      <summary>Relatório 1</summary>

  <header class="content">
      Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier
      <p><strong>Data:</strong> 06/10/2025</p>
    </div>
  </header>

      <div class="content">
          <h3>1. Introdução</h3>
          <div style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify;">
          <p> O presente Relatório 1 documenta as atividades iniciais da disciplina ESZI032 – Processamento de Vídeo, cujo objetivo é iniciar o uso do OpenCV, compreender os comandos básicos para visualizar e gravar imagens e vídeos e produzir um primeiro vídeo demonstrativo a ser inserido no relatório. As tarefas foram organizadas em três partes: preparação do ambiente e execução guiada (Parte 1), estudo e adaptação de operações básicas em imagens e vídeos (Parte 2) e obtenção de material próprio com webcam (Parte 3), conforme roteiro do Laboratório 1 – Captura de Imagem e Vídeo (2025.3).</p>
          <p>Na Parte 1, foi configurado o ambiente seguindo as instruções disponibilizadas no Moodle. Na Parte 2, estudamos exemplos de leitura, exibição e gravação de imagens (incluindo a adaptação do tutorial para a imagem messi5.jpg e salvamento em .png) e de leitura/gravação de vídeos (execução dos cinco programas indicados e adaptação para o arquivo big_buck_bunny.mp4), descrevendo a função de cada programa e seu uso em processamento de vídeo. Por fim, na Parte 3, produzimos materiais próprios: uma foto do grupo (com roupas destacando as cores RGB), uma montagem de “avatares” e quatro vídeos de teste (movimentos lentos/rápidos com pessoas e com objeto), que servirão de base para análises futuras e evolução do projeto.</p>

<p>Além de cumprir os objetivos imediatos do laboratório, este relatório busca tornar-se uma referência prática para a equipe RoadWatch sobre a programação de entrada e saída com câmeras, bem como o manuseio de arquivos de imagem e vídeo, estabelecendo um padrão de organização e clareza que será reaproveitado nas próximas etapas.</p>
          </div>

  
  <div class="card">
  <h3>2. Fundamentos Básicos</h3>

  <h4>2.1. Imagem digital e representação em memória</h4>
  <p>Uma imagem digital é uma matriz <em>H × W</em> de pixels com <em>C</em> canais (por exemplo, 3 para imagens coloridas). No OpenCV (C++), a estrutura mais usada é <code>cv::Mat</code>, com valores normalmente inteiros de 8 bits por canal (0–255) em imagens usuais.</p>

  <h4>2.2. Operações básicas em imagens (E/S)</h4>
  <ul>
    <li><strong>Leitura</strong> (entrada): carregar arquivo de imagem com <code>cv::imread()</code>.</li>
    <li><strong>Exibição</strong>: mostrar a imagem com <code>cv::imshow()</code> e aguardar tecla com <code>cv::waitKey()</code>.</li>
    <li><strong>Gravação</strong> (saída): salvar em outro formato com <code>cv::imwrite()</code> (por exemplo, PNG).</li>
  </ul>
  <p>Essas três operações consolidam o fluxo de entrada/saída e permitem conversão simples de formatos.</p>

  <h4>2.3. Vídeo: frames, FPS, resolução, codec e contêiner</h4>
  <ul>
    <li><strong>Frame</strong>: cada imagem individual da sequência do vídeo.</li>
    <li><strong>FPS</strong>: quadros por segundo; influencia a fluidez visual.</li>
    <li><strong>Resolução</strong>: largura × altura; afeta nitidez e tamanho do arquivo.</li>
    <li><strong>Codec/Contêiner</strong>: compressão e empacotamento (ex.: MP4/<code>mp4v</code>).</li>
  </ul>
  <p>Na prática, usamos <code>cv::VideoCapture</code> para leitura e <code>cv::VideoWriter</code> para gravação, ajustando FPS, resolução e codec conforme o objetivo.</p>

  <h4>2.4. Captura com webcam (dispositivos de vídeo)</h4>
  <ol>
    <li>Abrir o dispositivo: <code>cv::VideoCapture(0)</code> (ou outro índice/caminho).</li>
    <li>Ler frames em laço; opcionalmente exibir e/ou gravar.</li>
    <li>Finalizar liberando recursos (fechar janelas e <code>release()</code>).</li>
  </ol>
  <p>Condições como iluminação e velocidade de movimento influenciam nitidez, desfoque e taxa de compressão.</p>

  <h4>2.5. Organização dos exemplos e referencial de estudo</h4>
  <p>Para cada programa executado, descreva o que ele faz, os parâmetros usados (caminhos, FPS, resolução, codec) e como pode ser reutilizado no projeto. Essa documentação vira um guia rápido para as próximas etapas.</p>

  <h4>2.6. Conexão com os objetivos do Lab 1</h4>
  <p>Os conceitos acima (E/S de imagens, leitura/gravação de vídeos e captura com webcam) atendem aos objetivos do Laboratório 1 e servem de base para o desenvolvimento do sistema RoadWatch nas etapas seguintes.</p>
</div>


<div class="card">
  <h3>3. Materiais e Métodos</h3>

  <h4>3.1. Diagrama de Blocos Funcional</h4>
  <figure>
    <img src="imagens/Diagrama de Blocos RoadWatch.png"
         alt="Diagrama de blocos do sistema RoadWatch, com captura pela câmera, pré-processamento, detecção de gestos de uso do celular e emissão de alertas."
         style="max-width:100%;height:auto">
    <figcaption>Fluxo funcional do SPV: Captura → Pré-processamento → Detecção → Alerta/Registro.</figcaption>
  </figure>

  <h4>3.2. Ambiente de Experimentos</h4>
  <ul>
    <li><strong>Sistema Operacional:</strong> Ubuntu 22.04 LTS (64-bit)</li>
    <li><strong>Compilador / Interpretação:</strong> g++ ≥ 11 (C++) ou Python ≥ 3.10</li>
    <li><strong>Bibliotecas:</strong> OpenCV ≥ 4.x (core, imgcodecs, highgui, videoio)</li>
    <li><strong>Hardware:</strong> Webcam USB 720p/1080p; notebook/minipc</li>
    <li><strong>Arquivos de teste (vídeo/imagem):</strong> <code>messi5.jpg</code>, <code>big_buck_bunny.mp4</code></li>
    <li><strong>Ferramentas auxiliares:</strong> Visualizador de imagens, reprodutor de vídeo, terminal</li>
  </ul>

  <h4>3.3. Procedimentos Experimentais</h4>
  <ol>
    <li><strong>Imagem – leitura/exibição/gravação:</strong>
      carregar <code>messi5.jpg</code>, exibir em janela e salvar como <code>saida.png</code> (verificar que o arquivo foi gerado).</li>
    <li><strong>Vídeo – leitura:</strong> abrir <code>big_buck_bunny.mp4</code> com <code>VideoCapture</code>, ler frames em laço e exibir.</li>
    <li><strong>Vídeo – gravação:</strong> gravar um trecho para <code>saida.mp4</code> definindo resolução e FPS (ex.: 640×480 @ 30fps).</li>
    <li><strong>Webcam – captura de foto:</strong> capturar uma imagem do grupo e salvar como <code>foto_grupo.png</code>
      (roupas com cores RGB destacadas).</li>
    <li><strong>Webcam – “avatares”:</strong> montar uma imagem com retratos (avatars) dos integrantes e salvar como <code>avatares.png</code>.</li>
    <li><strong>Webcam – vídeos próprios:</strong> gravar 4 vídeos curtos:
      <ul>
        <li>Pessoa com movimento <em>lento</em> e <em>rápido</em></li>
        <li>Objeto com movimento <em>lento</em> e <em>rápido</em></li>
      </ul>
      Nomear como <code>pessoa_lento.mp4</code>, <code>pessoa_rapido.mp4</code>, <code>objeto_lento.mp4</code>, <code>objeto_rapido.mp4</code>.
    </li>
    <li><strong>Anotações no relatório:</strong> descrever o que cada programa faz, parâmetros usados (FPS, resolução, codec) e observações sobre iluminação, nitidez, desfoque e compressão.</li>
  </ol>

  <h4>3.4. Organização de Arquivos</h4>
  <pre><code>projeto/
├─ imagens/
│  ├─ messi5.jpg
│  ├─ saida.png
│  └─ avatares.png
├─ videos/
│  ├─ big_buck_bunny.mp4
│  ├─ saida.mp4
│  ├─ pessoa_lento.mp4
│  ├─ pessoa_rapido.mp4
│  ├─ objeto_lento.mp4
│  └─ objeto_rapido.mp4
└─ src/
   ├─ img_io.cpp        # leitura/exibição/gravação de imagem
   ├─ video_read.cpp    # leitura de vídeo
   ├─ video_write.cpp   # gravação de vídeo
   └─ webcam_cap.cpp    # captura da webcam
</code></pre>

  <h4>3.5. Parâmetros e Configuração Utilizada</h4>
  <ul>
    <li><strong>Resoluções:</strong> 640×480 e 1280×720</li>
    <li><strong>FPS:</strong> 24 e 30 (comparar fluidez)</li>
    <li><strong>Codec:</strong> <code>mp4v</code> (MP4) para arquivos de saída</li>
    <li><strong>Iluminação:</strong> ambiente interno com luz frontal difusa; repetir com luz lateral</li>
    <li><strong>Duração dos vídeos:</strong> 5–10 s (cada)</li>
  </ul>

  <h4>3.6. Critérios de Validação e Registro</h4>
  <ul>
    <li>Verificar se a <strong>resolução e FPS</strong> do arquivo de saída batem com o solicitado.</li>
    <li>Registrar <strong>observações</strong> de nitidez, ruído, desfoque por movimento e variações de iluminação.</li>
    <li>Inserir no relatório <strong>figuras</strong> (frames representativos) com <strong>legenda acessível</strong> e breve análise.</li>
    <li>Guardar <strong>prints de terminal</strong> com logs de execução e mensagens de erro (se houver).</li>
  </ul>

        <div class="card">
          <h3>4. Resultados e Análises</h3>
          <h4>(A) Leitura de imagem em arquivo e exibir na tela:</h4>
          <p style="text-align: justify;"> As imagens apresentadas correspondem à execução prática do tutorial “Getting Started with Images” da biblioteca OpenCV, utilizando a imagem messi5.jpg. O objetivo é demonstrar a leitura de uma imagem a partir de um arquivo e sua exibição em uma janela na tela.</p>
      
          <p><strong>Primeira imagem — Execução pela linha de comando (C++)</strong></p>
       <p style="text-align: justify;">Vê-se a janela do programa DisplayImage, aberta com o título “Display Image”.
Dentro dessa janela, aparece uma fotografia colorida do jogador de futebol Lionel Messi. A figura demonstra o funcionamento correto do programa em C++ para leitura e exibição de imagens, conforme o tutorial da documentação do OpenCV. A execução confirma que o código foi compilado e executado com sucesso, exibindo a imagem carregada.</p>
          <div style="text-align: center; font-family: Arial, sans-serif;">
  <figure>
    <img src="imagens/captura_displayimage.png" 
         alt="Captura de tela no Linux mostrando a execução do programa DisplayImage no terminal. Uma janela chamada 'Display Image' exibe uma foto de um jogador de futebol em uniforme azul e grená do Barcelona, realizando um movimento com a bola em campo." 
         style="max-width: 90%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
    <figcaption style="margin-top: 8px; font-size: 14px; color: ##000000;">
      Captura de tela do programa <b>DisplayImage</b> em execução no terminal Linux, 
      exibindo uma janela com a imagem de um jogador de futebol em campo. 
      Esse resultado demonstra o correto funcionamento do código para abertura de imagens.
    </figcaption>
  </figure>
</div>

<p><strong>Segunda imagem — Execução em Python</strong></p>
<p style="text-align: justify;">Abre-se uma janela intitulada “image”, exibindo a mesma fotografia de Messi, agora processada via script Python utilizando as funções cv2.imread() e cv2.imshow().O resultado visual confirma que o código em Python reproduz o mesmo comportamento do programa em C++, realizando corretamente a leitura e exibição da imagem messi5.jpg.</p>
  <div style="text-align: center; font-family: Arial, sans-serif;">
  <figure>
    <img src="imagens/py.png" 
         alt="Captura de tela no Linux mostrando a execução do programa DisplayImage no terminal. Uma janela chamada 'Display Image' exibe uma foto de um jogador de futebol em uniforme azul e grená do Barcelona, realizando um movimento com a bola em campo." 
         style="max-width: 90%; height: auto; border: 1px solid #ccc; border-radius: 8px;">
    <figcaption style="margin-top: 8px; font-size: 14px; color: ##000000;">
      Configurando o Python
    </figcaption>
  </figure>
</div>

  <h4> (B) Leitura e gravação de vídeo: </h4>
  <h4 style="text-align:left; margin: 16px 32px;"> 1) video_read_from_files</h4>
  <p style="margin: 16px 32px; text-align: justify;"> 
    O programa utiliza a biblioteca OpenCV para abrir, ler e exibir um vídeo a partir de um arquivo, mostrando cada frame em uma nova janela e permitindo que o usuário interrompa a reprodução pressionando a tecla 'q'. 
  </p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-38-24.png" 
       alt="Descrição acessível da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Frame do vídeo "Cars.mp4", mostrando a captura em tempo real após a execução do programa.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;">2) video_read_from_image_sequence</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa lê uma sequência de imagens numeradas e as exibe em sequência como se fossem um vídeo, utilizando a biblioteca OpenCV.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-53-22.png" 
       alt="Descrição acessível da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    A imagem exibe um frame da sequência de imagens enumeradas.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 3) video_read_from_webcam</h4>
<p style="margin: 16px 32px; text-align: justify;"> 
  O programa captura vídeo em tempo real a partir da webcam do computador, exibindo os quadros em uma nova janela.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-44-32.png" 
       alt="Descrição acessível da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Captura de tela a partir da webcam do computador.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 4) video_write_from_webcam</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa captura um vídeo em tempo real a partir da webcam do computador, exibindo os quadros em uma nova janela e simultaneamente salvando o vídeo em um novo arquivo.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <div style="position:relative; max-width:60%; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Gravação de Tela 2025-10-04 145536.mp4" type="video/mp4">
      Seu navegador não suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Captura de vídeo em tempo real a partir da webcam do computador.
  </figcaption>

<h4 style="text-align:left; margin: 16px 32px;"> 5) video_write_to_file</h4>
<p style="margin: 16px 32px; text-align: justify;">
  O programa lê um vídeo pré-existente, exibe os quadros em tempo real em uma nova janela e simultaneamente grava o conteúdo em um novo arquivo de vídeo.
</p>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/Captura de tela de 2025-10-01 11-45-32.png" 
       alt="Descrição acessível da imagem exibida" 
       style="max-width:60%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    A imagem exibe um frame do vídeo "Cars.mp4", acionado após a execução do programa.
  </figcaption>

        
      <h4 style="text-align:left;"> Obtenção de Fotos e Vídeos</h4>
      <h4 style="text-align:left;"> a) </h4>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/equipe.png" 
       alt="Descrição acessível da imagem exibida" 
       style="max-width:90%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Foto de todos integrantes do grupo obtida via webcam.
  </figcaption>
</figure>

  <h4 style="text-align:left;"> b) </h4>
      <figure style="text-align:center; margin:16px auto;">
  <img src="imagens/avatares_juntos.png"
       alt="Descrição acessível da imagem exibida" 
       style="max-width:70%; height:auto; border:1px solid #ccc; border-radius:8px;">
  <figcaption style="margin-top:8px; font-size:14px; color:#00000;">
    Avatares dos integrantes do grupo.
  </figcaption>
</figure>

 <h4 style="text-align:left;"> c) Vídeo com a webcam com pessoas e com um objeto:</h4>
    <figure style="text-align:center">
  <div style="position:relative; max-width:900px; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-34-00.webm" type="video/webm">
      Seu navegador não suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption>Leitura e gravação de vídeo com mudanças lentas de movimento  — reprodução local.</figcaption>

  <!-- teste: abrir o arquivo direto -->
  <p style="margin-top:8px">
    <a href="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-34-00.webm" target="_blank" rel="noopener">
      Abrir arquivo WEBM diretamente
    </a>
  </p>
</figure>

    <figure style="text-align:center">
  <div style="position:relative; max-width:900px; margin:12px auto; aspect-ratio:16/9;">
    <video id="vid1"
           controls
           preload="metadata"
           playsinline
           style="width:100%; height:100%; display:block; border:1px solid #ddd; border-radius:10px;">
      <source src="imagens/Gravação de tela de 2025-10-01 11-35-37.webm " type="video/webm">
      Seu navegador não suporta o elemento <code>video</code>.
    </video>
  </div>
  <figcaption>Leitura e gravação de vídeo com mudanças rápidas de movimento — reprodução local.</figcaption>

  <!-- teste: abrir o arquivo direto -->
  <p style="margin-top:8px">
    <a href="imagens/Grava%C3%A7%C3%A3o%20de%20tela%20de%202025-10-01%2011-35-37.webm" target="_blank" rel="noopener">
      Abrir arquivo WEBM diretamente
    </a>
  </p>
</figure>


      <div class="card">
        <h3 style="text-align:left;">5. Conclusões</h3>
        	<p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             O Laboratório 1 permitiu que o grupo se familiarizasse com o uso do OpenCV, 
          	 consolidando conhecimentos sobre leitura, exibição e gravação de imagens e vídeos, 
          	 além da prática de captura com webcam. As atividades reforçaram o entendimento de conceitos como 
             resolução, representação de frames e formatos de arquivo, bem como a importância de fatores 
             externos como iluminação e velocidade de movimento na qualidade do material obtido.
        	</p>
        	<p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             Entre as principais dificuldades observadas estão a necessidade de ajustes finos de parâmetros 
             para garantir nitidez e fluidez nos vídeos, além dos efeitos causados por variações de iluminação, 
        	   que influenciam diretamente a qualidade da captura. Tais desafios, entretanto, contribuíram para a 
        	   compreensão prática do impacto desses fatores em aplicações de visão computacional.
        	</p>
  	      <p style="font-family: Arial, sans-serif; line-height: 1.5; text-align: justify; margin: 16px 32px;">
             Como próximo passo, este relatório servirá como referência para as etapas seguintes da disciplina 
             e para o desenvolvimento do sistema RoadWatch, um programa que, por meio de uma câmera, 
             será capaz de identificar se o motorista está utilizando o celular durante a condução. 
             Além de guiar a organização de arquivos, a padronização de experimentos e a documentação dos resultados, 
             esta base inicial estabelece condições sólidas para avançar no nosso projeto final.
  	      </p>
        </div>
      </div>
    </details>
  </div>
    
      <!-- RELATÓRIO 2 -->
  <details>
    <summary>Relatório 2</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 15/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
      <p style="text-align:justify;">Esta parte do trabalho investiga, de forma sistemática, o efeito de diferentes filtros espaciais e tamanhos de máscara na qualidade de imagens estáticas e em vídeo. Partimos do conjunto de imagens próprias obtidas no Lab 1 e aplicamos quatro técnicas clássicas de suavização: média, gaussiano, mediana e bilateral. Para cada filtro, variamos o tamanho do kernel (3×3, 5×5, 7×7 e 11×11) e salvamos os resultados em formato `.jpg`, de modo a permitir comparações diretas entre combinações de método e escala. O objetivo é compreender o compromisso entre redução de ruído e preservação de detalhes (bordas e texturas), bem como o impacto da granularidade da vizinhança na suavização.</p>

<p style="text-align:justify;">Na sequência, repetimos os experimentos sob uma condição mais adversa: a adição de ruído sal-e-pimenta à imagem original. Esse cenário é particularmente útil para destacar diferenças entre filtros que operam por média/ponderação (média e gaussiano), filtros não lineares robustos a impulsos (mediana) e o filtro bilateral, que combina proximidade espacial e similaridade radiométrica para preservar bordas. A análise contempla tanto a variação do kernel dentro de cada filtro quanto a comparação entre filtros para um mesmo kernel.</p>

<p style="text-align:justify;">Por fim, avaliamos o comportamento em tempo real com entrada de webcam, mostrando continuamente a imagem filtrada em uma janela do OpenCV e permitindo capturas (`.jpg`) via teclado. As escolhas dos dois filtros e kernels usados nesta etapa derivam dos melhores e piores desempenhos observados no cenário com ruído, consolidando a discussão com um caso prático. Opcionalmente, propomos ainda uma extensão interativa que habilita a seleção dinâmica do tipo de filtragem e do tamanho da máscara pelo usuário. Para reprodutibilidade e organização, cada programa é mantido em pastas distintas, com saídas nomeadas de forma consistente.
</p>
      <h3>2. Fundamentos Básicos</h3>
  <p><strong>Objetivo:</strong> reduzir variações de alta frequência (ruído/detalhes finos) preservando estruturas relevantes (bordas e texturas). É etapa comum de pré-processamento para segmentação, detecção de bordas e descrição de regiões.</p>

  <h3>1) Modelo de ruído</h3>
  <p>Imagem observada: <code>g(x,y) = f(x,y) + n(x,y)</code>, onde <em>f</em> é a imagem ideal e <em>n</em> o ruído (gaussiano, sal-e-pimenta etc.). A filtragem busca aproximar <em>f</em> atenuando <em>n</em>.</p>

  <h3>2) Filtragem espacial e convolução</h3>
  <p>Filtros aplicam uma máscara (<em>kernel</em>) sobre vizinhanças. Em filtros lineares:</p>
  <p style="margin:8px 0"><code>I'(x,y) = Σ Σ K(i,j) · I(x+i, y+j)</code></p>
  <p>É necessário definir tratamento de borda (<code>padding</code>: replicar, refletir, constante).</p>

  <h3>3) Tamanho do kernel (3×3, 5×5, 7×7, 11×11)</h3>
  <ul>
    <li><strong>Maior kernel</strong>: remove mais ruído, porém aumenta borramento e custo computacional.</li>
    <li><strong>Menor kernel</strong>: preserva detalhes/bordas, remove menos ruído.</li>
  </ul>

  <h3>4) Tipos de filtro</h3>
  <ul>
    <li><strong>Média (box blur) — linear:</strong> substitui pelo valor médio da vizinhança. Simples e rápido; tende a borrar bordas.</li>
    <li><strong>Gaussiano — linear:</strong> pesos conforme distribuição normal (maior peso no centro). Melhor que a média para preservar bordas; parâmetro-chave: <em>σ</em>.</li>
    <li><strong>Mediana — não linear:</strong> substitui pelo valor mediano. Excelente para <em>sal-e-pimenta</em>; preserva bordas finas; pode “achatar” texturas.</li>
    <li><strong>Bilateral — não linear (espaço + intensidade):</strong> suaviza regiões mantendo bordas ao reduzir peso quando a diferença de intensidade é grande. Qualidade alta, custo computacional maior; sensível a <code>d</code>, <code>sigmaColor</code>, <code>sigmaSpace</code>.</li>
  </ul>

  <h3>5) Ruído sal-e-pimenta</h3>
  <ul>
    <li><strong>Mediana</strong> costuma ser a melhor escolha (remove impulsos sem espalhá-los).</li>
    <li><strong>Média/Gaussiano</strong> reduzem mas podem espalhar impulsos (halos).</li>
    <li><strong>Bilateral</strong> pode funcionar bem, porém exige ajuste fino e é mais lento.</li>
  </ul>

  <h3>6) Métricas para análise</h3>
  <ul>
    <li><strong>Avaliação visual</strong> de borramento e preservação de bordas.</li>
    <li><strong>Histograma</strong> (estreitamento indica suavização).</li>
    <li><strong>PSNR/SSIM</strong> quando houver referência limpa (opcional).</li>
    <li><strong>Tempo de execução</strong> (cresce com o kernel; bilateral é o mais caro).</li>
  </ul>

  <h3>7) Boas práticas (OpenCV)</h3>
  <ul>
    <li>Para RGB, filtrar por canal ou converter para tons de cinza para análise.</li>
    <li>Escolher borda adequada (<code>BORDER_REFLECT</code> geralmente é bom padrão).</li>
    <li>Registrar parâmetros (tipo de filtro + kernel + σ) e salvar resultados por pasta (exigência do lab).</li>
  </ul>

  <h3>8) Expectativa ao variar o kernel</h3>
  <ul>
    <li><strong>3×3</strong>: leve redução de ruído; detalhes preservados.</li>
    <li><strong>5×5 / 7×7</strong>: bom compromisso entre limpeza e nitidez.</li>
    <li><strong>11×11</strong>: forte suavização; útil para ruído pesado, com perda de texturas.</li>
  </ul>


      <h3>3. Materiais e Métodos</h3>
      <p style="text-align:justify;">Os experimentos foram realizados utilizando a linguagem de programação C++ e a biblioteca OpenCV 4.x, em ambiente Linux (Ubuntu), com o compilador g++ e editor de código Visual Studio Code. Foram empregadas funções nativas do OpenCV para leitura, exibição e gravação de imagens, tais como `imread()`, `imshow()` e `imwrite()`, além das funções de filtragem `blur()`, `GaussianBlur()`, `medianBlur()` e `bilateralFilter()`. Para a parte prática envolvendo captura de vídeo, utilizou-se também a função `VideoCapture()` para acesso à webcam do computador. A imagem original utilizada como base foi a obtida no Laboratório 1, sendo processada e salva em formato .jpg em diferentes etapas.</p>

<p style="text-align:justify;">Inicialmente, desenvolveu-se um programa para aplicar quatro tipos de filtros espaciais — média, gaussiano, mediana e bilateral — sobre a imagem original, utilizando um kernel de tamanho 3×3. O procedimento foi repetido com os tamanhos de kernel 5×5, 7×7 e 11×11, de modo a avaliar a influência do aumento da vizinhança na suavização da imagem. Cada imagem resultante foi armazenada em pastas distintas, nomeadas de acordo com o filtro e o tamanho do kernel utilizado. Em seguida, os resultados foram analisados visualmente para verificar o efeito do aumento do kernel sobre a nitidez e a preservação das bordas, bem como a diferença de desempenho entre os tipos de filtros aplicados.</p>

<p style="text-align:justify;">Na segunda etapa, adicionou-se ruído do tipo sal-e-pimenta à imagem original e repetiram-se as filtragens. O objetivo foi comparar a eficiência de cada filtro na remoção desse tipo de ruído impulsivo, observando a suavização e a preservação das bordas. Posteriormente, foi desenvolvido um novo programa para processar imagens capturadas em tempo real pela webcam, permitindo aplicar dois dos filtros estudados — selecionados com base nos melhores e piores resultados das etapas anteriores. A exibição dos resultados foi feita em uma janela do OpenCV, com a possibilidade de salvar a imagem corrente ao pressionar a tecla “s”.</p>

<p style="text-align:justify;">Por fim, foi elaborado um programa adicional (desafio opcional) que permite ao usuário selecionar o tipo de filtro e o tamanho do kernel por meio do teclado, durante a execução da captura da webcam. As teclas [a], [g], [m] e [b] foram associadas aos filtros de média, gaussiano, mediana e bilateral, respectivamente, enquanto as teclas [3], [5], [7], [9] e [11] definiram os tamanhos de kernel correspondentes. O sistema exibia em tempo real o resultado da filtragem conforme as seleções do usuário. Todo o conjunto de imagens processadas foi salvo de forma organizada em subpastas nomeadas conforme o tipo de filtro e o kernel utilizados, permitindo uma análise sistemática dos efeitos de cada variação.
</p>

<p><strong>Diagrama de bloco</strong></p>
<figure style="max-width: 720px; margin: 16px auto; text-align: center;">
  <img src="imagens/diagrama_bloco2.png"
       alt="Diagrama de blocos do experimento de filtragem e suavização de imagens com decisões if, sim e não."
       loading="lazy"
       style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
  <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
    Diagrama de Blocos — Filtragem e Suavização de Imagens (OpenCV).
  </figcaption>
</figure>
  
      <h3>4. Resultados e Análises</h3>
      <p><strong>1)</strong> Nesta parte, usamos nossas próprias imagens obtidas no Lab1.</p> 

   <p style="text-align:justify;">Elaboramos um programa que realize as filtragens (com os filtros de média, gaussiano, mediana, e bilateral) na sua imagem com um kernel 3x3, e salvamos as imagens resultantes de cada filtragem, em formato .jpg.  Repita todas filtragens, elaborando novos programas com kernel 5x5, 7x7, e 11x11, salvando a imagem resultante de cada kernel. </p>

<!DOCTYPE html>
<html lang="pt-BR">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Filtragens — Resultados do Lab 1</title>
<style>
  body {
    background-color: #eebbbb;
    color: #222;
    font-family: Arial, Helvetica, sans-serif;
    margin: 0;
    padding: 16px;
  }
  p {
    text-align: justify;
    margin: 16px auto;
    max-width: 900px;
  }
  figure {
    max-width: 720px;
    margin: 20px auto;
    text-align: center;
  }
  figure img {
    width: 60%;
    height: auto;
    border-radius: 8px;
    border: 1px solid #ccc;
    display: block;
    margin: 0 auto;
  }
  figcaption {
    font: 14px/1.4 Arial, sans-serif;
    color: #000;
    margin-top: 8px;
  }
</style>
</head>
<body>

<p>Elaboramos um programa que realize as filtragens (com os filtros de média, gaussiano, mediana e bilateral) na sua imagem com um kernel 3x3, e salvamos as imagens resultantes de cada filtragem, em formato .jpg. Repetimos todas filtragens, elaborando novos programas com kernel 5x5, 7x7 e 11x11, salvando a imagem resultante de cada kernel.</p>

<figure>
  <img src="imagens/foto_grupo.jpg" alt="Foto em grupo original do Lab 1.">
  <figcaption>Foto em grupo original do Lab 1.</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_3x3.jpg" alt="Saída Bilateral 3x3.">
  <figcaption>Saída Bilateral 3×3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_5x5.jpg" alt="Saída Bilateral 5x5.">
  <figcaption>Saída Bilateral 5×5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_7x7.jpg" alt="Saída Bilateral 7x7.">
  <figcaption>Saída Bilateral 7×7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_11x11.jpg" alt="Saída Bilateral 11x11.">
  <figcaption>Saída Bilateral 11×11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_3x3.jpg" alt="Saída Gaussiana 3x3.">
  <figcaption>Saída Gaussiana 3×3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_5x5.jpg" alt="Saída Gaussiana 5x5.">
  <figcaption>Saída Gaussiana 5×5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_7x7.jpg" alt="Saída Gaussiana 7x7.">
  <figcaption>Saída Gaussiana 7×7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_gaussiana_11x11.jpg" alt="Saída Gaussiana 11x11.">
  <figcaption>Saída Gaussiana 11×11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_3x3.jpg" alt="Saída Média 3x3.">
  <figcaption>Saída Média 3×3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_5x5.jpg" alt="Saída Média 5x5.">
  <figcaption>Saída Média 5×5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_7x7.jpg" alt="Saída Média 7x7.">
  <figcaption>Saída Média 7×7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_media_11x11.jpg" alt="Saída Média 11x11.">
  <figcaption>Saída Média 11×11</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_3x3.jpg" alt="Saída Mediana 3x3.">
  <figcaption>Saída Mediana 3×3</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_5x5.jpg" alt="Saída Mediana 5x5.">
  <figcaption>Saída Mediana 5×5</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_7x7.jpg" alt="Saída Mediana 7x7.">
  <figcaption>Saída Mediana 7×7</figcaption>
</figure>

<figure>
  <img src="imagens/saida_mediana_11x11.jpg" alt="Saída Mediana 11x11.">
  <figcaption>Saída Mediana 11×11</figcaption>
</figure>

</body>
</html>

<p><strong>2)</strong> Repetimos o procedimento acima, adicionando o ruído tipo sal-e-pimenta na imagem original. </p>

<figure>
  <img src="imagens/original.png" 
  <figcaption>Imagem Sal e Pimenta original.</figcaption>
</figure>

<figure>
  <img src="imagens/saida_bilateral_3x3_noise.jpg" 
  <figcaption>Saída Bilateral 3x3.</figcaption>
</figure>
    
<figure>
  <img src="imagens/saida_bilateral_5x5_noise.jpg" 
  <figcaption>Saída Bilateral 5x5.</figcaption>
</figure>    
   
<figure>
  <img src="imagens/saida_bilateral_7x7_noise.jpg" 
  <figcaption>Saída Bilateral 7x7.</figcaption>
</figure>      

<figure>
  <img src="imagens/saida_bilateral_11x11_noise.jpg" 
  <figcaption>Saída Bilateral 11x11.</figcaption>
</figure>  

<figure>
  <img src="imagens/saida_gaussiana_3x3_noise.jpg" 
  <figcaption>Saída Gaussiana 3x3.</figcaption>
</figure>       
      
<figure>
  <img src="imagens/saida_gaussiana_5x5_noise.jpg" 
  <figcaption>Saída Gaussiana 5x5.</figcaption>
</figure>       
           
<figure>
  <img src="imagens/saida_gaussiana_7x7_noise.jpg" 
  <figcaption>Saída Gaussiana 7x7.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_gaussiana_11x11_noise.jpg" 
  <figcaption>Saída Gaussiana 11x11.</figcaption>
</figure>  
      
     
<figure>
  <img src="imagens/saida_media_3x3_noise.jpg" 
  <figcaption>Saída Média 3x3.</figcaption>
</figure>       
      
<figure>
  <img src="imagens/saida_media_5x5_noise.jpg" 
  <figcaption>Saída Média 5x5.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_media_7x7_noise.jpg" 
  <figcaption>Saída Média 7x7.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_media_11x11_noise.jpg" 
  <figcaption>Saída Média 11x11.</figcaption>
</figure>       
     
<figure>
  <img src="imagens/saida_mediana_3x3_noise.jpg" 
  <figcaption>Saída Mediana 3x3.</figcaption>
</figure>       

<figure>
  <img src="imagens/saida_mediana_5x5_noise.jpg" 
  <figcaption>Saída Mediana 5x5.</figcaption>
</figure>       
      
 <figure>
  <img src="imagens/saida_mediana_7x7_noise.jpg" 
  <figcaption>Saída Mediana 7x7.</figcaption>
  
  
  <figure>
  <img src="imagens/saida_mediana_11x11_noise.jpg" 
  <figcaption>Saída Mediana 11x11.</figcaption>
</figure>       
  
  <figure>
  <img src="imagens/imagem_ruido_salpimenta.jpg" 
  <figcaption>Saída com ruído.</figcaption>
</figure> 

  <p>Foram aplicados quatro tipos de filtragem (<strong>Gaussiano</strong>, <strong>Bilateral</strong>, <strong>Média</strong> e <strong>Mediana</strong>) sobre a mesma imagem original, utilizando tamanhos de kernel 3×3, 5×5, 7×7 e 11×11.  
  A seguir são descritos os efeitos observados visualmente em cada caso.</p>

  <h3>1. Filtro Gaussiano</h3>
  <div class="resumo">
    <p><strong>3×3:</strong> leve suavização; contornos permanecem bem definidos.<br>
    <strong>5×5:</strong> suavização perceptível, pequenas áreas de ruído desaparecem.<br>
    <strong>7×7:</strong> textura mais aveludada; perda de detalhes finos.<br>
    <strong>11×11:</strong> borramento forte e perda de nitidez geral.</p>
  </div>

  <h3>2. Filtro Bilateral</h3>
  <div class="resumo">
    <p><strong>3×3:</strong> suavização leve, sem perda de contornos.<br>
    <strong>5×5:</strong> aspecto limpo e suave, bordas ainda bem definidas.<br>
    <strong>7×7:</strong> aparência de “pele filtrada”, contraste natural.<br>
    <strong>11×11:</strong> efeito de pintura digital; superfícies uniformes e contornos bem preservados.</p>
  </div>

  <h3>3. Filtro de Média</h3>
  <div class="resumo">
    <p><strong>3×3:</strong> leve borramento; ruídos pequenos desaparecem.<br>
    <strong>5×5:</strong> perda de nitidez moderada; bordas suavizadas.<br>
    <strong>7×7:</strong> imagem mais desfocada, texturas diluídas.<br>
    <strong>11×11:</strong> borramento intenso; aparência de embaçado geral.</p>
  </div>

  <h3>4. Filtro Mediana</h3>
  <div class="resumo">
    <p><strong>3×3:</strong> remove ruídos leves sem alterar contornos.<br>
    <strong>5×5:</strong> ruídos isolados totalmente removidos.<br>
    <strong>7×7:</strong> superfícies mais uniformes; pequenas bordas suavizadas.<br>
    <strong>11×11:</strong> possível distorção em detalhes finos, porém excelente redução de ruído.</p>


    <p style="text-align:justify;"> Logo, o filtro bilateral apresenta o melhor equilíbrio entre suavização e preservação de detalhes, enquanto o filtro de média é o que mais degrada a nitidez. O filtro mediana se destaca em imagens com ruído tipo sal-e-pimenta, e o Gaussiano oferece suavização homogênea com bom controle visual.</p>

  </main>
  </body>
  </html>
    
<p> <strong> 3) </strong> Elabore um novo programa em que a imagem de entrada é da webcam, e que mostre o resultado da filtragem numa janela opencv, de forma contínua na tela do computar. Utilize a tecla [s] do teclado para permitir salvar a imagem sendo apresentada na tela, em formato .jpg. Neste caso, escolha apenas  dois tipos de filtragem e tamanho de kernel, baseado no melhor e no pior resultado obtidos na parte (2) acima.</p>    


 
<figure>
  <div class="video-container">
    <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
      <source src="imagens/Gravação de tela de 2025-10-08 10-38-21.webm" type="video/webm">
      Seu navegador não suporta a reprodução de vídeo.
    </video>
  </div>
  <figcaption>
    Gravação de tela mostrando a execução do programa de filtragem.<kbd>s</kbd>.
  </figcaption>
</figure>   
</body>
</html>
       
  <figure>
  <img src="imagens/frame_melhor_20251008_103834.jpg" 
  <figcaption>Tela com melhor frame.</figcaption>
</figure>     

  <figure>
  <img src="imagens/frame_pior_20251008_103512.jpg" 
  <figcaption>Tela com pior frame.</figcaption>
</figure>     

  <h3>Arquivo C++ </h3>


  <div class="frame-wrap">
  
  <iframe src="imagens/video_read_from_webcam.cpp"  style="width:80%;height:500px;border:2px solid #751a32;border-radius:10px;background:#fff;resize:both;overflow:auto;display:block;margin:20px auto;"></iframe>
</div>

<div class="download">
  <a href="imagens/video_read_from_webcam.cpp" download> Baixar código</a>
</div>

</body>
</html>
    
      <h3>5. Conclusões</h3>
      <p style="text-align:justify;">A partir dos experimentos, verificou-se que o tamanho do kernel é determinante no compromisso entre suavização e nitidez: kernels maiores (7×7 e 11×11) intensificam a redução de ruído, porém degradam detalhes e bordas; kernels menores (3×3 e 5×5) preservam melhor as texturas e contornos, com suavização mais discreta.</p>

<p style="text-align:justify;">Entre os métodos, o filtro de Média apresentou maior borramento global à medida que o kernel cresce, sendo útil para reduzir ruído de baixa intensidade, mas com perda acentuada de nitidez. O Gaussiano trouxe um equilíbrio melhor entre suavização e preservação de contornos, porém ainda com borramento cumulativo em kernels grandes. O Bilateral destacou-se por suavizar áreas homogêneas preservando bordas, oferecendo o melhor compromisso visual quando se deseja redução de ruído sem perda de arestas. Já o filtro Mediana foi o mais eficaz contra ruído sal-e-pimenta, removendo impulsos sem comprometer tanto as bordas — especialmente com kernels pequenos e médios.</p>

<p style="text-align:justify;">Ao repetir as filtragens com ruído sal-e-pimenta, confirmou-se a superioridade da Mediana na limpeza desse tipo de ruído, enquanto o Bilateral manteve contornos nítidos com aparência natural. Com base nisso, para a aplicação em tempo real (webcam) selecionamos dois filtros e um tamanho de kernel representativos do melhor e do pior desempenho observados na etapa (2): a Mediana (ou Bilateral) com kernel moderado como melhor cenário (boa remoção de ruído com preservação de bordas) e a Média com kernel grande como pior cenário (suavização excessiva e perda de detalhes). O programa em OpenCV funcionou conforme o esperado, exibindo os resultados continuamente e permitindo o salvamento com a tecla “s”, atendendo aos requisitos propostos.</p>

<p style="text-align:justify;">De forma geral, recomenda-se utilizar o filtro Mediana para tratar ruídos impulsivos e o Bilateral quando for essencial preservar as bordas da imagem. O Gaussiano apresenta um bom equilíbrio entre suavização e nitidez, enquanto o filtro de Média deve ser aplicado com cuidado, pois tende a provocar borramento excessivo. A seleção do tamanho do kernel deve levar em conta a intensidade do ruído presente e o grau de fidelidade visual desejado na imagem final.</p>
      </div>
      </div>
      </details>
      
      
        <!-- RELATÓRIO 3 -->
  <details>
    <summary>Relatório 3</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 22/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
      <p style="text-align:justify;">O presente experimento tem como objetivo estudar a representação e conversão entre diferentes espaços de cores utilizando a biblioteca OpenCV. A compreensão desses espaços é essencial no processamento de imagens e visão computacional, pois muitas operações de segmentação, filtragem e detecção de objetos dependem da forma como as cores são representadas numericamente.</p>

 <p style="text-align:justify;">Inicialmente, são exploradas as principais conversões entre espaços de cores — como RGB ↔ GRAY, RGB ↔ YCrCb, RGB ↔ HSV e Bayer → RGB — por meio das funções de conversão de cor disponibilizadas pelo OpenCV. Em seguida, é realizado um experimento prático de mudança de espaço RGB para HSV, com captura em tempo real via webcam, permitindo observar o comportamento de diferentes objetos coloridos e aplicar a função inRange() para segmentar faixas específicas de cores.</p>

 <p style="text-align:justify;">Na sequência, o experimento é expandido com a aplicação de um filtro Gaussiano, comparando o efeito do pré-processamento na detecção de cores. Posteriormente, o detector de bordas Canny é implementado sobre as imagens filtradas, possibilitando analisar o impacto dos parâmetros de detecção e a resposta do sistema diante de diferentes objetos e níveis de ruído.</p>

 <p style="text-align:justify;">Por fim, são realizadas modificações adicionais para salvar imagens e gravar vídeos do processo de rastreamento, além da elaboração de um programa capaz de identificar múltiplas cores simultaneamente (como vermelho, verde e azul), consolidando os conceitos de espaço de cores, segmentação e detecção de bordas.</p>

 <p style="text-align:justify;">Essas etapas visam fortalecer o entendimento prático sobre a manipulação de imagens coloridas, destacando a importância da escolha adequada do espaço de cor e dos métodos de filtragem para aplicações de visão computacional em tempo real.</p>
 

  <section id="fundamentos-basicos" style="font-family: Arial, Helvetica, sans-serif; line-height: 1.6; color: #222; max-width: 900px; margin: 24px auto; background: #fff; padding: 24px; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,.06); text-align: justify;">    
  <h3>2. Fundamentos Básicos</h3>
  <p>
    As imagens digitais são formadas por <strong>pixels</strong>, e cada pixel possui valores numéricos que representam a
    <strong>intensidade</strong> e a <strong>composição de cor</strong>. Esses valores são interpretados conforme o
    <strong>espaço de cores</strong> adotado — um modelo matemático que descreve como as cores são representadas e manipuladas
    em aplicações de processamento de imagem.
  </p>

  <h3>1) Espaço de Cores RGB</h3>
  <p>
    O espaço <strong>RGB (Red, Green, Blue)</strong> é o mais comum em câmeras e monitores, onde cada pixel é composto pela
    combinação dos três canais de cor. Embora intuitivo, o RGB não é ideal para todas as aplicações de visão computacional:
    pequenas variações de <em>iluminação</em> podem alterar significativamente os valores dos canais, dificultando a segmentação
    de cores específicas.
  </p>

  <h3>2) Espaço de Cores HSV</h3>
  <p>
    O modelo <strong>HSV (Hue, Saturation, Value)</strong> é frequentemente usado para <strong>segmentação e rastreamento de cores</strong>,
    pois separa as informações de <strong>matiz (Hue)</strong>, <strong>saturação (S)</strong> e <strong>valor/brilho (V)</strong>:
  </p>
  <ul style="margin-top: 8px;">
    <li><strong>Hue (Matiz):</strong> define a cor propriamente dita (vermelho, verde, azul etc.).</li>
    <li><strong>Saturation (Saturação):</strong> grau de pureza/intensidade da cor.</li>
    <li><strong>Value (Valor):</strong> brilho/luminosidade da cor.</li>
  </ul>
  <p>
    Essa separação torna o HSV mais <strong>robusto à variação de iluminação</strong>, facilitando a detecção de cores específicas por meio
    de limiares definidos com <code>inRange()</code> (OpenCV).
  </p>

  <h3>3) Conversões de Espaço de Cor</h3>
  <p>
    O OpenCV fornece funções diretas para conversão entre espaços de cor via <code>cvtColor()</code>. As conversões mais usadas
    neste laboratório incluem:
  </p>
  <ul style="margin-top: 8px;">
    <li><code>COLOR_BGR2GRAY</code> → converte imagem colorida para tons de cinza.</li>
    <li><code>COLOR_BGR2HSV</code> → converte para HSV (útil na segmentação por cor).</li>
    <li><code>COLOR_BGR2YCrCb</code> → separa luminância (Y) e crominância (Cr, Cb), útil em compressão de vídeo e detecção de pele.</li>
  </ul>

  <h3>4) Segmentação de Cores com <code>inRange()</code></h3>
  <p>
    A função <code>inRange()</code> realiza uma operação de <strong>limiarização</strong>, gerando uma máscara binária:
    pixels com HSV dentro da faixa definida (mín–máx) tornam-se <strong>brancos (255)</strong>; os demais, <strong>pretos (0)</strong>.
    Essa técnica é fundamental para <strong>isolar objetos coloridos</strong> e será usada nas etapas de detecção/rastreamento.
  </p>

  <h3>5) Filtragem Gaussiana</h3>
  <p>
    O <strong>filtro Gaussiano</strong> é um método de suavização que reduz ruídos de alta frequência, tornando bordas e áreas de cor
    mais homogêneas. Aplicá-lo <em>antes</em> da conversão para HSV ajuda a evitar que ruídos/reflexos causem erros na segmentação.
  </p>

  <h3>6) Detector de Bordas Canny</h3>
  <p>
    O <strong>detector de Canny</strong> identifica bordas analisando gradientes de intensidade. Ele combina suavização, cálculo de derivadas e
    limiares duplos, permitindo contornos <strong>nítidos e bem definidos</strong>. No contexto do laboratório, aplica-se o Canny sobre a imagem
    filtrada (Gaussiano) para visualizar contornos dos objetos segmentados.
  </p>

  <hr style="border:none; border-top:1px solid #ddd; margin: 20px 0;">

  <p style="font-size: 0.95rem;">
    <strong>Em síntese:</strong> a escolha adequada do espaço de cor (HSV, YCrCb etc.) e o uso combinado de
    <em>filtragem</em> (Gaussiano), <em>conversões</em> (cvtColor) e <em>detecção</em> (inRange, Canny) são decisivos para
    obter segmentações estáveis e contornos consistentes em aplicações de visão computacional em tempo real.
  </p>
</section>

      <section id="materiais-metodos" style="font-family: Arial, Helvetica, sans-serif; line-height: 1.6; color: #222; text-align: justify;">
  <h3>3. Materiais e Métodos</h3>
  <p>
    O experimento foi realizado em computador com Linux e webcam integrada, utilizando Python 3.10 e as bibliotecas OpenCV e NumPy.
    Investigou-se conversão de espaços de cor, filtragens e detecção de bordas em tempo real.
  </p>
  <p>
    O programa capturou frames da webcam (<code>cv.VideoCapture(0)</code>) e, opcionalmente, aplicou filtro Gaussiano
    (<code>cv.GaussianBlur</code>) para reduzir ruído antes da conversão de cor. Em seguida, a imagem foi convertida de
    RGB para HSV (<code>cv.cvtColor</code>), por ser mais estável à variação de iluminação e favorecer a segmentação.
  </p>
  <p>
    Para isolar regiões por cor, empregou-se <code>cv.inRange()</code> na faixa HSV definida; quando necessário, máscaras para múltiplas cores
    (p. ex., vermelho, verde e azul) foram combinadas por operações lógicas. Opcionalmente, aplicou-se o detector de bordas de Canny
    (<code>cv.Canny</code>) para realçar contornos.
  </p>
  <p>
    O sistema exibiu janelas com a imagem original, HSV, máscaras e bordas; imagens e vídeos puderam ser salvos via
    <code>cv.imwrite()</code> e <code>cv.VideoWriter()</code>. A execução foi encerrada ao pressionar a tecla <strong>q</strong>.
  </p>
</section>

      
<p><strong>Diagrama de bloco</strong></p>
<figure style="max-width: 720px; margin: 16px auto; text-align: center;">
  <img src="imagens/fluxograma_rel3.png"
       loading="lazy"
       style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
  <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
    Diagrama de blocos representando o fluxo de execução do programa de conversão de cores, filtragem e detecção de bordas no OpenCV
  </figcaption>
</figure>
      
    <h3>4. Resultados e Análises</h3>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Aplicação do Filtro Gaussiano</h4>
    <p style="text-align:justify;">
      <p style="text-align:justify;">
        Neste experimento, o programa foi modificado para aplicar um <b>Filtro Gaussiano</b> à imagem capturada pela webcam antes da conversão para o espaço de cores HSV.
        Além disso, foi criada uma janela adicional exibindo lado a lado a imagem não filtrada e a filtrada. 
        O filtro suavizou a imagem e reduziu ruídos visuais, resultando em uma detecção mais estável das cores.
        Foram utilizados três objetos coloridos: dourado, laranja e preto.
      </p>
      
      <p style="text-align:justify;">
        Durante a análise, observou-se que o dourado foi identificado com uma tonalidade amarelo-esverdeada, devido à alta sensibilidade do canal de saturação no espaço HSV.
        Sua faixa típica foi aproximadamente H = [20, 35], S = [100, 255] e V = [120, 255], correspondendo a um valor médio RGB ≈ (180, 200, 60).
        Já o objeto laranja apresentou um tom ligeiramente amarelado após o processamento, com faixa H = [10, 25], S = [150, 255] e V = [150, 255], e valor médio RGB ≈ (220, 170, 50).
        Por fim, o objeto preto refletiu pequenas variações de iluminação, sendo detectado como um verde-água escurecido em alguns quadros, com faixa H = [70, 90], S = [50, 150] e V = [30, 80], e valor médio RGB ≈ (40, 60, 50).
        Essas distorções de cor são comuns em ambientes com iluminação oscilante e demonstram a influência direta da luz e do ruído óptico sobre a estabilidade do modelo HSV.
      </p>
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de Tela 2025-10-18 193202.mp4" type="video/mp4">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela mostrando a aplicação do Filtro Gaussiano.
      </figcaption>
    </figure>   

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Detector de Bordas Canny</h4>
    <p style="text-align:justify;">
      O segundo experimento consistiu em aplicar o <b>Detector Canny</b> sobre a imagem filtrada. 
      O programa passou a exibir duas janelas: uma com a imagem filtrada e outra mostrando as bordas detectadas pelo método Canny. 
      Utilizando os mesmos objetos (dourado, laranja e preto), foi possível observar que apenas seus contornos eram destacados, permitindo identificar suas formas com nitidez. 
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de tela de 2025-10-15 11-12-56.webm" type="video/webm">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela exibindo o programa após aplicação do Detector Canny.
      </figcaption>
    </figure>   

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Salvar e gravar Imagens/Vídeos</h4>
    <p style="text-align:justify;">
      Nesta versão, o programa foi aprimorado para incluir as funções de salvar imagens (tecla [s]) e gravar vídeos (teclas [k] para iniciar e [h] para encerrar).
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de tela de 2025-10-15 11-28-11.webm" type="video/webm">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela mostrando o funcionamento das teclas [s], [k] e [h].
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/captura_lab3c_20251015_112839.jpg"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem salva a partir da tecla [s]
      </figcaption>
    </figure>

    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/video_lab3c_MJPG_20251015_112840.mp4" type="video/mp4">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela a partir das teclas [k] e [h].
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Extração de mais de um objeto colorido</h4>
    <p style="text-align:justify;">
      Neste exercício, o programa foi modificado para detectar e extrair mais de um objeto colorido simultaneamente.
      Foram utilizados objetos com diferentes cores, e o sistema foi capaz de identificar e rastrear cada um de forma independente.
      Durante o experimento, observou-se que os itens coloridos traçavam um caminho visível na tela, com cada cor destacada individualmente.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de tela de 2025-10-15 11-33-04.webm" type="video/webm">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela demonstrando o rastreamento de objetos coloridos.
      </figcaption>
    </figure>   
    
    <h3>5. Conclusões</h3>
     <p style="text-align:justify;">
        Este relatório permitiu compreender de forma prática a influência das etapas de filtragem, detecção e rastreamento na análise de imagens em tempo real. 
        Inicialmente, a aplicação do filtro Gaussiano demonstrou-se fundamental para reduzir ruídos e suavizar as transições de cor, possibilitando uma detecção mais estável no espaço HSV. 
        Observou-se que o dourado assumiu tonalidades amarelo-esverdeadas, o laranja tornou-se mais amarelado e o preto apresentou coloração de verde-água, o que evidenciou o impacto da iluminação e da suavização no comportamento das cores. 
        As faixas HSV e os valores RGB obtidos mostraram-se coerentes com as variações ópticas esperadas em ambiente real.
      </p>
      
      <p style="text-align:justify;">
        Na sequência, a implementação do detector de bordas Canny possibilitou realçar os contornos dos objetos identificados, destacando formas e limites com precisão. 
        Posteriormente, a adição das funções de salvamento de imagens e gravação de vídeos ampliou as possibilidades de análise e documentação dos experimentos, tornando o sistema mais interativo e funcional. 
        Essas ferramentas permitiram registrar o comportamento dinâmico da detecção e avaliar o desempenho do programa ao longo do tempo.
      </p>
      
      <p style="text-align:justify;">
        Por fim, o exercício de extração de múltiplos objetos coloridos consolidou o aprendizado das etapas anteriores, possibilitando rastrear e distinguir simultaneamente diferentes cores em movimento. 
        O sistema foi capaz de acompanhar o deslocamento de cada objeto e representar visualmente o caminho percorrido, reforçando o entendimento da segmentação por faixas HSV.
        De forma geral, o conjunto dos experimentos evidenciou a evolução progressiva do sistema, desde o tratamento inicial da imagem até a análise multiobjeto. 
      </p>

    </div>
    </div>
  </details>
    
      <!-- RELATÓRIO 4 -->
  <details>
    <summary>Relatório 4</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 27/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
        <p style="text-align:justify;">Neste laboratório, estudamos as técnicas de processamento de imagens baseadas em histogramas e 
    operações de limiarização (binarização), utilizando a biblioteca OpenCV em linguagem C++. 
    O objetivo principal é compreender como a distribuição dos níveis de intensidade de uma imagem influencia sua qualidade visual 
    e como métodos de equalização e limiarização podem ser empregados para realçar contrastes e extrair informações relevantes.</p>
      

    <p style="text-align:justify;">O histograma de uma imagem representa a distribuição estatística dos tons de cinza (ou de cores) presentes, 
    sendo uma ferramenta fundamental na análise e no pré-processamento de imagens digitais. 
    A equalização de histograma busca redistribuir esses níveis de intensidade de forma mais uniforme, 
    aumentando o contraste global e evidenciando detalhes antes pouco visíveis. 
    Essa técnica é amplamente utilizada em aplicações de visão computacional e reconhecimento de padrões, 
    especialmente em condições de iluminação não uniforme.</p>


    <p style="text-align:justify;">Complementarmente, a limiarização (thresholding) é uma operação que converte imagens em tons de cinza em imagens binárias, 
    segmentando regiões de interesse com base em um valor de limiar. 
    Essa técnica é essencial em tarefas de detecção de objetos, reconhecimento facial e inspeção industrial. 
    Quando aplicada após a equalização de histograma, a limiarização tende a produzir resultados mais robustos, 
    pois os níveis de intensidade estão melhor distribuídos.</p>

  <p>Durante o experimento, foram desenvolvidos programas progressivos conforme o roteiro proposto:</p>
  <ul>
    <li>
      <strong>Equalização de histograma</strong> em imagens individuais convertidas para tons de cinza, 
      com salvamento dos histogramas antes e depois da equalização.
    </li>
    <li>
      <strong>Equalização em tempo real</strong> utilizando a webcam, exibindo simultaneamente a imagem em cinza e a equalizada.
    </li>
    <li>
      <strong>Limiarização e binarização</strong> aplicadas após a equalização, para avaliar o impacto dessa etapa 
      na qualidade da segmentação.
    </li>
    <li>
      <strong>Equalização em imagens coloridas</strong>, aplicando o processo separadamente em cada canal (B, G, R) 
      e analisando o efeito visual e estatístico sobre o histograma resultante.
    </li>
  </ul>

  <p>
    Esses experimentos possibilitam compreender o papel da equalização e da limiarização no realce e na segmentação de imagens, 
    fornecendo uma base sólida para aplicações mais complexas de processamento e análise visual.
  </p>
    
      <h3>2. Fundamentos Básicos</h3>
      <p style="text-align:justify;"> Os histogramas são representações gráficas da distribuição de intensidade dos pixels de uma imagem.
    No caso de imagens em tons de cinza, o eixo horizontal representa os níveis de intensidade (de 0 a 255),
    enquanto o eixo vertical indica a frequência com que cada nível ocorre. 
    Assim, o histograma permite identificar se uma imagem está muito escura (valores concentrados à esquerda),
    muito clara (valores concentrados à direita) ou com baixo contraste (valores concentrados em uma faixa estreita).</p>
    
   <p style="text-align:justify;">  A equalização de histograma é uma técnica de realce de contraste que visa redistribuir de forma mais uniforme 
    os níveis de intensidade dos pixels, expandindo a faixa tonal da imagem. 
    Isso é feito ajustando a <em>função de distribuição acumulada</em> (CDF) do histograma, 
    de modo que os valores de intensidade sejam mapeados para um intervalo mais amplo, 
    realçando detalhes em regiões claras e escuras. 
    Em OpenCV, essa operação pode ser realizada com a função <code>equalizeHist()</code>. </p>

     <p style="text-align:justify;">Em imagens coloridas, a equalização pode ser aplicada separadamente a cada canal de cor (B, G, R), 
    ou sobre o canal de luminância em um espaço de cor como YCrCb ou HSV. 
    No entanto, equalizar cada canal RGB de forma independente pode causar distorções cromáticas perceptíveis, 
    sendo mais indicado equalizar apenas o componente de brilho. </p>

     <p style="text-align:justify;">A limiarização</strong> (ou <em>thresholding</em>) é um método de <em>segmentação de imagem</em> que converte pixels em tons de cinza 
    em apenas dois níveis: preto (0) e branco (255), com base em um valor de limiar (<em>threshold</em>) definido pelo usuário ou calculado automaticamente. 
    Em OpenCV, a função <code>threshold()</code> oferece diversos modos, como: </p>

  <ul>
    <li><strong>THRESH_BINARY:</strong> pixels acima do limiar tornam-se brancos; abaixo, pretos.</li>
    <li><strong>THRESH_BINARY_INV:</strong> inversão da binarização tradicional.</li>
    <li><strong>THRESH_TRUNC:</strong> valores acima do limiar são truncados para o valor do limiar.</li>
    <li><strong>THRESH_TOZERO:</strong> valores abaixo do limiar tornam-se zero.</li>
    <li><strong>THRESH_OTSU:</strong> método automático que determina o limiar ótimo com base na minimização da variância intra-classe.</li>
  </ul>

  <p>
    Quando a limiarização é precedida pela equalização de histograma, os níveis de intensidade ficam mais bem distribuídos, 
    facilitando a separação entre regiões claras e escuras e resultando em uma binarização mais eficiente e menos sensível 
    às variações de iluminação.
  </p>

  <p>
    Em resumo, a combinação das técnicas de equalização e limiarização fornece uma poderosa abordagem 
    para o pré-processamento de imagens, melhorando a visibilidade de detalhes e preparando os dados para etapas 
    subsequentes de análise, como detecção de bordas, reconhecimento de padrões e segmentação de objetos.
  </p>  
    
      <h3>3. Materiais e Métodos</h3>
     <p style="text-align:justify;">
Para a realização do laboratório foram utilizados recursos de hardware e software necessários para o processamento e análise de imagens em tempo real. 
O ambiente de desenvolvimento adotado foi o Visual Studio Code, com a linguagem C++ e a biblioteca OpenCV (versão 4.x)</strong>, 
amplamente empregada em aplicações de visão computacional. Em alguns experimentos, também foi utilizada a webcam do computador para captura de imagens em tempo real.
</p>

<p style="text-align:justify;">
As principais funções do OpenCV utilizadas foram:
</p>

<ul style="text-align:justify;">
  <li><code>imread()</code> e <code>imshow()</code> para leitura e exibição das imagens;</li>
  <li><code>cvtColor()</code> para conversão de imagem colorida para tons de cinza;</li>
  <li><code>calcHist()</code> para cálculo do histograma;</li>
  <li><code>equalizeHist()</code> para equalização de histograma em imagens em tons de cinza;</li>
  <li><code>split()</code> e <code>merge()</code> para separar e recombinar os canais de cor (B, G, R);</li>
  <li><code>threshold()</code> para aplicar a limiarização;</li>
  <li><code>imwrite()</code> para salvar as imagens processadas;</li>
  <li><code>waitKey()</code> para controle de execução e captura de tecla.</li>
</ul>

<p style="text-align:justify;">
As imagens utilizadas foram fotos individuais dos integrantes do grupo e objetos coloridos, capturadas tanto a partir de arquivos quanto diretamente pela câmera. 
O processamento foi realizado em quatro etapas principais, conforme o roteiro experimental:
</p>

<ol style="text-align:justify;">
  <li><strong>Equalização em tons de cinza:</strong> conversão da imagem original para escala de cinza, cálculo do histograma e aplicação da equalização para realce de contraste.</li>
  <li><strong>Equalização em tempo real:</strong> aplicação da equalização de histograma sobre o fluxo contínuo da webcam, exibindo simultaneamente a imagem em cinza e a equalizada.</li>
  <li><strong>Limiarização (binarização):</strong> inserção de um valor de limiar fixo ou calculado automaticamente (método de Otsu) após a equalização, gerando uma imagem binária para segmentação.</li>
  <li><strong>Equalização em imagens coloridas:</strong> equalização aplicada separadamente nos três canais de cor (B, G, R), com posterior recombinação e comparação visual com a imagem original.</li>
</ol>

<p style="text-align:justify;">
Durante todo o processo, foram geradas e salvas as imagens intermediárias (original, equalizada, binarizada) e os gráficos de histograma correspondentes. 
Os resultados obtidos em cada etapa permitiram comparar o efeito da equalização e da limiarização, tanto em imagens estáticas quanto dinâmicas, 
avaliando a melhoria de contraste, a nitidez dos contornos e a eficiência da segmentação.
</p>

<h3>Diagrama de Blocos</h3>
<figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/lab4.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos do processo de equalização e limiarização de imagens no OpenCV.
  </figcaption>
</figure>

           
      <h3>4. Resultados e Análises</h3>
      <p style="text-align:justify;"><strong>(1) </strong> Desenvolva um programa para fazer a leitura de sua imagem, convertendo para tons de cinza, calcular o histograma, e realizar a equalização do histograma. Ao toque de uma tecla, o programa deve salvar a imagem de entrada em cinza e a imagem equalizada, através de comando OpenCV. Salve também as imagens dos gráficos de histograma antes e depois da equalização.Realize este experimento com as imagens individuais, separadamente, de cada integrante do grupo.
</p>
      
      <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Fernanda.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Fernanda.
  </figcaption>
</figure>
      
            <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Gabriel.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

      <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/1)Ingrid.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Ingrid.
  </figcaption>
</figure>

      <p style="text-align:justify;"><strong>(2)</strong> Elabore outro programa modificando o código do item (1), agora fazendo a leitura de imagem da webcam. 
Neste caso, o programa deve adicionalmente mostrar uma janela ao vivo com a imagem cinza e o resultado da imagem equalizada.
Realize este experimento com cada integrante do grupo e com um objeto colorido.</p>
      
         <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-27-22.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>   
      
        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-10.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Fernanda.
  </figcaption>
</figure>

        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-21.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Ingrid.
  </figcaption>
</figure>

        <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-28-36.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Objeto colorido.
  </figcaption>
</figure>

      <p style="text-align:justify;"><strong>(3)</strong> Estudo da binarização. Desenvolva outro programa modificando o item (2), incluindo a limiarização da imagem do tutorial acima, para obtenção de uma imagem binária. Adicione a Equalização de Histograma antes da limiarização.  Realize este experimento com pessoas e com objetos. Compare e analise o efeito sem e com a equalização de histograma no resultado da imagem binária.</p>
      
          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-35-27.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-35-37.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel com objeto colorido.
  </figcaption>
</figure>
      
      <p style="text-align:justify;"><strong>(4)</strong> Estudo da equalização nas cores: Elabore outro programa modificando o item (2), porém sem converter para cinza, mas agora realizando a equalização nas três cores separadamente, e juntar os canais equalizados na imagem colorida de saída. Realize este experimento com pessoas e com objetos coloridos. Compare e analise o efeito sem e com a equalização de histograma no resultado da imagem colorida gerada e no histograma.</p>
      
               <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-39-03.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel.
  </figcaption>
</figure>

          <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Captura de tela de 2025-10-22 10-39-14.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Gabriel com objeto colorido.
  </figcaption>
</figure>
      
      <h3>5. Conclusões</h3>
      <p style="text-align:justify;">O Laboratório 4 permitiu compreender, de forma prática e experimental, como o histograma é uma ferramenta fundamental para a análise e o processamento de imagens digitais. Por meio dos exercícios realizados, foi possível observar como a equalização de histograma melhora o contraste de imagens com faixas de intensidade concentradas, tornando detalhes antes imperceptíveis mais evidentes.</p>
         <p style="text-align:justify;">Na primeira etapa, ao converter imagens para tons de cinza e aplicar a equalização, notou-se a redistribuição dos níveis de brilho, o que resultou em uma aparência visual mais equilibrada. Já na segunda etapa, a execução em tempo real com a webcam possibilitou visualizar instantaneamente o efeito da equalização em diferentes condições de iluminação e em diferentes indivíduos e objetos, demonstrando a aplicabilidade do método em sistemas de visão computacional dinâmicos.</p>
         <p style="text-align:justify;">Com o estudo da binarização, foi possível compreender a importância da limiarização e o impacto da equalização prévia do histograma sobre o resultado da segmentação. As imagens binárias com equalização apresentaram bordas mais nítidas e separação mais eficiente entre fundo e objeto, especialmente em cenas com pouca iluminação ou contraste reduzido.</p>
         <p style="text-align:justify;">Por fim, a análise da equalização em imagens coloridas, com a aplicação individual da técnica em cada canal RGB, evidenciou o aumento de contraste, mas também possíveis distorções nas tonalidades originais. Isso reforçou a importância de aplicar a equalização com critério, considerando o equilíbrio entre realce visual e preservação da fidelidade das cores.</p>
          <p style="text-align:justify;">Em síntese, o laboratório proporcionou uma visão completa do uso dos histogramas e da limiarização em processamento digital de imagens, demonstrando que essas técnicas são fundamentais para etapas posteriores de detecção, segmentação e reconhecimento de padrões em sistemas de visão computacional.</p>
      
      
      
    </div>
    </div>
  </details>

      <!-- RELATÓRIO 5 -->
  <details>
    <summary>Relatório 5</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 29/10/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
      <p style="text-align:justify;">O presente laboratório tem como objetivo estudar e implementar técnicas de subtração de fundo e detecção de movimento utilizando a biblioteca OpenCV em C++. A subtração de fundo é uma das etapas fundamentais em sistemas de visão computacional, permitindo identificar objetos em movimento dentro de uma cena estática. Essa técnica baseia-se na comparação entre um modelo de fundo previamente aprendido e os quadros atuais do vídeo, destacando as regiões que sofrem alterações — geralmente correspondentes a pessoas, veículos ou objetos em deslocamento.</p>

<p style="text-align:justify;">Inicialmente, é proposto o estudo do tutorial oficial do OpenCV “How to Use Background Subtraction Methods”, bem como a execução do código de exemplo bg_sub.cpp com o vídeo vtest.avi, utilizando os modos MOG2 e KNN. A partir dessa base, foram desenvolvidos dois experimentos práticos: no primeiro, um programa realiza a subtração de fundo sobre vídeos gravados em aula, contendo movimentos lentos e rápidos, gerando uma saída que exibe apenas os elementos móveis e grava o resultado em arquivo. No segundo experimento, o código é adaptado para leitura direta da webcam, exibindo simultaneamente a imagem original e a imagem processada em tempo real, permitindo observar o desempenho da técnica com pessoas e objetos coloridos.</p>

<p style="text-align:justify;">Por fim, o laboratório propõe uma breve pesquisa sobre aplicações práticas dessas técnicas em áreas como vigilância, transporte, automação e interação humano-computador, além de uma leitura complementar sobre rastreamento de objetos baseado em aprendizado profundo (GOTURN). Com isso, busca-se compreender tanto os fundamentos teóricos quanto as aplicações reais da subtração de fundo e da detecção de movimento em sistemas inteligentes de visão computacional.</p>

      <h3>2. Fundamentos Básicos</h3>
      
       <p style="text-align:justify;"> A subtração de fundo é uma técnica essencial em visão computacional para detectar
      regiões em movimento em sequências de vídeo. O princípio é comparar cada novo
      quadro (frame) com um modelo de fundo (a cena estática). Diferenças
      significativas entre o frame atual e o modelo são classificadas como primeiro plano
      (objetos em movimento), gerando uma máscara binária que separa
      foreground e background. </p>
     <p style="text-align:justify;">O desempenho pode ser afetado por ruído, variações de iluminação,sombras e
      fundo dinâmico (por exemplo, árvores ao vento). Por isso, adotam-se modelos que
     se adaptam no tempo, atualizando continuamente o fundo.</p>

    <h3>Modelos no OpenCV</h3>
    <ul>
      <li>
        <strong>MOG2 (Mixture of Gaussians 2)</strong>: modela cada pixel como mistura de gaussianas.
        É robusto a mudanças suaves de iluminação e pode <strong>rotular sombras</strong> na máscara (valores intermediários).
      </li>
      <li>
        <strong>KNN (K-Nearest Neighbors)</strong>: usa uma janela temporal de amostras recentes para
        classificar o pixel como fundo ou primeiro plano. Adapta-se rapidamente a mudanças bruscas
        e funciona bem com <strong>fundos dinâmicos</strong>.
      </li>
    </ul>

    <h3>Pós-processamento</h3>
     <p style="text-align:justify;"> Após obter a máscara, aplicam-se operações morfológicas(abertura/fechamento) e
     limiarização para remover ruído e refinar contornos. Em seguida, a máscara pode ser
      sobreposta ao vídeo para visualizar apenas as áreas móveis ou alimentar etapas como
      rastreamento, contagem de objetos/pessoas e análise de padrões.
    </p>

     <p style="text-align:justify;">Essas técnicas são base de aplicações em <strong>vigilância inteligente</strong>, <strong>controle de tráfego</strong>,
      <strong>automação industrial</strong> e <strong>interação humano-computador</strong>, possibilitando monitoramento
      e decisões em tempo real. </p>
      
      <h3>3. Materiais e Métodos</h3>
      <p style="text-align:justify;">Para o desenvolvimento deste laboratório, foi utilizada a linguagem C++ com a biblioteca OpenCV 4.x, além de vídeos de teste e imagens capturadas por webcam. O estudo teórico foi baseado no tutorial oficial do OpenCV “How to Use Background Subtraction Methods”, que apresenta o funcionamento dos algoritmos MOG2 e KNN, amplamente usados em aplicações de detecção de movimento.</p>
      

 <p style="text-align:justify;">Inicialmente, foi criada a pasta lab5, contendo o código de exemplo bg_sub.cpp e o vídeo de teste vtest.avi, disponibilizados no repositório oficial do OpenCV. O programa foi compilado e executado nos modos MOG2 e KNN, permitindo observar as diferenças entre os dois métodos de subtração de fundo e compreender o comportamento de cada um frente a diferentes tipos de movimento e iluminação.</p>

<p style="text-align:justify;">Com base nesse estudo, foi desenvolvido um programa próprio em C++ capaz de ler vídeos gravados em aula, aplicar a subtração de fundo e exibir apenas os elementos em movimento. O sistema também grava automaticamente o vídeo processado, destacando o primeiro plano. Em seguida, o código foi modificado para funcionar com a webcam, exibindo em tempo real duas janelas: a imagem original e a imagem resultante do processamento. Durante os testes, foram realizados experimentos com pessoas e com objetos coloridos, observando o comportamento do algoritmo em movimentos lentos e rápidos.</p>

<p style="text-align:justify;">Durante o processamento, foram aplicadas técnicas de limiarização e operações morfológicas (abertura e fechamento) para reduzir ruídos e melhorar a qualidade da máscara de movimento. Os parâmetros learningRate, varThreshold e history foram ajustados empiricamente para equilibrar a sensibilidade da detecção e a estabilidade do modelo de fundo.</p>

<p style="text-align:justify;">Por fim, foram comparados os resultados obtidos com os métodos MOG2 e KNN, analisando sua eficiência, estabilidade e adaptação ao fundo. As observações foram registradas em vídeos e imagens, permitindo avaliar o desempenho de cada abordagem em diferentes condições de movimento e iluminação.</p>

   <h3>Diagrama de blocos</h3>
   <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/diagrama4.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de subtração de fundo e detecção de movimento no OpenCV.
  </figcaption>
</figure>
      
      
      <h3>4. Resultados e Análises</h3>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Vídeos com movimento lento e com movimento rápido</h4>
    <p style="text-align:justify;">
      Durante o experimento, observou-se que o algoritmo apresentou boa capacidade de detecção dos objetos em movimento, mesmo em cenários com variação de iluminação.
      O modelo de fundo foi atualizado gradualmente, permitindo distinguir de forma consistente as regiões em movimento. No vídeo com movimento lento, a detecção mostrou-se estável e precisa, 
      sendo possível visualizar claramente o rastro contínuo do objeto, que indicava sua trajetória ao longo do tempo. Já no vídeo com movimento rápido, embora a detecção tenha apresentado 
      menor nível de detalhamento, devido à velocidade, ainda foi possível identificar de forma nítida a presença e o deslocamento do objeto, comprovando a eficiência do método utilizado.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_subtracao.mp4" type="video/mp4">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela mostrando o resultado com vídeo lento.
      </figcaption>
    </figure>
      
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_subtracao (1).mp4" type="video/mp4">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela mostrando o resultado com vídeo rápido.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de imagem da webcam</h4>
    <p style="text-align:justify;">
      No segundo experimento, o programa apresentou um comportamento semelhante ao anterior.
      A principal dificuldade observada foi a variação de iluminação do ambiente, que interferiu 
      na atualização do modelo de fundo, causando pequenas áreas borradas, indicando falsa detecção de movimento em certos momentos.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/saida_lab5_2.mp4" type="video/mp4">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela exibindo movimentos dos integrantes do grupo e do objeto colorido.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Aplicação prática com subtração de fundo:</h4>
    <p style="text-align:justify;">
      No contexto do projeto RoadWatch, essas técnicas são fundamentais para o reconhecimento de ações do motorista.
      A subtração de fundo permite isolar os elementos em movimento dentro do veículo, como as mãos e o celular, enquanto 
      a detecção de movimento possibilita identificar quando o motorista realiza ações suspeitas, como pegar o telefone durante a condução. 
      Um experimento prático consiste em capturar, por meio da webcam, imagens do motorista durante a simulação de direção. 
      O sistema realiza a subtração de fundo para remover o cenário fixo e destaca apenas os movimentos das mãos e do celular.
      Quando o algoritmo identifica um movimento incompatível com a direção segura, o sistema emite um alerta visual ou sonoro indicando o uso indevido do aparelho.
      Esse tipo de aplicação demonstra como a subtração de fundo e a detecção de movimento podem ser integradas a sistemas de segurança veicular, 
      contribuindo para a redução de acidentes causados por distrações e promovendo uma condução mais segura e responsável.
    </p>
    </section>  
    
    <h3>5. Conclusões</h3>
     <p style="text-align:justify;">
        Este relatório sobre Subtração de Fundo e Detecção de Movimento permitiu compreender na prática o funcionamento de algoritmos fundamentais 
        do processamento de vídeo. Nos vídeos com movimento lento, a detecção foi mais detalhada, permitindo observar claramente o trajeto do objeto. 
        Já nos vídeos com movimento rápido, embora o detalhamento tenha diminuído, o sistema ainda conseguiu reconhecer o deslocamento, demonstrando sua eficiência 
        mesmo em situações com variação de iluminação e velocidade. Nos testes com a webcam, a técnica se mostrou sensível às variações de luz, mas eficaz na detecção 
        dos elementos não estáticos, como as mãos e objetos coloridos em movimento.
      </p>
      
      <p style="text-align:justify;">
        Esses resultados evidenciam a importância da subtração de fundo como etapa inicial de análise de comportamento e reconhecimento de ações.
        No contexto do projeto RoadWatch, essa técnica desempenha um papel fundamental na identificação do uso de celular durante a direção, uma vez 
        que permite isolar e acompanhar o movimento das mãos e do dispositivo, destacando ações que desviam a atenção do motorista.
        Assim, conclui-se que as técnicas de subtração de fundo e detecção de movimento são ferramentas essenciais, com aplicações que vão desde sistemas 
        de monitoramento e segurança até soluções inteligentes de transporte.
      </p>
    </div>
    </div>
  </details>
  
        <!-- RELATÓRIO 6 -->
  <details>
    <summary>Relatório 6</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 05/11/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
      <p style="text-align:justify;"> Nesta aula, foi estudado o conceito de features (características locais) em imagens, com foco na detecção de pontos de interesse utilizando diferentes métodos implementados na biblioteca OpenCV. Em visão computacional, features são elementos que se destacam em uma imagem, como cantos, bordas ou regiões com variação significativa de intensidade, e que podem ser reconhecidos novamente em outras imagens ou quadros de um vídeo. Esses pontos são fundamentais porque permanecem estáveis mesmo quando há mudanças de iluminação, rotação, escala ou perspectiva, tornando-se essenciais em aplicações como rastreamento de movimento, reconstrução tridimensional, calibração de câmeras e reconhecimento de padrões.</p>

<p style="text-align:justify;"> Durante o estudo teórico, foi analisado o material “Understanding Features”, que apresenta a importância desses pontos de interesse e como eles são usados para descrever e compreender o conteúdo de uma cena. No laboratório, foram aplicados os métodos clássicos de detecção, como o Shi–Tomasi Corner Detector e o Good Features to Track, além de outros detectores de features disponíveis no OpenCV. Inicialmente, foi desenvolvido um programa em C++ para leitura de imagens estáticas — tanto das capturas feitas anteriormente quanto das imagens utilizadas no projeto de vídeo — realizando a detecção e a marcação das features identificadas, com posterior salvamento dos resultados em arquivo.</p>

<p style="text-align:justify;"> Em seguida, o experimento foi ampliado para o ambiente dinâmico, com a implementação de um segundo programa em C++ capaz de capturar imagens em tempo real pela webcam. Esse programa exibe, simultaneamente, a imagem original e a imagem processada com as features destacadas, permitindo observar o funcionamento do algoritmo em diferentes condições. Foram realizadas filmagens tanto com os integrantes do grupo quanto com um tabuleiro de xadrez preto e branco, pois esse padrão geométrico contém muitos cantos bem definidos e é amplamente utilizado para testes e calibração de câmeras.</p>

<p style="text-align:justify;"> Por fim, foram discutidas aplicações práticas das técnicas de detecção de features e cantos, que incluem desde o rastreamento de objetos e reconhecimento facial até a reconstrução 3D e o mapeamento de ambientes para robôs autônomos. Assim, a Aula 6 proporcionou uma compreensão sólida sobre como detectar, visualizar e aplicar features em imagens e vídeos, consolidando a base teórica e prática para o desenvolvimento de sistemas de visão computacional.</p>

      
      <h3>2. Fundamentos Básicos</h3>
      <p style="text-align:justify;">As features, ou características locais, representam informações relevantes extraídas de uma imagem que permitem identificar e comparar regiões específicas entre diferentes imagens ou quadros de um vídeo. Em visão computacional, elas correspondem a pontos de interesse, bordas, cantos ou regiões únicas que se destacam em relação ao restante da cena, funcionando como elementos fundamentais para reconhecer, rastrear e descrever objetos. Essas características são especialmente úteis porque tendem a se manter estáveis mesmo quando há variações de iluminação, rotação, escala ou perspectiva, tornando-se essenciais em aplicações como reconhecimento de padrões, alinhamento de imagens, reconstrução tridimensional, rastreamento de movimento e detecção de objetos.</p>

<p style="text-align:justify;">Entre os principais tipos de features destacam-se os cantos (corners), as bordas (edges) e os blobs. Os cantos são regiões onde há grande variação de intensidade em duas direções, como nas interseções de linhas ou padrões geométricos — por exemplo, em um tabuleiro de xadrez. As bordas correspondem aos limites entre regiões com intensidades diferentes, sendo úteis na segmentação e no contorno de objetos. Já os blobs representam regiões que se sobressaem pela textura ou forma, sendo detectados por métodos como DoG (Difference of Gaussians) e LoG (Laplacian of Gaussian).</p>

<p style="text-align:justify;">Entre os algoritmos clássicos de detecção de cantos, destacam-se o Harris Corner Detector e o Shi–Tomasi (Good Features to Track). O método de Harris identifica pontos de variação significativa de intensidade em múltiplas direções, marcando regiões com alto contraste. Já o Shi–Tomasi é uma versão aprimorada do método de Harris, que seleciona apenas os cantos mais confiáveis, ideais para rastreamento entre quadros consecutivos de vídeo. Esses detectores são amplamente empregados em tarefas de rastreamento de movimento, calibração de câmeras e reconstrução de cenas.</p>

<p style="text-align:justify;">Após detectar os pontos de interesse, é possível extrair descritores, que são vetores numéricos capazes de representar matematicamente a vizinhança de cada feature. Esses descritores permitem comparar e identificar correspondências entre features de imagens distintas, sendo a base para técnicas como o emparelhamento de pontos e a fusão de imagens.</p>

<p style="text-align:justify;">A detecção de features tem inúmeras aplicações práticas, incluindo o rastreamento de objetos em vídeos, a montagem de panoramas por meio de image stitching, o reconhecimento de formas em sistemas industriais, a calibração e orientação de câmeras em robótica e a localização de padrões para realidade aumentada. Em síntese, o estudo de features possibilita converter informações visuais em dados matemáticos interpretáveis pelo computador, sendo um dos pilares da visão computacional moderna e permitindo que sistemas artificiais possam “entender” e analisar o conteúdo das imagens com precisão e consistência.</p>

      
      <h3>3. Materiais e Métodos</h3>
      
      <p style="text-align:justify;">Os experimentos da Aula 6 foram realizados utilizando a biblioteca OpenCV em linguagem C++, aplicada tanto para processamento de imagens estáticas quanto para captura e análise em tempo real por meio da webcam. Inicialmente, foram selecionadas as imagens obtidas em aulas anteriores, bem como algumas imagens utilizadas no projeto de vídeo do grupo, servindo como base para a implementação dos algoritmos de detecção de features. Todos os testes foram desenvolvidos em ambiente de programação configurado com o compilador g++ e suporte às bibliotecas do OpenCV, executados em sistemas compatíveis com Linux ou Windows.</p>

<p style="text-align:justify;">O primeiro experimento consistiu na criação de um programa capaz de realizar a leitura de imagens previamente armazenadas e aplicar os métodos de detecção de *features*, com destaque para o Shi–Tomasi Corner Detector e o algoritmo Good Features to Track, conforme apresentado na documentação oficial do OpenCV. O programa processou cada imagem identificando os pontos de interesse (cantos ou features) e marcou-os graficamente sobre a imagem original, salvando os resultados em arquivos de saída no formato de imagem. Essa etapa teve como objetivo visualizar o comportamento dos detectores em diferentes tipos de cena, com variações de textura, contraste e iluminação.</p>

<p style="text-align:justify;">No segundo experimento, o código foi modificado para realizar a captura de imagens em tempo real utilizando a webcam do computador. O programa foi estruturado para exibir simultaneamente duas janelas: uma contendo a imagem original capturada e outra apresentando o resultado da detecção das features em tempo real. Esse procedimento permitiu observar o desempenho dos algoritmos sob condições dinâmicas, avaliando a estabilidade e a quantidade de pontos detectados a cada quadro. Foram realizadas filmagens envolvendo os integrantes do grupo e também um tabuleiro de xadrez preto e branco, escolhido por conter um padrão geométrico com cantos bem definidos e amplamente utilizado em calibração de câmeras.</p>

<p style="text-align:justify;">Além da implementação prática, foram realizadas pesquisas complementares sobre as aplicações das técnicas de detecção de *features* e *corners* em diferentes áreas da visão computacional. Foram discutidos exemplos de uso em sistemas de rastreamento de movimento, reconstrução tridimensional, reconhecimento de objetos e calibração de câmeras, relacionando essas aplicações com possíveis usos no projeto desenvolvido na disciplina. Dessa forma, os métodos empregados combinaram estudo teórico, experimentação prática e análise comparativa, proporcionando uma compreensão abrangente dos fundamentos e das aplicações da detecção de features em imagens digitais.</p>

<h3>Diagrama de Blocos</h3>
       <figure style="max-width: 1100px; margin: 24px auto; text-align: center;">
  <img src="imagens/Bloco_Features.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de Features.
  </figcaption>
</figure>
      
      <h3>4. Resultados e Análises</h3>

  <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Detecção de features em vídeos</h4>
    <p style="text-align:justify;">
      Durante o experimento com o vídeo previamente gravado, observou-se que o algoritmo de detecção de features apresentou desempenho considerável na identificação de pontos de interesse.
      Nas imagens contendo a mão mecânica, o método foi capaz de detectar diversos pontos ao longo de suas bordas e articulações, principalmente nas pontas dos dedos. Esses pontos foram 
      realçados em torno dos dedos e das junções, indicando que o detector conseguiu reconhecer áreas onde há variações de geometria. O algoritmo conseguiu reconhecer a variação de teclas 
      presentes no computador localizado no canto inferior da tela. Mesmo com pequenas variações de iluminação e movimentações sutis da mão mecânica durante o vídeo, as features permaneceram 
      estáveis. Esse comportamento confirma a eficiência do detector na identificação de estruturas com contornos bem definidos.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de tela de 2025-10-29 10-35-52.webm" type="video/webm">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela mostrando a detecção de features em um vídeo gravado pelo grupo.
      </figcaption>
    </figure>
      
    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/frame_0.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Identificação de Features a partir da webcam</h4>
    <p style="text-align:justify;">
      No experimento utilizando a webcam, foi possível observar uma detecção de features consistente e em tempo real, tanto em objetos estáticos quanto em movimento.
      Ao posicionar o tabuleiro de xadrez, o algoritmo identificou com alta precisão os cantos formados pela interseção dos quadrados pretos e brancos. As features 
      foram distribuídas de maneira uniforme sobre toda a superfície do tabuleiro, evidenciando o forte contraste e a simetria da cena. Nos testes com integrantes do grupo,
      as principais regiões de detecção concentraram-se ao redor dos olhos, contornos em geral, e em áreas de maior contraste, como o nariz. Mesmo com movimento moderado 
      ou inconsistências na iluminação, a detecção manteve-se estável, indicando que o sistema possui desempenho satisfatório para aplicações em tempo real.
    </p>
    </section>
    <figure>
      <div class="video-container">
        <video controls style="width:70%; max-width:900px; height:auto; border-radius:10px;">
          <source src="imagens/Gravação de tela de 2025-10-29 10-40-04.webm" type="video/webm">
          Seu navegador não suporta a reprodução de vídeo.
        </video>
      </div>
      <figcaption>
        Gravação de tela exibindo a identificação de Features em tempo real.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_152.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features na Fernanda.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_58.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features no Gabriel.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_234.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features na Ingrid.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/webcam_frame_35.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem identificando os Features no Tabuleiro de Xadrez
      </figcaption>
    </figure>
    
    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h3>Aplicações práticas</h3>

    <h4>1. Rastreamento de movimento (Optical Flow)</h4>
    <p>
      Cantos são pontos ideais para acompanhar o movimento de objetos entre 
      quadros consecutivos de um vídeo. Por exemplo, o método 
      Lucas–Kanade utiliza os cantos detectados porShi–Tomasi
      para rastrear o deslocamento desses pontos ao longo do tempo, permitindo 
      identificar o movimento de pessoas, veículos ou partes específicas de uma cena.
    </p>
  
    <h4>2. Calibração de câmeras</h4>
    <p>
      O padrão de tabuleiro xadrez é amplamente usado em calibração porque fornece 
      cantos bem definidos e facilmente detectáveis. Ao identificar as interseções 
      do tabuleiro em múltiplas imagens, é possível calcular os 
      parâmetros intrínsecos e extrínsecos da câmera (foco, 
      distorção, posição e orientação).
    </p>
  
    <h4>3. Reconstrução 3D e estereovisão</h4>
    <p>
      Em sistemas com duas câmeras, os cantos detectados nas duas imagens permitem 
      encontrar correspondências entre pontos homólogos. A partir dessas 
      correspondências, calcula-se a profundidade dos pontos e 
      reconstrói-se o modelo tridimensional da cena.
    </p>
  
    <h4>4. Mosaico de imagens (Image Stitching)</h4>
    <p>
      Ao detectar cantos e outros features em diferentes imagens de uma 
      mesma cena, é possível emparelhá-los e calcular as transformações necessárias 
      para uni-las, criando panoramas. Essa técnica é usada em aplicativos de 
      câmeras de celular e em softwares de edição de imagem.
    </p>
  
    <h4>5. Reconhecimento e localização de objetos</h4>
    <p>
      Detectores de features podem ser usados para reconhecer padrões 
      fixos (como logotipos, faces, placas ou produtos), comparando os pontos 
      detectados em uma imagem de referência com os da imagem analisada.
    </p>
  
    <h3>Experimento proposto (projeto da disciplina)</h3>
  
    <p>
      No contexto do projeto de vídeo do grupo (RoadWatch), 
      a detecção de features pode ser aplicada para 
      rastrear o movimento do celular nas mãos do motorista
      durante uma gravação. O experimento consiste em:
    </p>
  
    <ol>
      <li>
        Capturar um vídeo curto do motorista segurando o celular ao volante.
      </li>
      <li>
        Aplicar o detector Shi–Tomasi em cada quadro para localizar
        cantos nas bordas do celular.
      </li>
      <li>
        Usar o método Lucas–Kanade Optical Flow para acompanhar o 
        movimento desses pontos ao longo do tempo.
      </li>
      <li>
        Analisar o deslocamento das features: caso os pontos 
        correspondentes ao celular se movam significativamente enquanto o carro 
        está parado, o sistema pode identificar o uso do aparelho.
      </li>
    </ol>
  
    <p>
      Esse experimento demonstra como a detecção de cantos e o rastreamento de 
    features são ferramentas poderosas para 
     análise de comportamento em vídeo, podendo ser integradas em 
      aplicações de segurança veicular, monitoramento 
      e visão inteligente.
    </p>
    </section>  
    
    <h3>5. Conclusões</h3>
     <p style="text-align:justify;">
        Este relatório permitiu compreender o papel das features em Processamento de Vídeo e sua importância em sistemas de detecção, reconhecimento e rastreamento.
        Os experimentos comprovaram que técnicas como Shi-Tomasi e Good Features to Track são eficientes para identificar cantos e pontos relevantes em imagens 
        estáticas e em tempo real.
      </p>
      
      <p style="text-align:justify;">
        No tabuleiro de xadrez, a detecção foi precisa, evidenciando a força da técnica em padrões bem definidos. Já em humanos, os cantos dos olhos, nariz e 
        mãos se destacaram como features importantes. Conclui-se que o uso de detectores de features é essencial para o projeto de Processamento de Vídeo, 
        podendo contribuir para identificar elementos como mãos, celular ou movimentos do motorista.
      </p>
    </div>
    </div>
  </details>

      <!-- RELATÓRIO 7 -->
  <details>
    <summary>Relatório 7</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> 10/11/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
        <p style="text-align:justify;">Esta prática tem como objetivo introduzir os conceitos fundamentais de detecção de objetos utilizando o OpenCV em C++, com foco na detecção de rostos por meio de classificadores em cascata (Cascade Classifier) baseados em Haar-like features. Para isso, o estudante deve primeiro revisar a teoria, compreendendo como esses modelos funcionam, desde a extração de características e o uso de janelas deslizantes até a estrutura em múltiplas etapas que permite decidir rapidamente se uma região da imagem representa ou não o objeto de interesse. Em seguida, serão utilizados modelos pré-treinados em arquivos XML disponibilizados pelo OpenCV, responsáveis por reconhecer padrões visuais, como rostos frontais e perfis faciais.</p>

  <p style="text-align:justify;">No primeiro experimento, será desenvolvido um programa em C++ capaz de carregar imagens dos integrantes da equipe, além da imagem com avatares e imagens do projeto de vídeo. O sistema deverá realizar a detecção de rostos e/ou outros objetos escolhidos, marcando as regiões detectadas com retângulos ou indicadores visuais em **cores contrastantes. O programa também deve permitir que o usuário salve as imagens anotadas ao pressionar uma tecla, registrando no relatório qual arquivo XML foi utilizado (por exemplo, `haarcascade_frontalface_default.xml` ou outro modelo Haarcascade disponível no repositório oficial do OpenCV).</p>

  <p style="text-align:justify;">No segundo experimento, o código será adaptado para realizar a captura ao vivo pela webcam, exibindo em tempo real a imagem da câmera juntamente com o resultado da detecção de objetos. Da mesma forma, o usuário deverá poder salvar quadros detectados sob comando. Ao final, espera-se que o estudante compreenda o funcionamento e as limitações desses detectores clássicos — como sensibilidade à iluminação, variação de pose e oclusões — além de adquirir prática na configuração e uso dos classificadores Haarcascade, ajustes de parâmetros, integração com webcams e manipulação de imagens no OpenCV. O relatório deve incluir as imagens geradas, observações sobre a precisão da detecção, parâmetros utilizados e um breve comentário sobre boas práticas e ética no uso de detecção facial, incluindo cuidado com privacidade e consentimento ao registrar imagens de pessoas reais.</p>
 
      <h3>2. Fundamentos Básicos</h3>
       <p style="text-align:justify;">A detecção automática de objetos é uma tarefa fundamental em visão computacional, permitindo identificar e localizar elementos relevantes em imagens ou vídeos, como rostos, pessoas, veículos ou outros padrões visuais. No contexto desta atividade, será utilizada a técnica de classificadores em cascata baseada em Haar-like features, um método clássico que antecede o uso extensivo de redes neurais e ainda é bastante útil para fins acadêmicos e aplicações em tempo real.</p>

 <p style="text-align:justify;">Os classificadores Haarcascade funcionam a partir de um conjunto de características simples que analisam variações de intensidade entre regiões claras e escuras da imagem. Essas características são calculadas de forma muito rápida por meio da estrutura chamada imagem integral, que agiliza o cálculo de somas de pixels em regiões retangulares. A detecção ocorre por meio de uma sequência de etapas chamada cascata: cada estágio avalia a imagem e descarta rapidamente áreas que não correspondem ao objeto; apenas as regiões que passam por todas as fases são consideradas como detecções válidas. Essa abordagem garante eficiência e velocidade, permitindo seu uso em dispositivos com potência de processamento limitada.</p>

 <p style="text-align:justify;">Para facilitar sua aplicação, o OpenCV disponibiliza arquivos XML pré-treinados, conhecidos como Haarcascades, contendo modelos preparados para diferentes objetos, como rosto frontal, perfil facial, olhos, sorriso e outros. Assim, o estudante pode focar na implementação e experimentação do algoritmo sem necessidade de treinar um modelo do zero. A detecção costuma ser feita em imagens em tons de cinza, já que o método se baseia exclusivamente em diferenças de intensidade, e frequentemente são aplicadas técnicas simples de pré-processamento, como equalização de histograma, para melhorar contraste e estabilidade da detecção.</p>

 <p style="text-align:justify;">Entre os pontos fortes desse método destacam-se sua simplicidade, velocidade e boa performance em situações controladas. Por outro lado, há limitações, como sensibilidade a iluminação variável, mudanças bruscas de posição ou rotação do rosto e oclusões parciais, como uso de máscaras ou objetos cobrindo o rosto. Mesmo com essas restrições, o estudo desse tipo de algoritmo é importante, pois ele representa um marco histórico no avanço da visão computacional e fornece uma base sólida para compreender métodos modernos de detecção, como aqueles baseados em aprendizado profundo.</p>


      <h3>3. Materiais e Métodos</h3>
      <p style="text-align:justify;">Para a realização desta atividade, foi utilizado um computador com sistema operacional compatível com a biblioteca OpenCV e suporte à compilação em C++. O ambiente de desenvolvimento empregado inclui o compilador g++, juntamente com a instalação da biblioteca OpenCV versão 4.x ou superior, necessária para carregar os modelos pré-treinados, manipular imagens, acessar a webcam e executar o algoritmo de detecção. Também foram utilizados arquivos XML contendo modelos Haarcascade disponibilizados pelo OpenCV, incluindo, por exemplo, o classificador frontal para detecção de rostos. As imagens de referência consistiram em fotografias dos integrantes do grupo, avatares utilizados em atividades anteriores e quadros extraídos do vídeo desenvolvido no projeto da disciplina, além do uso de uma webcam integrada ou externa para a fase de captura em tempo real.</p>

<p style="text-align:justify;">O procedimento foi dividido em duas etapas principais. Na primeira, desenvolveu-se um programa em C++ capaz de carregar imagens estáticas e aplicar o classificador em cascata para detecção de rostos ou outros objetos. Inicialmente, a imagem foi lida e convertida para tons de cinza, permitindo ao detector trabalhar apenas com variações de intensidade luminosa. Em seguida, aplicou-se, quando necessário, a equalização de histograma para melhorar o contraste e aumentar as chances de detecção. Depois disso, o classificador Haarcascade foi carregado a partir do arquivo XML correspondente e o método detectMultiScale foi utilizado para localizar possíveis regiões contendo rostos. As áreas detectadas foram marcadas com retângulos coloridos para facilitar a visualização e o programa foi configurado para salvar as imagens anotadas ao pressionar uma tecla. Essa etapa permitiu avaliar diferentes parâmetros, como escala, número mínimo de vizinhos e tamanho mínimo de objeto detectado.</p>

<p style="text-align:justify;">Na segunda etapa, o código foi modificado para operar com captura ao vivo por webcam. O programa passou a abrir uma janela que exibia continuamente o fluxo de vídeo, aplicando o mesmo processo de detecção quadro a quadro. Assim como na etapa anterior, foi mantida a possibilidade de salvar imagens com as detecções sempre que o usuário solicitasse. Foram observados aspectos como taxa de atualização, latência e consistência das detecções em movimento, além do impacto de condições de iluminação e distância do rosto em relação à câmera.</p>
      
       <h3>Diagrama de Blocos</h3>
             <figure style="max-width: 1500px; margin: 24px auto; text-align: center;">
  <img src="imagens/fluxograma_lab7.png"
       loading="lazy"
       style="width:100%; height:auto; border-radius:12px; border:2px solid #aaa; box-shadow:0 4px 12px rgba(0,0,0,0.2);">
  <figcaption style="font:15px/1.5 Arial, sans-serif; color:#333; margin-top:10px;">
    Diagrama de blocos de Haarcascade.
  </figcaption>
</figure>
      
      <h3>4. Resultados e Análises</h3>
      <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de Imagem dos Integrantes e Avatares</h4>
    <p style="text-align:justify;">
      O programa foi configurado para carregar imagens dos membros da equipe e de seus respectivos avatares. As detecções foram exibidas 
      na tela e, ao pressionar uma tecla, as imagens foram salvas com os retângulos marcando os objetos detectados. As faces dos integrantes
      foram detectadas usando o modelo haarcascade_frontalface_alt2.xml do OpenCV. Com o modelo haarcascade_eye_tree_eyeglasses.xml, os olhos
      foram identificados, mas não com a mesma precisão de identificação do modelo anterior, que identificou corretamente o formato das faces.
      Nos avatares, a detecção funcionou melhor devido a uma melhor iluminação e definição de imagem.
    </p>
    </section>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/teste_1_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Imagem da Ingrid após aplicação do modelo HaarCascade.
      </figcaption>
    </figure>  
      
    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/avatarFernanda_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Avatar da Fernanda após aplicação do modelo HaarCascade.
      </figcaption>
    </figure>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/avatarGabriel_detected.png"
           loading="lazy"
           style="width:60%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Avatar do Gabriel após aplicação do modelo HaarCascade.
      </figcaption>
    </figure>

    <section style="max-width: 900px; margin: 20px auto; text-align: justify; font-family: Arial, sans-serif; line-height: 1.6;">
    <h4>Leitura de Imagem da Webcam</h4>
    <p style="text-align:justify;">
      No segundo experimento utilizou-se a webcam para capturar e exibir as detecções em tempo real. O algoritmo aplicava a detecção 
      de rostos e olhos, e ao pressionar uma tecla, salvava a imagem com as marcações em diferentes cores. A detecção da face foi 
      estável e rápida, mesmo com variações leves de posição e iluminação. Na região dos olhos ouve uma certa variação de precisão,
      mas ainda assim o algoritmo conseguiu detectar e marcar com retângulos vermelhos a região. Quando havia movimento rápido ou 
      baixa iluminação, algumas falhas ocorreram, mas o algoritmo se recuperava rapidamente.
    </p>
    </section>

    <figure style="max-width: 720px; margin: 16px auto; text-align: center;">
      <img src="imagens/Captura de tela de 2025-11-03 09-17-34.png"
           loading="lazy"
           style="width:70%; height:auto; border-radius:8px; border:1px solid #ccc;">
      <figcaption style="font:14px/1.4 Arial, sans-serif; color:#00000; margin-top:8px;">
        Captura de Tela da Webcam após aplicação do modelo HaarCascade.
      </figcaption>
    </figure>
    
    <h3>5. Conclusões</h3>
     <p style="text-align:justify;">
       A partir das análises realizadas, pode-se concluir que o método Haar Cascade apresentou 
       desempenho considerável na detecção de rostos e olhos, tanto em imagens estáticas quanto 
       em vídeo ao vivo. A utilização dos modelos haarcascade_frontalface_alt2.xml e 
       haarcascade_eye_tree_eyeglasses.xml garantiu resultados satisfatórios, com identificação 
       mesmo diante de expressões faciais variadas.

       No entanto, observou-se que a eficácia do método está diretamente relacionada a condições adequadas 
       de captura, como boa iluminação, posição frontal do rosto e ausência de movimentações bruscas.
       Assim, embora o algoritmo seja eficiente, seu desempenho pode ser comprometido em ambientes com baixa 
       luminosidade ou ângulos desfavoráveis.
      </p>
    </div>
    </div>
  </details>
  
      <!-- RELATÓRIO 8 -->
  <details>
    <summary>Relatório 8</summary>
    <header class="content">
      <p><strong>Integrantes:</strong> Fernanda Ayumi Kuroiwa — Gabriel Henrique Pensado Rothen — Ingrid Mara Xavier</p>
      <p><strong>Data:</strong> X/X/2025</p>
    </header>
    <div class="content">
      <h3>1. Introdução</h3>
      <p style="text-align:justify;">Resumo do objetivo do Relatório 2...</p>
      <h3>2. Fundamentos Básicos</h3>
      <p style="text-align:justify;">Resumo do objetivo do Relatório 3...</p>
      <h3>3. Materiais e Métodos</h3>
      <p style="text-align:justify;">Resumo do objetivo do Relatório 3...</p>
      <h3>4. Resultados e Análises</h3>
      <p style="text-align:justify;">Resumo do objetivo do Relatório 3...</p>
      <h3>5. Conclusões</h3>
      <p style="text-align:justify;">Resumo do objetivo do Relatório 3...</p>
    </div>
    </div>
  </details>  
  
      <div class="back-top"><a href="#topo">↑ Voltar ao topo</a></div>
  </section>
  
  <footer class="footer">
    © 2025 — Equipe RoadWatch
  </footer>
</body>
</html>
